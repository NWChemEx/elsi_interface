/* Copyright 2014 - 2023, A. Marek */

/*     This file is part of ELPA. */

/*     The ELPA library was originally created by the ELPA consortium, */
/*     consisting of the following organizations: */

/*     - Max Planck Computing and Data Facility (MPCDF), formerly known as */
/*       Rechenzentrum Garching der Max-Planck-Gesellschaft (RZG), */
/*     - Bergische Universität Wuppertal, Lehrstuhl für angewandte */
/*       Informatik, */
/*     - Technische Universität München, Lehrstuhl für Informatik mit */
/*       Schwerpunkt Wissenschaftliches Rechnen , */
/*     - Fritz-Haber-Institut, Berlin, Abt. Theorie, */
/*     - Max-Plack-Institut für Mathematik in den Naturwissenschaften, */
/*       Leipzig, Abt. Komplexe Strukutren in Biologie und Kognition, */
/*       and */
/*     - IBM Deutschland GmbH */


/*     More information can be found here: */
/*     http://elpa.mpcdf.mpg.de/ */

/*     ELPA is free software: you can redistribute it and/or modify */
/*     it under the terms of the version 3 of the license of the */
/*     GNU Lesser General Public License as published by the Free */
/*     Software Foundation. */

/*     ELPA is distributed in the hope that it will be useful, */
/*     but WITHOUT ANY WARRANTY; without even the implied warranty of */
/*     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the */
/*     GNU Lesser General Public License for more details. */

/*     You should have received a copy of the GNU Lesser General Public License */
/*     along with ELPA.  If not, see <http://www.gnu.org/licenses/> */

/*     ELPA reflects a substantial effort on the part of the original */
/*     ELPA consortium, and we ask you to respect the spirit of the */
/*     license that we chose: i.e., please contribute any changes you */
/*     may have back to the original ELPA library distribution, and keep */
/*     any derivatives of ELPA under the same license that we chose for */
/*     the original distribution, the GNU Lesser General Public License. */

/*  Author: Andreas Marek, MPCDF */
/*  This file is the generated version. Do NOT edit */



# 1 "../src/elpa2/kernels/real_avx2_2hv_double_precision.c"
# 1 "<built-in>" 1
# 1 "<built-in>" 3
# 378 "<built-in>" 3
# 1 "<command line>" 1
# 1 "<built-in>" 2
# 1 "../src/elpa2/kernels/real_avx2_2hv_double_precision.c" 2
# 47 "../src/elpa2/kernels/real_avx2_2hv_double_precision.c"
# 1 "./config-f90.h" 1
# 48 "../src/elpa2/kernels/real_avx2_2hv_double_precision.c" 2





# 1 "../src/elpa2/kernels/../../general/precision_macros.h" 1
# 54 "../src/elpa2/kernels/real_avx2_2hv_double_precision.c" 2
# 1 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c" 1
# 49 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
# 1 "./config-f90.h" 1
# 50 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c" 2

# 1 "/usr/include/stdlib.h" 1 3 4
# 24 "/usr/include/stdlib.h" 3 4
# 1 "/usr/include/features.h" 1 3 4
# 345 "/usr/include/features.h" 3 4
# 1 "/usr/include/stdc-predef.h" 1 3 4
# 346 "/usr/include/features.h" 2 3 4
# 375 "/usr/include/features.h" 3 4
# 1 "/usr/include/sys/cdefs.h" 1 3 4
# 392 "/usr/include/sys/cdefs.h" 3 4
# 1 "/usr/include/bits/wordsize.h" 1 3 4
# 393 "/usr/include/sys/cdefs.h" 2 3 4
# 376 "/usr/include/features.h" 2 3 4
# 399 "/usr/include/features.h" 3 4
# 1 "/usr/include/gnu/stubs.h" 1 3 4
# 10 "/usr/include/gnu/stubs.h" 3 4
# 1 "/usr/include/gnu/stubs-64.h" 1 3 4
# 11 "/usr/include/gnu/stubs.h" 2 3 4
# 400 "/usr/include/features.h" 2 3 4
# 25 "/usr/include/stdlib.h" 2 3 4







# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/stddef.h" 1 3 4
# 62 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/stddef.h" 3 4
typedef long unsigned int size_t;
# 90 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/stddef.h" 3 4
typedef int wchar_t;
# 33 "/usr/include/stdlib.h" 2 3 4








# 1 "/usr/include/bits/waitflags.h" 1 3 4
# 42 "/usr/include/stdlib.h" 2 3 4
# 1 "/usr/include/bits/waitstatus.h" 1 3 4
# 64 "/usr/include/bits/waitstatus.h" 3 4
# 1 "/usr/include/endian.h" 1 3 4
# 36 "/usr/include/endian.h" 3 4
# 1 "/usr/include/bits/endian.h" 1 3 4
# 37 "/usr/include/endian.h" 2 3 4
# 60 "/usr/include/endian.h" 3 4
# 1 "/usr/include/bits/byteswap.h" 1 3 4
# 27 "/usr/include/bits/byteswap.h" 3 4
# 1 "/usr/include/bits/types.h" 1 3 4
# 27 "/usr/include/bits/types.h" 3 4
# 1 "/usr/include/bits/wordsize.h" 1 3 4
# 28 "/usr/include/bits/types.h" 2 3 4


typedef unsigned char __u_char;
typedef unsigned short int __u_short;
typedef unsigned int __u_int;
typedef unsigned long int __u_long;


typedef signed char __int8_t;
typedef unsigned char __uint8_t;
typedef signed short int __int16_t;
typedef unsigned short int __uint16_t;
typedef signed int __int32_t;
typedef unsigned int __uint32_t;

typedef signed long int __int64_t;
typedef unsigned long int __uint64_t;







typedef long int __quad_t;
typedef unsigned long int __u_quad_t;
# 130 "/usr/include/bits/types.h" 3 4
# 1 "/usr/include/bits/typesizes.h" 1 3 4
# 131 "/usr/include/bits/types.h" 2 3 4


typedef unsigned long int __dev_t;
typedef unsigned int __uid_t;
typedef unsigned int __gid_t;
typedef unsigned long int __ino_t;
typedef unsigned long int __ino64_t;
typedef unsigned int __mode_t;
typedef unsigned long int __nlink_t;
typedef long int __off_t;
typedef long int __off64_t;
typedef int __pid_t;
typedef struct { int __val[2]; } __fsid_t;
typedef long int __clock_t;
typedef unsigned long int __rlim_t;
typedef unsigned long int __rlim64_t;
typedef unsigned int __id_t;
typedef long int __time_t;
typedef unsigned int __useconds_t;
typedef long int __suseconds_t;

typedef int __daddr_t;
typedef int __key_t;


typedef int __clockid_t;


typedef void * __timer_t;


typedef long int __blksize_t;




typedef long int __blkcnt_t;
typedef long int __blkcnt64_t;


typedef unsigned long int __fsblkcnt_t;
typedef unsigned long int __fsblkcnt64_t;


typedef unsigned long int __fsfilcnt_t;
typedef unsigned long int __fsfilcnt64_t;


typedef long int __fsword_t;

typedef long int __ssize_t;


typedef long int __syscall_slong_t;

typedef unsigned long int __syscall_ulong_t;



typedef __off64_t __loff_t;
typedef __quad_t *__qaddr_t;
typedef char *__caddr_t;


typedef long int __intptr_t;


typedef unsigned int __socklen_t;
# 28 "/usr/include/bits/byteswap.h" 2 3 4
# 1 "/usr/include/bits/wordsize.h" 1 3 4
# 29 "/usr/include/bits/byteswap.h" 2 3 4






# 1 "/usr/include/bits/byteswap-16.h" 1 3 4
# 36 "/usr/include/bits/byteswap.h" 2 3 4
# 61 "/usr/include/endian.h" 2 3 4
# 65 "/usr/include/bits/waitstatus.h" 2 3 4

union wait
  {
    int w_status;
    struct
      {

 unsigned int __w_termsig:7;
 unsigned int __w_coredump:1;
 unsigned int __w_retcode:8;
 unsigned int:16;







      } __wait_terminated;
    struct
      {

 unsigned int __w_stopval:8;
 unsigned int __w_stopsig:8;
 unsigned int:16;






      } __wait_stopped;
  };
# 43 "/usr/include/stdlib.h" 2 3 4
# 67 "/usr/include/stdlib.h" 3 4
typedef union
  {
    union wait *__uptr;
    int *__iptr;
  } __WAIT_STATUS __attribute__ ((__transparent_union__));
# 97 "/usr/include/stdlib.h" 3 4
typedef struct
  {
    int quot;
    int rem;
  } div_t;



typedef struct
  {
    long int quot;
    long int rem;
  } ldiv_t;







__extension__ typedef struct
  {
    long long int quot;
    long long int rem;
  } lldiv_t;
# 139 "/usr/include/stdlib.h" 3 4
extern size_t __ctype_get_mb_cur_max (void) __attribute__ ((__nothrow__ )) ;




extern double atof (const char *__nptr)
     __attribute__ ((__nothrow__ )) __attribute__ ((__pure__)) __attribute__ ((__nonnull__ (1))) ;

extern int atoi (const char *__nptr)
     __attribute__ ((__nothrow__ )) __attribute__ ((__pure__)) __attribute__ ((__nonnull__ (1))) ;

extern long int atol (const char *__nptr)
     __attribute__ ((__nothrow__ )) __attribute__ ((__pure__)) __attribute__ ((__nonnull__ (1))) ;





__extension__ extern long long int atoll (const char *__nptr)
     __attribute__ ((__nothrow__ )) __attribute__ ((__pure__)) __attribute__ ((__nonnull__ (1))) ;





extern double strtod (const char *__restrict __nptr,
        char **__restrict __endptr)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1)));





extern float strtof (const char *__restrict __nptr,
       char **__restrict __endptr) __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1)));

extern long double strtold (const char *__restrict __nptr,
       char **__restrict __endptr)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1)));





extern long int strtol (const char *__restrict __nptr,
   char **__restrict __endptr, int __base)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1)));

extern unsigned long int strtoul (const char *__restrict __nptr,
      char **__restrict __endptr, int __base)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1)));




__extension__
extern long long int strtoq (const char *__restrict __nptr,
        char **__restrict __endptr, int __base)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1)));

__extension__
extern unsigned long long int strtouq (const char *__restrict __nptr,
           char **__restrict __endptr, int __base)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1)));





__extension__
extern long long int strtoll (const char *__restrict __nptr,
         char **__restrict __endptr, int __base)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1)));

__extension__
extern unsigned long long int strtoull (const char *__restrict __nptr,
     char **__restrict __endptr, int __base)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1)));
# 277 "/usr/include/stdlib.h" 3 4
extern __inline __attribute__ ((__gnu_inline__)) int
__attribute__ ((__nothrow__ )) atoi (const char *__nptr)
{
  return (int) strtol (__nptr, (char **) ((void*)0), 10);
}
extern __inline __attribute__ ((__gnu_inline__)) long int
__attribute__ ((__nothrow__ )) atol (const char *__nptr)
{
  return strtol (__nptr, (char **) ((void*)0), 10);
}




__extension__ extern __inline __attribute__ ((__gnu_inline__)) long long int
__attribute__ ((__nothrow__ )) atoll (const char *__nptr)
{
  return strtoll (__nptr, (char **) ((void*)0), 10);
}
# 305 "/usr/include/stdlib.h" 3 4
extern char *l64a (long int __n) __attribute__ ((__nothrow__ )) ;


extern long int a64l (const char *__s)
     __attribute__ ((__nothrow__ )) __attribute__ ((__pure__)) __attribute__ ((__nonnull__ (1))) ;




# 1 "/usr/include/sys/types.h" 1 3 4
# 33 "/usr/include/sys/types.h" 3 4
typedef __u_char u_char;
typedef __u_short u_short;
typedef __u_int u_int;
typedef __u_long u_long;
typedef __quad_t quad_t;
typedef __u_quad_t u_quad_t;
typedef __fsid_t fsid_t;




typedef __loff_t loff_t;



typedef __ino_t ino_t;
# 60 "/usr/include/sys/types.h" 3 4
typedef __dev_t dev_t;




typedef __gid_t gid_t;




typedef __mode_t mode_t;




typedef __nlink_t nlink_t;




typedef __uid_t uid_t;





typedef __off_t off_t;
# 98 "/usr/include/sys/types.h" 3 4
typedef __pid_t pid_t;





typedef __id_t id_t;




typedef __ssize_t ssize_t;





typedef __daddr_t daddr_t;
typedef __caddr_t caddr_t;





typedef __key_t key_t;
# 132 "/usr/include/sys/types.h" 3 4
# 1 "/usr/include/time.h" 1 3 4
# 59 "/usr/include/time.h" 3 4
typedef __clock_t clock_t;
# 75 "/usr/include/time.h" 3 4
typedef __time_t time_t;
# 91 "/usr/include/time.h" 3 4
typedef __clockid_t clockid_t;
# 103 "/usr/include/time.h" 3 4
typedef __timer_t timer_t;
# 133 "/usr/include/sys/types.h" 2 3 4
# 146 "/usr/include/sys/types.h" 3 4
# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/stddef.h" 1 3 4
# 147 "/usr/include/sys/types.h" 2 3 4



typedef unsigned long int ulong;
typedef unsigned short int ushort;
typedef unsigned int uint;
# 194 "/usr/include/sys/types.h" 3 4
typedef int int8_t __attribute__ ((__mode__ (__QI__)));
typedef int int16_t __attribute__ ((__mode__ (__HI__)));
typedef int int32_t __attribute__ ((__mode__ (__SI__)));
typedef int int64_t __attribute__ ((__mode__ (__DI__)));


typedef unsigned int u_int8_t __attribute__ ((__mode__ (__QI__)));
typedef unsigned int u_int16_t __attribute__ ((__mode__ (__HI__)));
typedef unsigned int u_int32_t __attribute__ ((__mode__ (__SI__)));
typedef unsigned int u_int64_t __attribute__ ((__mode__ (__DI__)));

typedef int register_t __attribute__ ((__mode__ (__word__)));
# 219 "/usr/include/sys/types.h" 3 4
# 1 "/usr/include/sys/select.h" 1 3 4
# 30 "/usr/include/sys/select.h" 3 4
# 1 "/usr/include/bits/select.h" 1 3 4
# 22 "/usr/include/bits/select.h" 3 4
# 1 "/usr/include/bits/wordsize.h" 1 3 4
# 23 "/usr/include/bits/select.h" 2 3 4
# 31 "/usr/include/sys/select.h" 2 3 4


# 1 "/usr/include/bits/sigset.h" 1 3 4
# 23 "/usr/include/bits/sigset.h" 3 4
typedef int __sig_atomic_t;




typedef struct
  {
    unsigned long int __val[(1024 / (8 * sizeof (unsigned long int)))];
  } __sigset_t;
# 34 "/usr/include/sys/select.h" 2 3 4



typedef __sigset_t sigset_t;





# 1 "/usr/include/time.h" 1 3 4
# 120 "/usr/include/time.h" 3 4
struct timespec
  {
    __time_t tv_sec;
    __syscall_slong_t tv_nsec;
  };
# 44 "/usr/include/sys/select.h" 2 3 4

# 1 "/usr/include/bits/time.h" 1 3 4
# 30 "/usr/include/bits/time.h" 3 4
struct timeval
  {
    __time_t tv_sec;
    __suseconds_t tv_usec;
  };
# 46 "/usr/include/sys/select.h" 2 3 4


typedef __suseconds_t suseconds_t;





typedef long int __fd_mask;
# 64 "/usr/include/sys/select.h" 3 4
typedef struct
  {






    __fd_mask __fds_bits[1024 / (8 * (int) sizeof (__fd_mask))];


  } fd_set;






typedef __fd_mask fd_mask;
# 106 "/usr/include/sys/select.h" 3 4
extern int select (int __nfds, fd_set *__restrict __readfds,
     fd_set *__restrict __writefds,
     fd_set *__restrict __exceptfds,
     struct timeval *__restrict __timeout);
# 118 "/usr/include/sys/select.h" 3 4
extern int pselect (int __nfds, fd_set *__restrict __readfds,
      fd_set *__restrict __writefds,
      fd_set *__restrict __exceptfds,
      const struct timespec *__restrict __timeout,
      const __sigset_t *__restrict __sigmask);
# 220 "/usr/include/sys/types.h" 2 3 4


# 1 "/usr/include/sys/sysmacros.h" 1 3 4
# 31 "/usr/include/sys/sysmacros.h" 3 4
__extension__
extern unsigned int gnu_dev_major (unsigned long long int __dev)
     __attribute__ ((__nothrow__ )) __attribute__ ((__const__));
__extension__
extern unsigned int gnu_dev_minor (unsigned long long int __dev)
     __attribute__ ((__nothrow__ )) __attribute__ ((__const__));
__extension__
extern unsigned long long int gnu_dev_makedev (unsigned int __major,
            unsigned int __minor)
     __attribute__ ((__nothrow__ )) __attribute__ ((__const__));


__extension__ extern __inline __attribute__ ((__gnu_inline__)) __attribute__ ((__const__)) unsigned int
__attribute__ ((__nothrow__ )) gnu_dev_major (unsigned long long int __dev)
{
  return ((__dev >> 8) & 0xfff) | ((unsigned int) (__dev >> 32) & ~0xfff);
}

__extension__ extern __inline __attribute__ ((__gnu_inline__)) __attribute__ ((__const__)) unsigned int
__attribute__ ((__nothrow__ )) gnu_dev_minor (unsigned long long int __dev)
{
  return (__dev & 0xff) | ((unsigned int) (__dev >> 12) & ~0xff);
}

__extension__ extern __inline __attribute__ ((__gnu_inline__)) __attribute__ ((__const__)) unsigned long long int
__attribute__ ((__nothrow__ )) gnu_dev_makedev (unsigned int __major, unsigned int __minor)
{
  return ((__minor & 0xff) | ((__major & 0xfff) << 8)
   | (((unsigned long long int) (__minor & ~0xff)) << 12)
   | (((unsigned long long int) (__major & ~0xfff)) << 32));
}
# 223 "/usr/include/sys/types.h" 2 3 4





typedef __blksize_t blksize_t;






typedef __blkcnt_t blkcnt_t;



typedef __fsblkcnt_t fsblkcnt_t;



typedef __fsfilcnt_t fsfilcnt_t;
# 270 "/usr/include/sys/types.h" 3 4
# 1 "/usr/include/bits/pthreadtypes.h" 1 3 4
# 21 "/usr/include/bits/pthreadtypes.h" 3 4
# 1 "/usr/include/bits/wordsize.h" 1 3 4
# 22 "/usr/include/bits/pthreadtypes.h" 2 3 4
# 60 "/usr/include/bits/pthreadtypes.h" 3 4
typedef unsigned long int pthread_t;


union pthread_attr_t
{
  char __size[56];
  long int __align;
};

typedef union pthread_attr_t pthread_attr_t;





typedef struct __pthread_internal_list
{
  struct __pthread_internal_list *__prev;
  struct __pthread_internal_list *__next;
} __pthread_list_t;
# 90 "/usr/include/bits/pthreadtypes.h" 3 4
typedef union
{
  struct __pthread_mutex_s
  {
    int __lock;
    unsigned int __count;
    int __owner;

    unsigned int __nusers;



    int __kind;

    short __spins;
    short __elision;
    __pthread_list_t __list;
# 125 "/usr/include/bits/pthreadtypes.h" 3 4
  } __data;
  char __size[40];
  long int __align;
} pthread_mutex_t;

typedef union
{
  char __size[4];
  int __align;
} pthread_mutexattr_t;




typedef union
{
  struct
  {
    int __lock;
    unsigned int __futex;
    __extension__ unsigned long long int __total_seq;
    __extension__ unsigned long long int __wakeup_seq;
    __extension__ unsigned long long int __woken_seq;
    void *__mutex;
    unsigned int __nwaiters;
    unsigned int __broadcast_seq;
  } __data;
  char __size[48];
  __extension__ long long int __align;
} pthread_cond_t;

typedef union
{
  char __size[4];
  int __align;
} pthread_condattr_t;



typedef unsigned int pthread_key_t;



typedef int pthread_once_t;





typedef union
{

  struct
  {
    int __lock;
    unsigned int __nr_readers;
    unsigned int __readers_wakeup;
    unsigned int __writer_wakeup;
    unsigned int __nr_readers_queued;
    unsigned int __nr_writers_queued;
    int __writer;
    int __shared;
    unsigned long int __pad1;
    unsigned long int __pad2;


    unsigned int __flags;

  } __data;
# 212 "/usr/include/bits/pthreadtypes.h" 3 4
  char __size[56];
  long int __align;
} pthread_rwlock_t;

typedef union
{
  char __size[8];
  long int __align;
} pthread_rwlockattr_t;





typedef volatile int pthread_spinlock_t;




typedef union
{
  char __size[32];
  long int __align;
} pthread_barrier_t;

typedef union
{
  char __size[4];
  int __align;
} pthread_barrierattr_t;
# 271 "/usr/include/sys/types.h" 2 3 4
# 315 "/usr/include/stdlib.h" 2 3 4






extern long int random (void) __attribute__ ((__nothrow__ ));


extern void srandom (unsigned int __seed) __attribute__ ((__nothrow__ ));





extern char *initstate (unsigned int __seed, char *__statebuf,
   size_t __statelen) __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (2)));



extern char *setstate (char *__statebuf) __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1)));







struct random_data
  {
    int32_t *fptr;
    int32_t *rptr;
    int32_t *state;
    int rand_type;
    int rand_deg;
    int rand_sep;
    int32_t *end_ptr;
  };

extern int random_r (struct random_data *__restrict __buf,
       int32_t *__restrict __result) __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1, 2)));

extern int srandom_r (unsigned int __seed, struct random_data *__buf)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (2)));

extern int initstate_r (unsigned int __seed, char *__restrict __statebuf,
   size_t __statelen,
   struct random_data *__restrict __buf)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (2, 4)));

extern int setstate_r (char *__restrict __statebuf,
         struct random_data *__restrict __buf)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1, 2)));






extern int rand (void) __attribute__ ((__nothrow__ ));

extern void srand (unsigned int __seed) __attribute__ ((__nothrow__ ));




extern int rand_r (unsigned int *__seed) __attribute__ ((__nothrow__ ));







extern double drand48 (void) __attribute__ ((__nothrow__ ));
extern double erand48 (unsigned short int __xsubi[3]) __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1)));


extern long int lrand48 (void) __attribute__ ((__nothrow__ ));
extern long int nrand48 (unsigned short int __xsubi[3])
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1)));


extern long int mrand48 (void) __attribute__ ((__nothrow__ ));
extern long int jrand48 (unsigned short int __xsubi[3])
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1)));


extern void srand48 (long int __seedval) __attribute__ ((__nothrow__ ));
extern unsigned short int *seed48 (unsigned short int __seed16v[3])
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1)));
extern void lcong48 (unsigned short int __param[7]) __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1)));





struct drand48_data
  {
    unsigned short int __x[3];
    unsigned short int __old_x[3];
    unsigned short int __c;
    unsigned short int __init;
    unsigned long long int __a;
  };


extern int drand48_r (struct drand48_data *__restrict __buffer,
        double *__restrict __result) __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1, 2)));
extern int erand48_r (unsigned short int __xsubi[3],
        struct drand48_data *__restrict __buffer,
        double *__restrict __result) __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1, 2)));


extern int lrand48_r (struct drand48_data *__restrict __buffer,
        long int *__restrict __result)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1, 2)));
extern int nrand48_r (unsigned short int __xsubi[3],
        struct drand48_data *__restrict __buffer,
        long int *__restrict __result)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1, 2)));


extern int mrand48_r (struct drand48_data *__restrict __buffer,
        long int *__restrict __result)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1, 2)));
extern int jrand48_r (unsigned short int __xsubi[3],
        struct drand48_data *__restrict __buffer,
        long int *__restrict __result)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1, 2)));


extern int srand48_r (long int __seedval, struct drand48_data *__buffer)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (2)));

extern int seed48_r (unsigned short int __seed16v[3],
       struct drand48_data *__buffer) __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1, 2)));

extern int lcong48_r (unsigned short int __param[7],
        struct drand48_data *__buffer)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1, 2)));
# 465 "/usr/include/stdlib.h" 3 4
extern void *malloc (size_t __size) __attribute__ ((__nothrow__ )) __attribute__ ((__malloc__)) ;

extern void *calloc (size_t __nmemb, size_t __size)
     __attribute__ ((__nothrow__ )) __attribute__ ((__malloc__)) ;
# 479 "/usr/include/stdlib.h" 3 4
extern void *realloc (void *__ptr, size_t __size)
     __attribute__ ((__nothrow__ )) __attribute__ ((__warn_unused_result__));

extern void free (void *__ptr) __attribute__ ((__nothrow__ ));




extern void cfree (void *__ptr) __attribute__ ((__nothrow__ ));



# 1 "/usr/include/alloca.h" 1 3 4
# 24 "/usr/include/alloca.h" 3 4
# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/stddef.h" 1 3 4
# 25 "/usr/include/alloca.h" 2 3 4







extern void *alloca (size_t __size) __attribute__ ((__nothrow__ ));
# 492 "/usr/include/stdlib.h" 2 3 4





extern void *valloc (size_t __size) __attribute__ ((__nothrow__ )) __attribute__ ((__malloc__)) ;




extern int posix_memalign (void **__memptr, size_t __alignment, size_t __size)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1))) ;




extern void *aligned_alloc (size_t __alignment, size_t __size)
     __attribute__ ((__nothrow__ )) __attribute__ ((__malloc__, __alloc_size__ (2)));




extern void abort (void) __attribute__ ((__nothrow__ )) __attribute__ ((__noreturn__));



extern int atexit (void (*__func) (void)) __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1)));







extern int at_quick_exit (void (*__func) (void)) __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1)));







extern int on_exit (void (*__func) (int __status, void *__arg), void *__arg)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1)));






extern void exit (int __status) __attribute__ ((__nothrow__ )) __attribute__ ((__noreturn__));





extern void quick_exit (int __status) __attribute__ ((__nothrow__ )) __attribute__ ((__noreturn__));







extern void _Exit (int __status) __attribute__ ((__nothrow__ )) __attribute__ ((__noreturn__));






extern char *getenv (const char *__name) __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1))) ;
# 577 "/usr/include/stdlib.h" 3 4
extern int putenv (char *__string) __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1)));





extern int setenv (const char *__name, const char *__value, int __replace)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (2)));


extern int unsetenv (const char *__name) __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1)));






extern int clearenv (void) __attribute__ ((__nothrow__ ));
# 605 "/usr/include/stdlib.h" 3 4
extern char *mktemp (char *__template) __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1)));
# 619 "/usr/include/stdlib.h" 3 4
extern int mkstemp (char *__template) __attribute__ ((__nonnull__ (1))) ;
# 641 "/usr/include/stdlib.h" 3 4
extern int mkstemps (char *__template, int __suffixlen) __attribute__ ((__nonnull__ (1))) ;
# 662 "/usr/include/stdlib.h" 3 4
extern char *mkdtemp (char *__template) __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1))) ;
# 716 "/usr/include/stdlib.h" 3 4
extern int system (const char *__command) ;
# 733 "/usr/include/stdlib.h" 3 4
extern char *realpath (const char *__restrict __name,
         char *__restrict __resolved) __attribute__ ((__nothrow__ )) ;






typedef int (*__compar_fn_t) (const void *, const void *);
# 754 "/usr/include/stdlib.h" 3 4
extern void *bsearch (const void *__key, const void *__base,
        size_t __nmemb, size_t __size, __compar_fn_t __compar)
     __attribute__ ((__nonnull__ (1, 2, 5))) ;



extern void qsort (void *__base, size_t __nmemb, size_t __size,
     __compar_fn_t __compar) __attribute__ ((__nonnull__ (1, 4)));
# 770 "/usr/include/stdlib.h" 3 4
extern int abs (int __x) __attribute__ ((__nothrow__ )) __attribute__ ((__const__)) ;
extern long int labs (long int __x) __attribute__ ((__nothrow__ )) __attribute__ ((__const__)) ;



__extension__ extern long long int llabs (long long int __x)
     __attribute__ ((__nothrow__ )) __attribute__ ((__const__)) ;







extern div_t div (int __numer, int __denom)
     __attribute__ ((__nothrow__ )) __attribute__ ((__const__)) ;
extern ldiv_t ldiv (long int __numer, long int __denom)
     __attribute__ ((__nothrow__ )) __attribute__ ((__const__)) ;




__extension__ extern lldiv_t lldiv (long long int __numer,
        long long int __denom)
     __attribute__ ((__nothrow__ )) __attribute__ ((__const__)) ;
# 807 "/usr/include/stdlib.h" 3 4
extern char *ecvt (double __value, int __ndigit, int *__restrict __decpt,
     int *__restrict __sign) __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (3, 4))) ;




extern char *fcvt (double __value, int __ndigit, int *__restrict __decpt,
     int *__restrict __sign) __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (3, 4))) ;




extern char *gcvt (double __value, int __ndigit, char *__buf)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (3))) ;




extern char *qecvt (long double __value, int __ndigit,
      int *__restrict __decpt, int *__restrict __sign)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (3, 4))) ;
extern char *qfcvt (long double __value, int __ndigit,
      int *__restrict __decpt, int *__restrict __sign)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (3, 4))) ;
extern char *qgcvt (long double __value, int __ndigit, char *__buf)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (3))) ;




extern int ecvt_r (double __value, int __ndigit, int *__restrict __decpt,
     int *__restrict __sign, char *__restrict __buf,
     size_t __len) __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (3, 4, 5)));
extern int fcvt_r (double __value, int __ndigit, int *__restrict __decpt,
     int *__restrict __sign, char *__restrict __buf,
     size_t __len) __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (3, 4, 5)));

extern int qecvt_r (long double __value, int __ndigit,
      int *__restrict __decpt, int *__restrict __sign,
      char *__restrict __buf, size_t __len)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (3, 4, 5)));
extern int qfcvt_r (long double __value, int __ndigit,
      int *__restrict __decpt, int *__restrict __sign,
      char *__restrict __buf, size_t __len)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (3, 4, 5)));







extern int mblen (const char *__s, size_t __n) __attribute__ ((__nothrow__ )) ;


extern int mbtowc (wchar_t *__restrict __pwc,
     const char *__restrict __s, size_t __n) __attribute__ ((__nothrow__ )) ;


extern int wctomb (char *__s, wchar_t __wchar) __attribute__ ((__nothrow__ )) ;



extern size_t mbstowcs (wchar_t *__restrict __pwcs,
   const char *__restrict __s, size_t __n) __attribute__ ((__nothrow__ ));

extern size_t wcstombs (char *__restrict __s,
   const wchar_t *__restrict __pwcs, size_t __n)
     __attribute__ ((__nothrow__ ));
# 884 "/usr/include/stdlib.h" 3 4
extern int rpmatch (const char *__response) __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1))) ;
# 895 "/usr/include/stdlib.h" 3 4
extern int getsubopt (char **__restrict __optionp,
        char *const *__restrict __tokens,
        char **__restrict __valuep)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1, 2, 3))) ;
# 947 "/usr/include/stdlib.h" 3 4
extern int getloadavg (double __loadavg[], int __nelem)
     __attribute__ ((__nothrow__ )) __attribute__ ((__nonnull__ (1)));


# 1 "/usr/include/bits/stdlib-float.h" 1 3 4
# 25 "/usr/include/bits/stdlib-float.h" 3 4
extern __inline __attribute__ ((__gnu_inline__)) double
__attribute__ ((__nothrow__ )) atof (const char *__nptr)
{
  return strtod (__nptr, (char **) ((void*)0));
}
# 952 "/usr/include/stdlib.h" 2 3 4
# 52 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c" 2
# 85 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/x86intrin.h" 1 3
# 27 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/x86intrin.h" 3
#pragma float_control(push)
#pragma float_control(precise, on)




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 1 3
# 27 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 3
#pragma float_control(push)
#pragma float_control(precise, on)






typedef int _tile1024i __attribute__((__vector_size__(1024), __aligned__(64)));





typedef struct __tile1024i_str {
  const unsigned short row;
  const unsigned short col;
  _tile1024i tile;
} __tile1024i;





# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/x86gprintrin.h" 1 3
# 28 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/x86gprintrin.h" 3
# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/hresetintrin.h" 1 3
# 39 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/hresetintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("hreset")))
_hreset(int __eax)
{
  __asm__ ("hreset $0" :: "a"(__eax));
}
# 29 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/x86gprintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/uintrintrin.h" 1 3
# 23 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/uintrintrin.h" 3
struct __uintr_frame
{
  unsigned long long rip;
  unsigned long long rflags;
  unsigned long long rsp;
};
# 45 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/uintrintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("uintr")))
_clui (void)
{
  __builtin_ia32_clui();
}
# 66 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/uintrintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("uintr")))
_stui (void)
{
  __builtin_ia32_stui();
}
# 93 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/uintrintrin.h" 3
static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("uintr")))
_testui (void)
{
  return __builtin_ia32_testui();
}
# 147 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/uintrintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("uintr")))
_senduipi (unsigned long long __a)
{
  __builtin_ia32_senduipi(__a);
}
# 34 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/x86gprintrin.h" 2 3





# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/crc32intrin.h" 1 3
# 30 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/crc32intrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("crc32")))
_mm_crc32_u8(unsigned int __C, unsigned char __D)
{
  return __builtin_ia32_crc32qi(__C, __D);
}
# 50 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/crc32intrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("crc32")))
_mm_crc32_u16(unsigned int __C, unsigned short __D)
{
  return __builtin_ia32_crc32hi(__C, __D);
}
# 70 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/crc32intrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("crc32")))
_mm_crc32_u32(unsigned int __C, unsigned int __D)
{
  return __builtin_ia32_crc32si(__C, __D);
}
# 91 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/crc32intrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("crc32")))
_mm_crc32_u64(unsigned long long __C, unsigned long long __D)
{
  return __builtin_ia32_crc32di(__C, __D);
}
# 40 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/x86gprintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/prfchiintrin.h" 1 3
# 31 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/prfchiintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("prefetchi")))
_m_prefetchit0(volatile const void *__P) {
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wcast-qual"
  __builtin_ia32_prefetchi((const void *)__P, 3 );
#pragma clang diagnostic pop
}
# 51 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/prfchiintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("prefetchi")))
_m_prefetchit1(volatile const void *__P) {
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wcast-qual"
  __builtin_ia32_prefetchi((const void *)__P, 2 );
#pragma clang diagnostic pop
}
# 45 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/x86gprintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/raointintrin.h" 1 3
# 38 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/raointintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("raoint"))) _aadd_i32(int *__A, int __B) {
  __builtin_ia32_aadd32((int *)__A, __B);
}
# 60 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/raointintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("raoint"))) _aand_i32(int *__A, int __B) {
  __builtin_ia32_aand32((int *)__A, __B);
}
# 82 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/raointintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("raoint"))) _aor_i32(int *__A, int __B) {
  __builtin_ia32_aor32((int *)__A, __B);
}
# 104 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/raointintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("raoint"))) _axor_i32(int *__A, int __B) {
  __builtin_ia32_axor32((int *)__A, __B);
}
# 127 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/raointintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("raoint"))) _aadd_i64(long long *__A,
                                                    long long __B) {
  __builtin_ia32_aadd64((long long *)__A, __B);
}
# 150 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/raointintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("raoint"))) _aand_i64(long long *__A,
                                                    long long __B) {
  __builtin_ia32_aand64((long long *)__A, __B);
}
# 173 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/raointintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("raoint"))) _aor_i64(long long *__A,
                                                   long long __B) {
  __builtin_ia32_aor64((long long *)__A, __B);
}
# 196 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/raointintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("raoint"))) _axor_i64(long long *__A,
                                                    long long __B) {
  __builtin_ia32_axor64((long long *)__A, __B);
}
# 50 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/x86gprintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/cmpccxaddintrin.h" 1 3
# 19 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/cmpccxaddintrin.h" 3
typedef enum {
  _CMPCCX_O,
  _CMPCCX_NO,
  _CMPCCX_B,
  _CMPCCX_NB,
  _CMPCCX_Z,
  _CMPCCX_NZ,
  _CMPCCX_BE,
  _CMPCCX_NBE,
  _CMPCCX_S,
  _CMPCCX_NS,
  _CMPCCX_P,
  _CMPCCX_NP,
  _CMPCCX_L,
  _CMPCCX_NL,
  _CMPCCX_LE,
  _CMPCCX_NLE,
} _CMPCCX_ENUM;
# 55 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/x86gprintrin.h" 2 3
# 52 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3

# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ia32intrin.h" 1 3
# 13 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ia32intrin.h" 3
# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 1 3
# 14 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ia32intrin.h" 2 3
# 39 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ia32intrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__))
__bsfd(int __A) {
  return __builtin_ctz((unsigned int)__A);
}
# 56 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ia32intrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__))
__bsrd(int __A) {
  return 31 - __builtin_clz((unsigned int)__A);
}
# 72 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ia32intrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__))
__bswapd(int __A) {
  return (int)__builtin_bswap32((unsigned int)__A);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__))
_bswap(int __A) {
  return (int)__builtin_bswap32((unsigned int)__A);
}
# 98 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ia32intrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__))
__bsfq(long long __A) {
  return (long long)__builtin_ctzll((unsigned long long)__A);
}
# 115 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ia32intrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__))
__bsrq(long long __A) {
  return 63 - __builtin_clzll((unsigned long long)__A);
}
# 131 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ia32intrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__))
__bswapq(long long __A) {
  return (long long)__builtin_bswap64((unsigned long long)__A);
}
# 151 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ia32intrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__))
__popcntd(unsigned int __A)
{
  return __builtin_popcount(__A);
}
# 172 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ia32intrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__))
__popcntq(unsigned long long __A)
{
  return __builtin_popcountll(__A);
}





static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__readeflags(void)
{
  return __builtin_ia32_readeflags_u64();
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
__writeeflags(unsigned long long __f)
{
  __builtin_ia32_writeeflags_u64(__f);
}
# 218 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ia32intrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__))
_castf32_u32(float __A) {
  return __builtin_bit_cast(unsigned int, __A);
}
# 233 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ia32intrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__))
_castf64_u64(double __A) {
  return __builtin_bit_cast(unsigned long long, __A);
}
# 248 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ia32intrin.h" 3
static __inline__ float __attribute__((__always_inline__))
_castu32_f32(unsigned int __A) {
  return __builtin_bit_cast(float, __A);
}
# 263 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ia32intrin.h" 3
static __inline__ double __attribute__((__always_inline__))
_castu64_f64(unsigned long long __A) {
  return __builtin_bit_cast(double, __A);
}
# 283 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ia32intrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("crc32")))
__crc32b(unsigned int __C, unsigned char __D)
{
  return __builtin_ia32_crc32qi(__C, __D);
}
# 304 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ia32intrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("crc32")))
__crc32w(unsigned int __C, unsigned short __D)
{
  return __builtin_ia32_crc32hi(__C, __D);
}
# 325 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ia32intrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("crc32")))
__crc32d(unsigned int __C, unsigned int __D)
{
  return __builtin_ia32_crc32si(__C, __D);
}
# 347 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ia32intrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("crc32")))
__crc32q(unsigned long long __C, unsigned long long __D)
{
  return __builtin_ia32_crc32di(__C, __D);
}


static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__rdpmc(int __A) {
  return __builtin_ia32_rdpmc(__A);
}


static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__rdtscp(unsigned int *__A) {
  return __builtin_ia32_rdtscp(__A);
}





static __inline__ void __attribute__((__always_inline__, __nodebug__))
_wbinvd(void) {
  __builtin_ia32_wbinvd();
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__))
__rolb(unsigned char __X, int __C) {
  return __builtin_rotateleft8(__X, __C);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__))
__rorb(unsigned char __X, int __C) {
  return __builtin_rotateright8(__X, __C);
}

static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__))
__rolw(unsigned short __X, int __C) {
  return __builtin_rotateleft16(__X, __C);
}

static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__))
__rorw(unsigned short __X, int __C) {
  return __builtin_rotateright16(__X, __C);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__))
__rold(unsigned int __X, int __C) {
  return __builtin_rotateleft32(__X, (unsigned int)__C);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__))
__rord(unsigned int __X, int __C) {
  return __builtin_rotateright32(__X, (unsigned int)__C);
}


static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__rolq(unsigned long long __X, int __C) {
  return __builtin_rotateleft64(__X, (unsigned long long)__C);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__rorq(unsigned long long __X, int __C) {
  return __builtin_rotateright64(__X, (unsigned long long)__C);
}
# 54 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3



# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 1 3
# 17 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
typedef long long __m64 __attribute__((__vector_size__(8), __aligned__(8)));

typedef long long __v1di __attribute__((__vector_size__(8)));
typedef int __v2si __attribute__((__vector_size__(8)));
typedef short __v4hi __attribute__((__vector_size__(8)));
typedef char __v8qi __attribute__((__vector_size__(8)));
# 34 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("mmx")))
_mm_empty(void)
{
    __builtin_ia32_emms();
}
# 51 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_cvtsi32_si64(int __i)
{
    return (__m64)__builtin_ia32_vec_init_v2si(__i, 0);
}
# 68 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_cvtsi64_si32(__m64 __m)
{
    return __builtin_ia32_vec_ext_v2si((__v2si)__m, 0);
}
# 84 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_cvtsi64_m64(long long __i)
{
    return (__m64)__i;
}
# 100 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_cvtm64_si64(__m64 __m)
{
    return (long long)__m;
}
# 130 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_packs_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_packsswb((__v4hi)__m1, (__v4hi)__m2);
}
# 160 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_packs_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_packssdw((__v2si)__m1, (__v2si)__m2);
}
# 190 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_packs_pu16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_packuswb((__v4hi)__m1, (__v4hi)__m2);
}
# 217 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_unpackhi_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_punpckhbw((__v8qi)__m1, (__v8qi)__m2);
}
# 240 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_unpackhi_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_punpckhwd((__v4hi)__m1, (__v4hi)__m2);
}
# 261 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_unpackhi_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_punpckhdq((__v2si)__m1, (__v2si)__m2);
}
# 288 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_unpacklo_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_punpcklbw((__v8qi)__m1, (__v8qi)__m2);
}
# 311 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_unpacklo_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_punpcklwd((__v4hi)__m1, (__v4hi)__m2);
}
# 332 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_unpacklo_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_punpckldq((__v2si)__m1, (__v2si)__m2);
}
# 353 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_add_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddb((__v8qi)__m1, (__v8qi)__m2);
}
# 374 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_add_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddw((__v4hi)__m1, (__v4hi)__m2);
}
# 395 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_add_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddd((__v2si)__m1, (__v2si)__m2);
}
# 417 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_adds_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddsb((__v8qi)__m1, (__v8qi)__m2);
}
# 440 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_adds_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddsw((__v4hi)__m1, (__v4hi)__m2);
}
# 462 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_adds_pu8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddusb((__v8qi)__m1, (__v8qi)__m2);
}
# 484 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_adds_pu16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_paddusw((__v4hi)__m1, (__v4hi)__m2);
}
# 505 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_sub_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubb((__v8qi)__m1, (__v8qi)__m2);
}
# 526 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_sub_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubw((__v4hi)__m1, (__v4hi)__m2);
}
# 547 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_sub_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubd((__v2si)__m1, (__v2si)__m2);
}
# 570 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_subs_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubsb((__v8qi)__m1, (__v8qi)__m2);
}
# 593 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_subs_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubsw((__v4hi)__m1, (__v4hi)__m2);
}
# 617 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_subs_pu8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubusb((__v8qi)__m1, (__v8qi)__m2);
}
# 641 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_subs_pu16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_psubusw((__v4hi)__m1, (__v4hi)__m2);
}
# 668 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_madd_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pmaddwd((__v4hi)__m1, (__v4hi)__m2);
}
# 689 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_mulhi_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pmulhw((__v4hi)__m1, (__v4hi)__m2);
}
# 710 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_mullo_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pmullw((__v4hi)__m1, (__v4hi)__m2);
}
# 733 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_sll_pi16(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psllw((__v4hi)__m, __count);
}
# 755 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_slli_pi16(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psllwi((__v4hi)__m, __count);
}
# 778 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_sll_pi32(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_pslld((__v2si)__m, __count);
}
# 800 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_slli_pi32(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_pslldi((__v2si)__m, __count);
}
# 820 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_sll_si64(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psllq((__v1di)__m, __count);
}
# 840 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_slli_si64(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psllqi((__v1di)__m, __count);
}
# 864 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_sra_pi16(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psraw((__v4hi)__m, __count);
}
# 887 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_srai_pi16(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psrawi((__v4hi)__m, __count);
}
# 911 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_sra_pi32(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psrad((__v2si)__m, __count);
}
# 934 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_srai_pi32(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psradi((__v2si)__m, __count);
}
# 957 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_srl_pi16(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psrlw((__v4hi)__m, __count);
}
# 979 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_srli_pi16(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psrlwi((__v4hi)__m, __count);
}
# 1002 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_srl_pi32(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psrld((__v2si)__m, __count);
}
# 1024 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_srli_pi32(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psrldi((__v2si)__m, __count);
}
# 1044 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_srl_si64(__m64 __m, __m64 __count)
{
    return (__m64)__builtin_ia32_psrlq((__v1di)__m, __count);
}
# 1065 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_srli_si64(__m64 __m, int __count)
{
    return (__m64)__builtin_ia32_psrlqi((__v1di)__m, __count);
}
# 1083 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_and_si64(__m64 __m1, __m64 __m2)
{
    return __builtin_ia32_pand((__v1di)__m1, (__v1di)__m2);
}
# 1104 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_andnot_si64(__m64 __m1, __m64 __m2)
{
    return __builtin_ia32_pandn((__v1di)__m1, (__v1di)__m2);
}
# 1122 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_or_si64(__m64 __m1, __m64 __m2)
{
    return __builtin_ia32_por((__v1di)__m1, (__v1di)__m2);
}
# 1140 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_xor_si64(__m64 __m1, __m64 __m2)
{
    return __builtin_ia32_pxor((__v1di)__m1, (__v1di)__m2);
}
# 1162 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_cmpeq_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pcmpeqb((__v8qi)__m1, (__v8qi)__m2);
}
# 1184 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_cmpeq_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pcmpeqw((__v4hi)__m1, (__v4hi)__m2);
}
# 1206 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_cmpeq_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pcmpeqd((__v2si)__m1, (__v2si)__m2);
}
# 1228 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_cmpgt_pi8(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pcmpgtb((__v8qi)__m1, (__v8qi)__m2);
}
# 1250 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_cmpgt_pi16(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pcmpgtw((__v4hi)__m1, (__v4hi)__m2);
}
# 1272 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_cmpgt_pi32(__m64 __m1, __m64 __m2)
{
    return (__m64)__builtin_ia32_pcmpgtd((__v2si)__m1, (__v2si)__m2);
}
# 1285 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_setzero_si64(void)
{
    return __extension__ (__m64){ 0LL };
}
# 1306 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_set_pi32(int __i1, int __i0)
{
    return (__m64)__builtin_ia32_vec_init_v2si(__i0, __i1);
}
# 1329 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_set_pi16(short __s3, short __s2, short __s1, short __s0)
{
    return (__m64)__builtin_ia32_vec_init_v4hi(__s0, __s1, __s2, __s3);
}
# 1360 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_set_pi8(char __b7, char __b6, char __b5, char __b4, char __b3, char __b2,
            char __b1, char __b0)
{
    return (__m64)__builtin_ia32_vec_init_v8qi(__b0, __b1, __b2, __b3,
                                               __b4, __b5, __b6, __b7);
}
# 1381 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_set1_pi32(int __i)
{
    return _mm_set_pi32(__i, __i);
}
# 1400 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_set1_pi16(short __w)
{
    return _mm_set_pi16(__w, __w, __w, __w);
}
# 1418 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_set1_pi8(char __b)
{
    return _mm_set_pi8(__b, __b, __b, __b, __b, __b, __b, __b);
}
# 1439 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_setr_pi32(int __i0, int __i1)
{
    return _mm_set_pi32(__i1, __i0);
}
# 1462 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_setr_pi16(short __w0, short __w1, short __w2, short __w3)
{
    return _mm_set_pi16(__w3, __w2, __w1, __w0);
}
# 1493 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx"), __min_vector_width__(64)))
_mm_setr_pi8(char __b0, char __b1, char __b2, char __b3, char __b4, char __b5,
             char __b6, char __b7)
{
    return _mm_set_pi8(__b7, __b6, __b5, __b4, __b3, __b2, __b1, __b0);
}
# 58 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 1 3
# 27 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
#pragma float_control(push)
#pragma float_control(precise, on)







typedef int __v4si __attribute__((__vector_size__(16)));
typedef float __v4sf __attribute__((__vector_size__(16)));
typedef float __m128 __attribute__((__vector_size__(16), __aligned__(16)));

typedef float __m128_u __attribute__((__vector_size__(16), __aligned__(1)));


typedef unsigned int __v4su __attribute__((__vector_size__(16)));




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mm_malloc.h" 1 3
# 19 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mm_malloc.h" 3
extern int posix_memalign(void **__memptr, size_t __alignment, size_t __size);
# 30 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mm_malloc.h" 3
static __inline__ void *__attribute__((__always_inline__, __nodebug__,
                                       __malloc__, __alloc_size__(1),
                                       __alloc_align__(2)))
_mm_malloc(size_t __size, size_t __align) {
  if (__align == 1) {
    return malloc(__size);
  }

  if (!(__align & (__align - 1)) && __align < sizeof(void *))
    __align = sizeof(void *);

  void *__mallocedMemory;





  if (posix_memalign(&__mallocedMemory, __align, __size))
    return 0;


  return __mallocedMemory;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_mm_free(void *__p)
{





  free(__p);

}
# 49 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 2 3
# 70 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_add_ss(__m128 __a, __m128 __b)
{
  __a[0] += __b[0];
  return __a;
}
# 90 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_add_ps(__m128 __a, __m128 __b)
{
  return (__m128)((__v4sf)__a + (__v4sf)__b);
}
# 112 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_sub_ss(__m128 __a, __m128 __b)
{
  __a[0] -= __b[0];
  return __a;
}
# 133 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_sub_ps(__m128 __a, __m128 __b)
{
  return (__m128)((__v4sf)__a - (__v4sf)__b);
}
# 155 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_mul_ss(__m128 __a, __m128 __b)
{
  __a[0] *= __b[0];
  return __a;
}
# 175 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_mul_ps(__m128 __a, __m128 __b)
{
  return (__m128)((__v4sf)__a * (__v4sf)__b);
}
# 197 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_div_ss(__m128 __a, __m128 __b)
{
  __a[0] /= __b[0];
  return __a;
}
# 216 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_div_ps(__m128 __a, __m128 __b)
{
  return (__m128)((__v4sf)__a / (__v4sf)__b);
}
# 234 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_sqrt_ss(__m128 __a)
{
  return (__m128)__builtin_ia32_sqrtss((__v4sf)__a);
}
# 251 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_sqrt_ps(__m128 __a)
{
  return __builtin_ia32_sqrtps((__v4sf)__a);
}
# 269 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_rcp_ss(__m128 __a)
{
  return (__m128)__builtin_ia32_rcpss((__v4sf)__a);
}
# 286 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_rcp_ps(__m128 __a)
{
  return (__m128)__builtin_ia32_rcpps((__v4sf)__a);
}
# 305 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_rsqrt_ss(__m128 __a)
{
  return __builtin_ia32_rsqrtss((__v4sf)__a);
}
# 322 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_rsqrt_ps(__m128 __a)
{
  return __builtin_ia32_rsqrtps((__v4sf)__a);
}
# 345 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_min_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_minss((__v4sf)__a, (__v4sf)__b);
}
# 364 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_min_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_minps((__v4sf)__a, (__v4sf)__b);
}
# 387 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_max_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_maxss((__v4sf)__a, (__v4sf)__b);
}
# 406 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_max_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_maxps((__v4sf)__a, (__v4sf)__b);
}
# 424 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_and_ps(__m128 __a, __m128 __b)
{
  return (__m128)((__v4su)__a & (__v4su)__b);
}
# 446 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_andnot_ps(__m128 __a, __m128 __b)
{
  return (__m128)(~(__v4su)__a & (__v4su)__b);
}
# 464 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_or_ps(__m128 __a, __m128 __b)
{
  return (__m128)((__v4su)__a | (__v4su)__b);
}
# 483 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_xor_ps(__m128 __a, __m128 __b)
{
  return (__m128)((__v4su)__a ^ (__v4su)__b);
}
# 505 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cmpeq_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpeqss((__v4sf)__a, (__v4sf)__b);
}
# 523 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cmpeq_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpeqps((__v4sf)__a, (__v4sf)__b);
}
# 546 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cmplt_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpltss((__v4sf)__a, (__v4sf)__b);
}
# 565 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cmplt_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpltps((__v4sf)__a, (__v4sf)__b);
}
# 589 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cmple_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpless((__v4sf)__a, (__v4sf)__b);
}
# 608 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cmple_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpleps((__v4sf)__a, (__v4sf)__b);
}
# 631 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cmpgt_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_shufflevector((__v4sf)__a,
                                         (__v4sf)__builtin_ia32_cmpltss((__v4sf)__b, (__v4sf)__a),
                                         4, 1, 2, 3);
}
# 652 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cmpgt_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpltps((__v4sf)__b, (__v4sf)__a);
}
# 676 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cmpge_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_shufflevector((__v4sf)__a,
                                         (__v4sf)__builtin_ia32_cmpless((__v4sf)__b, (__v4sf)__a),
                                         4, 1, 2, 3);
}
# 697 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cmpge_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpleps((__v4sf)__b, (__v4sf)__a);
}
# 720 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cmpneq_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpneqss((__v4sf)__a, (__v4sf)__b);
}
# 739 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cmpneq_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpneqps((__v4sf)__a, (__v4sf)__b);
}
# 763 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cmpnlt_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpnltss((__v4sf)__a, (__v4sf)__b);
}
# 783 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cmpnlt_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpnltps((__v4sf)__a, (__v4sf)__b);
}
# 808 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cmpnle_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpnless((__v4sf)__a, (__v4sf)__b);
}
# 828 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cmpnle_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpnleps((__v4sf)__a, (__v4sf)__b);
}
# 853 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cmpngt_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_shufflevector((__v4sf)__a,
                                         (__v4sf)__builtin_ia32_cmpnltss((__v4sf)__b, (__v4sf)__a),
                                         4, 1, 2, 3);
}
# 875 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cmpngt_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpnltps((__v4sf)__b, (__v4sf)__a);
}
# 900 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cmpnge_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_shufflevector((__v4sf)__a,
                                         (__v4sf)__builtin_ia32_cmpnless((__v4sf)__b, (__v4sf)__a),
                                         4, 1, 2, 3);
}
# 922 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cmpnge_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpnleps((__v4sf)__b, (__v4sf)__a);
}
# 947 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cmpord_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpordss((__v4sf)__a, (__v4sf)__b);
}
# 967 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cmpord_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpordps((__v4sf)__a, (__v4sf)__b);
}
# 992 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cmpunord_ss(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpunordss((__v4sf)__a, (__v4sf)__b);
}
# 1012 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cmpunord_ps(__m128 __a, __m128 __b)
{
  return (__m128)__builtin_ia32_cmpunordps((__v4sf)__a, (__v4sf)__b);
}
# 1036 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_comieq_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_comieq((__v4sf)__a, (__v4sf)__b);
}
# 1061 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_comilt_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_comilt((__v4sf)__a, (__v4sf)__b);
}
# 1085 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_comile_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_comile((__v4sf)__a, (__v4sf)__b);
}
# 1109 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_comigt_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_comigt((__v4sf)__a, (__v4sf)__b);
}
# 1133 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_comige_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_comige((__v4sf)__a, (__v4sf)__b);
}
# 1157 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_comineq_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_comineq((__v4sf)__a, (__v4sf)__b);
}
# 1181 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_ucomieq_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_ucomieq((__v4sf)__a, (__v4sf)__b);
}
# 1205 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_ucomilt_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_ucomilt((__v4sf)__a, (__v4sf)__b);
}
# 1230 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_ucomile_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_ucomile((__v4sf)__a, (__v4sf)__b);
}
# 1255 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_ucomigt_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_ucomigt((__v4sf)__a, (__v4sf)__b);
}
# 1280 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_ucomige_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_ucomige((__v4sf)__a, (__v4sf)__b);
}
# 1304 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_ucomineq_ss(__m128 __a, __m128 __b)
{
  return __builtin_ia32_ucomineq((__v4sf)__a, (__v4sf)__b);
}
# 1322 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cvtss_si32(__m128 __a)
{
  return __builtin_ia32_cvtss2si((__v4sf)__a);
}
# 1340 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cvt_ss2si(__m128 __a)
{
  return _mm_cvtss_si32(__a);
}
# 1360 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cvtss_si64(__m128 __a)
{
  return __builtin_ia32_cvtss2si64((__v4sf)__a);
}
# 1378 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse"), __min_vector_width__(64)))
_mm_cvtps_pi32(__m128 __a)
{
  return (__m64)__builtin_ia32_cvtps2pi((__v4sf)__a);
}
# 1394 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse"), __min_vector_width__(64)))
_mm_cvt_ps2pi(__m128 __a)
{
  return _mm_cvtps_pi32(__a);
}
# 1413 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cvttss_si32(__m128 __a)
{
  return __builtin_ia32_cvttss2si((__v4sf)__a);
}
# 1432 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cvtt_ss2si(__m128 __a)
{
  return _mm_cvttss_si32(__a);
}
# 1452 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cvttss_si64(__m128 __a)
{
  return __builtin_ia32_cvttss2si64((__v4sf)__a);
}
# 1471 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse"), __min_vector_width__(64)))
_mm_cvttps_pi32(__m128 __a)
{
  return (__m64)__builtin_ia32_cvttps2pi((__v4sf)__a);
}
# 1488 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse"), __min_vector_width__(64)))
_mm_cvtt_ps2pi(__m128 __a)
{
  return _mm_cvttps_pi32(__a);
}
# 1510 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cvtsi32_ss(__m128 __a, int __b)
{
  __a[0] = __b;
  return __a;
}
# 1533 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cvt_si2ss(__m128 __a, int __b)
{
  return _mm_cvtsi32_ss(__a, __b);
}
# 1557 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cvtsi64_ss(__m128 __a, long long __b)
{
  __a[0] = __b;
  return __a;
}
# 1583 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse"), __min_vector_width__(64)))
_mm_cvtpi32_ps(__m128 __a, __m64 __b)
{
  return __builtin_ia32_cvtpi2ps((__v4sf)__a, (__v2si)__b);
}
# 1606 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse"), __min_vector_width__(64)))
_mm_cvt_pi2ps(__m128 __a, __m64 __b)
{
  return _mm_cvtpi32_ps(__a, __b);
}
# 1623 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_cvtss_f32(__m128 __a)
{
  return __a[0];
}
# 1644 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_loadh_pi(__m128 __a, const __m64 *__p)
{
  typedef float __mm_loadh_pi_v2f32 __attribute__((__vector_size__(8)));
  struct __mm_loadh_pi_struct {
    __mm_loadh_pi_v2f32 __u;
  } __attribute__((__packed__, __may_alias__));
  __mm_loadh_pi_v2f32 __b = ((const struct __mm_loadh_pi_struct*)__p)->__u;
  __m128 __bb = __builtin_shufflevector(__b, __b, 0, 1, 0, 1);
  return __builtin_shufflevector(__a, __bb, 0, 1, 4, 5);
}
# 1671 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_loadl_pi(__m128 __a, const __m64 *__p)
{
  typedef float __mm_loadl_pi_v2f32 __attribute__((__vector_size__(8)));
  struct __mm_loadl_pi_struct {
    __mm_loadl_pi_v2f32 __u;
  } __attribute__((__packed__, __may_alias__));
  __mm_loadl_pi_v2f32 __b = ((const struct __mm_loadl_pi_struct*)__p)->__u;
  __m128 __bb = __builtin_shufflevector(__b, __b, 0, 1, 0, 1);
  return __builtin_shufflevector(__a, __bb, 4, 5, 2, 3);
}
# 1698 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_load_ss(const float *__p)
{
  struct __mm_load_ss_struct {
    float __u;
  } __attribute__((__packed__, __may_alias__));
  float __u = ((const struct __mm_load_ss_struct*)__p)->__u;
  return __extension__ (__m128){ __u, 0, 0, 0 };
}
# 1720 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_load1_ps(const float *__p)
{
  struct __mm_load1_ps_struct {
    float __u;
  } __attribute__((__packed__, __may_alias__));
  float __u = ((const struct __mm_load1_ps_struct*)__p)->__u;
  return __extension__ (__m128){ __u, __u, __u, __u };
}
# 1743 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_load_ps(const float *__p)
{
  return *(const __m128*)__p;
}
# 1760 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_loadu_ps(const float *__p)
{
  struct __loadu_ps {
    __m128_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_ps*)__p)->__v;
}
# 1782 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_loadr_ps(const float *__p)
{
  __m128 __a = _mm_load_ps(__p);
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 3, 2, 1, 0);
}
# 1796 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_undefined_ps(void)
{
  return (__m128)__builtin_ia32_undef128();
}
# 1816 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_set_ss(float __w)
{
  return __extension__ (__m128){ __w, 0, 0, 0 };
}
# 1834 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_set1_ps(float __w)
{
  return __extension__ (__m128){ __w, __w, __w, __w };
}
# 1853 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_set_ps1(float __w)
{
    return _mm_set1_ps(__w);
}
# 1880 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_set_ps(float __z, float __y, float __x, float __w)
{
  return __extension__ (__m128){ __w, __x, __y, __z };
}
# 1908 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_setr_ps(float __z, float __y, float __x, float __w)
{
  return __extension__ (__m128){ __z, __y, __x, __w };
}
# 1923 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_setzero_ps(void)
{
  return __extension__ (__m128){ 0.0f, 0.0f, 0.0f, 0.0f };
}
# 1940 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_storeh_pi(__m64 *__p, __m128 __a)
{
  typedef float __mm_storeh_pi_v2f32 __attribute__((__vector_size__(8)));
  struct __mm_storeh_pi_struct {
    __mm_storeh_pi_v2f32 __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_storeh_pi_struct*)__p)->__u = __builtin_shufflevector(__a, __a, 2, 3);
}
# 1961 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_storel_pi(__m64 *__p, __m128 __a)
{
  typedef float __mm_storeh_pi_v2f32 __attribute__((__vector_size__(8)));
  struct __mm_storeh_pi_struct {
    __mm_storeh_pi_v2f32 __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_storeh_pi_struct*)__p)->__u = __builtin_shufflevector(__a, __a, 0, 1);
}
# 1982 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_store_ss(float *__p, __m128 __a)
{
  struct __mm_store_ss_struct {
    float __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_store_ss_struct*)__p)->__u = __a[0];
}
# 2003 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_storeu_ps(float *__p, __m128 __a)
{
  struct __storeu_ps {
    __m128_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_ps*)__p)->__v = __a;
}
# 2024 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_store_ps(float *__p, __m128 __a)
{
  *(__m128*)__p = __a;
}
# 2043 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_store1_ps(float *__p, __m128 __a)
{
  __a = __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 0, 0, 0, 0);
  _mm_store_ps(__p, __a);
}
# 2063 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_store_ps1(float *__p, __m128 __a)
{
  _mm_store1_ps(__p, __a);
}
# 2082 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_storer_ps(float *__p, __m128 __a)
{
  __a = __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 3, 2, 1, 0);
  _mm_store_ps(__p, __a);
}
# 2140 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse"), __min_vector_width__(64)))
_mm_stream_pi(__m64 *__p, __m64 __a)
{
  __builtin_ia32_movntq(__p, __a);
}
# 2159 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_stream_ps(float *__p, __m128 __a)
{
  __builtin_nontemporal_store((__v4sf)__a, (__v4sf*)__p);
}
# 2178 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
void _mm_sfence(void);
# 2251 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse"), __min_vector_width__(64)))
_mm_max_pi16(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_pmaxsw((__v4hi)__a, (__v4hi)__b);
}
# 2270 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse"), __min_vector_width__(64)))
_mm_max_pu8(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_pmaxub((__v8qi)__a, (__v8qi)__b);
}
# 2289 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse"), __min_vector_width__(64)))
_mm_min_pi16(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_pminsw((__v4hi)__a, (__v4hi)__b);
}
# 2308 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse"), __min_vector_width__(64)))
_mm_min_pu8(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_pminub((__v8qi)__a, (__v8qi)__b);
}
# 2326 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse"), __min_vector_width__(64)))
_mm_movemask_pi8(__m64 __a)
{
  return __builtin_ia32_pmovmskb((__v8qi)__a);
}
# 2345 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse"), __min_vector_width__(64)))
_mm_mulhi_pu16(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_pmulhuw((__v4hi)__a, (__v4hi)__b);
}
# 2411 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse"), __min_vector_width__(64)))
_mm_maskmove_si64(__m64 __d, __m64 __n, char *__p)
{
  __builtin_ia32_maskmovq((__v8qi)__d, (__v8qi)__n, __p);
}
# 2430 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse"), __min_vector_width__(64)))
_mm_avg_pu8(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_pavgb((__v8qi)__a, (__v8qi)__b);
}
# 2449 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse"), __min_vector_width__(64)))
_mm_avg_pu16(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_pavgw((__v4hi)__a, (__v4hi)__b);
}
# 2471 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse"), __min_vector_width__(64)))
_mm_sad_pu8(__m64 __a, __m64 __b)
{
  return (__m64)__builtin_ia32_psadbw((__v8qi)__a, (__v8qi)__b);
}
# 2531 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
unsigned int _mm_getcsr(void);
# 2585 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
void _mm_setcsr(unsigned int __i);
# 2650 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_unpackhi_ps(__m128 __a, __m128 __b)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__b, 2, 6, 3, 7);
}
# 2672 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_unpacklo_ps(__m128 __a, __m128 __b)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__b, 0, 4, 1, 5);
}
# 2694 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_move_ss(__m128 __a, __m128 __b)
{
  __a[0] = __b[0];
  return __a;
}
# 2716 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_movehl_ps(__m128 __a, __m128 __b)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__b, 6, 7, 2, 3);
}
# 2737 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_movelh_ps(__m128 __a, __m128 __b)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__b, 0, 1, 4, 5);
}
# 2755 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse"), __min_vector_width__(64)))
_mm_cvtpi16_ps(__m64 __a)
{
  __m64 __b, __c;
  __m128 __r;

  __b = _mm_setzero_si64();
  __b = _mm_cmpgt_pi16(__b, __a);
  __c = _mm_unpackhi_pi16(__a, __b);
  __r = _mm_setzero_ps();
  __r = _mm_cvtpi32_ps(__r, __c);
  __r = _mm_movelh_ps(__r, __r);
  __c = _mm_unpacklo_pi16(__a, __b);
  __r = _mm_cvtpi32_ps(__r, __c);

  return __r;
}
# 2785 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse"), __min_vector_width__(64)))
_mm_cvtpu16_ps(__m64 __a)
{
  __m64 __b, __c;
  __m128 __r;

  __b = _mm_setzero_si64();
  __c = _mm_unpackhi_pi16(__a, __b);
  __r = _mm_setzero_ps();
  __r = _mm_cvtpi32_ps(__r, __c);
  __r = _mm_movelh_ps(__r, __r);
  __c = _mm_unpacklo_pi16(__a, __b);
  __r = _mm_cvtpi32_ps(__r, __c);

  return __r;
}
# 2814 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse"), __min_vector_width__(64)))
_mm_cvtpi8_ps(__m64 __a)
{
  __m64 __b;

  __b = _mm_setzero_si64();
  __b = _mm_cmpgt_pi8(__b, __a);
  __b = _mm_unpacklo_pi8(__a, __b);

  return _mm_cvtpi16_ps(__b);
}
# 2839 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse"), __min_vector_width__(64)))
_mm_cvtpu8_ps(__m64 __a)
{
  __m64 __b;

  __b = _mm_setzero_si64();
  __b = _mm_unpacklo_pi8(__a, __b);

  return _mm_cvtpi16_ps(__b);
}
# 2866 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse"), __min_vector_width__(64)))
_mm_cvtpi32x2_ps(__m64 __a, __m64 __b)
{
  __m128 __c;

  __c = _mm_setzero_ps();
  __c = _mm_cvtpi32_ps(__c, __b);
  __c = _mm_movelh_ps(__c, __c);

  return _mm_cvtpi32_ps(__c, __a);
}
# 2895 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse"), __min_vector_width__(64)))
_mm_cvtps_pi16(__m128 __a)
{
  __m64 __b, __c;

  __b = _mm_cvtps_pi32(__a);
  __a = _mm_movehl_ps(__a, __a);
  __c = _mm_cvtps_pi32(__a);

  return _mm_packs_pi32(__b, __c);
}
# 2925 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse"), __min_vector_width__(64)))
_mm_cvtps_pi8(__m128 __a)
{
  __m64 __b, __c;

  __b = _mm_cvtps_pi16(__a);
  __c = _mm_setzero_si64();

  return _mm_packs_pi16(__b, __c);
}
# 2950 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse"), __min_vector_width__(128)))
_mm_movemask_ps(__m128 __a)
{
  return __builtin_ia32_movmskps((__v4sf)__a);
}
# 3031 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 3
# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 1 3
# 27 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
#pragma float_control(push)
#pragma float_control(precise, on)





# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 1 3
# 35 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 2 3

typedef double __m128d __attribute__((__vector_size__(16), __aligned__(16)));
typedef long long __m128i __attribute__((__vector_size__(16), __aligned__(16)));

typedef double __m128d_u __attribute__((__vector_size__(16), __aligned__(1)));
typedef long long __m128i_u
    __attribute__((__vector_size__(16), __aligned__(1)));


typedef double __v2df __attribute__((__vector_size__(16)));
typedef long long __v2di __attribute__((__vector_size__(16)));
typedef short __v8hi __attribute__((__vector_size__(16)));
typedef char __v16qi __attribute__((__vector_size__(16)));


typedef unsigned long long __v2du __attribute__((__vector_size__(16)));
typedef unsigned short __v8hu __attribute__((__vector_size__(16)));
typedef unsigned char __v16qu __attribute__((__vector_size__(16)));



typedef signed char __v16qs __attribute__((__vector_size__(16)));



typedef _Float16 __v8hf __attribute__((__vector_size__(16), __aligned__(16)));
typedef _Float16 __m128h __attribute__((__vector_size__(16), __aligned__(16)));
typedef _Float16 __m128h_u __attribute__((__vector_size__(16), __aligned__(1)));

typedef __bf16 __v8bf __attribute__((__vector_size__(16), __aligned__(16)));
typedef __bf16 __m128bh __attribute__((__vector_size__(16), __aligned__(16)));
# 91 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_add_sd(__m128d __a,
                                                        __m128d __b) {
  __a[0] += __b[0];
  return __a;
}
# 109 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_add_pd(__m128d __a,
                                                        __m128d __b) {
  return (__m128d)((__v2df)__a + (__v2df)__b);
}
# 131 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_sub_sd(__m128d __a,
                                                        __m128d __b) {
  __a[0] -= __b[0];
  return __a;
}
# 149 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_sub_pd(__m128d __a,
                                                        __m128d __b) {
  return (__m128d)((__v2df)__a - (__v2df)__b);
}
# 170 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_mul_sd(__m128d __a,
                                                        __m128d __b) {
  __a[0] *= __b[0];
  return __a;
}
# 188 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_mul_pd(__m128d __a,
                                                        __m128d __b) {
  return (__m128d)((__v2df)__a * (__v2df)__b);
}
# 210 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_div_sd(__m128d __a,
                                                        __m128d __b) {
  __a[0] /= __b[0];
  return __a;
}
# 229 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_div_pd(__m128d __a,
                                                        __m128d __b) {
  return (__m128d)((__v2df)__a / (__v2df)__b);
}
# 253 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_sqrt_sd(__m128d __a,
                                                         __m128d __b) {
  __m128d __c = __builtin_ia32_sqrtsd((__v2df)__b);
  return __extension__(__m128d){__c[0], __a[1]};
}
# 270 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_sqrt_pd(__m128d __a) {
  return __builtin_ia32_sqrtpd((__v2df)__a);
}
# 292 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_min_sd(__m128d __a,
                                                        __m128d __b) {
  return __builtin_ia32_minsd((__v2df)__a, (__v2df)__b);
}
# 311 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_min_pd(__m128d __a,
                                                        __m128d __b) {
  return __builtin_ia32_minpd((__v2df)__a, (__v2df)__b);
}
# 334 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_max_sd(__m128d __a,
                                                        __m128d __b) {
  return __builtin_ia32_maxsd((__v2df)__a, (__v2df)__b);
}
# 353 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_max_pd(__m128d __a,
                                                        __m128d __b) {
  return __builtin_ia32_maxpd((__v2df)__a, (__v2df)__b);
}
# 370 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_and_pd(__m128d __a,
                                                        __m128d __b) {
  return (__m128d)((__v2du)__a & (__v2du)__b);
}
# 390 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_andnot_pd(__m128d __a,
                                                           __m128d __b) {
  return (__m128d)(~(__v2du)__a & (__v2du)__b);
}
# 407 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_or_pd(__m128d __a,
                                                       __m128d __b) {
  return (__m128d)((__v2du)__a | (__v2du)__b);
}
# 424 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_xor_pd(__m128d __a,
                                                        __m128d __b) {
  return (__m128d)((__v2du)__a ^ (__v2du)__b);
}
# 442 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpeq_pd(__m128d __a,
                                                          __m128d __b) {
  return (__m128d)__builtin_ia32_cmpeqpd((__v2df)__a, (__v2df)__b);
}
# 461 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmplt_pd(__m128d __a,
                                                          __m128d __b) {
  return (__m128d)__builtin_ia32_cmpltpd((__v2df)__a, (__v2df)__b);
}
# 481 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmple_pd(__m128d __a,
                                                          __m128d __b) {
  return (__m128d)__builtin_ia32_cmplepd((__v2df)__a, (__v2df)__b);
}
# 501 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpgt_pd(__m128d __a,
                                                          __m128d __b) {
  return (__m128d)__builtin_ia32_cmpltpd((__v2df)__b, (__v2df)__a);
}
# 521 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpge_pd(__m128d __a,
                                                          __m128d __b) {
  return (__m128d)__builtin_ia32_cmplepd((__v2df)__b, (__v2df)__a);
}
# 543 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpord_pd(__m128d __a,
                                                           __m128d __b) {
  return (__m128d)__builtin_ia32_cmpordpd((__v2df)__a, (__v2df)__b);
}
# 566 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpunord_pd(__m128d __a,
                                                             __m128d __b) {
  return (__m128d)__builtin_ia32_cmpunordpd((__v2df)__a, (__v2df)__b);
}
# 586 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpneq_pd(__m128d __a,
                                                           __m128d __b) {
  return (__m128d)__builtin_ia32_cmpneqpd((__v2df)__a, (__v2df)__b);
}
# 606 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpnlt_pd(__m128d __a,
                                                           __m128d __b) {
  return (__m128d)__builtin_ia32_cmpnltpd((__v2df)__a, (__v2df)__b);
}
# 626 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpnle_pd(__m128d __a,
                                                           __m128d __b) {
  return (__m128d)__builtin_ia32_cmpnlepd((__v2df)__a, (__v2df)__b);
}
# 646 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpngt_pd(__m128d __a,
                                                           __m128d __b) {
  return (__m128d)__builtin_ia32_cmpnltpd((__v2df)__b, (__v2df)__a);
}
# 666 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpnge_pd(__m128d __a,
                                                           __m128d __b) {
  return (__m128d)__builtin_ia32_cmpnlepd((__v2df)__b, (__v2df)__a);
}
# 688 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpeq_sd(__m128d __a,
                                                          __m128d __b) {
  return (__m128d)__builtin_ia32_cmpeqsd((__v2df)__a, (__v2df)__b);
}
# 712 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmplt_sd(__m128d __a,
                                                          __m128d __b) {
  return (__m128d)__builtin_ia32_cmpltsd((__v2df)__a, (__v2df)__b);
}
# 736 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmple_sd(__m128d __a,
                                                          __m128d __b) {
  return (__m128d)__builtin_ia32_cmplesd((__v2df)__a, (__v2df)__b);
}
# 760 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpgt_sd(__m128d __a,
                                                          __m128d __b) {
  __m128d __c = __builtin_ia32_cmpltsd((__v2df)__b, (__v2df)__a);
  return __extension__(__m128d){__c[0], __a[1]};
}
# 785 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpge_sd(__m128d __a,
                                                          __m128d __b) {
  __m128d __c = __builtin_ia32_cmplesd((__v2df)__b, (__v2df)__a);
  return __extension__(__m128d){__c[0], __a[1]};
}
# 812 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpord_sd(__m128d __a,
                                                           __m128d __b) {
  return (__m128d)__builtin_ia32_cmpordsd((__v2df)__a, (__v2df)__b);
}
# 839 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpunord_sd(__m128d __a,
                                                             __m128d __b) {
  return (__m128d)__builtin_ia32_cmpunordsd((__v2df)__a, (__v2df)__b);
}
# 863 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpneq_sd(__m128d __a,
                                                           __m128d __b) {
  return (__m128d)__builtin_ia32_cmpneqsd((__v2df)__a, (__v2df)__b);
}
# 887 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpnlt_sd(__m128d __a,
                                                           __m128d __b) {
  return (__m128d)__builtin_ia32_cmpnltsd((__v2df)__a, (__v2df)__b);
}
# 911 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpnle_sd(__m128d __a,
                                                           __m128d __b) {
  return (__m128d)__builtin_ia32_cmpnlesd((__v2df)__a, (__v2df)__b);
}
# 935 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpngt_sd(__m128d __a,
                                                           __m128d __b) {
  __m128d __c = __builtin_ia32_cmpnltsd((__v2df)__b, (__v2df)__a);
  return __extension__(__m128d){__c[0], __a[1]};
}
# 960 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpnge_sd(__m128d __a,
                                                           __m128d __b) {
  __m128d __c = __builtin_ia32_cmpnlesd((__v2df)__b, (__v2df)__a);
  return __extension__(__m128d){__c[0], __a[1]};
}
# 984 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_comieq_sd(__m128d __a,
                                                       __m128d __b) {
  return __builtin_ia32_comisdeq((__v2df)__a, (__v2df)__b);
}
# 1009 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_comilt_sd(__m128d __a,
                                                       __m128d __b) {
  return __builtin_ia32_comisdlt((__v2df)__a, (__v2df)__b);
}
# 1034 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_comile_sd(__m128d __a,
                                                       __m128d __b) {
  return __builtin_ia32_comisdle((__v2df)__a, (__v2df)__b);
}
# 1059 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_comigt_sd(__m128d __a,
                                                       __m128d __b) {
  return __builtin_ia32_comisdgt((__v2df)__a, (__v2df)__b);
}
# 1084 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_comige_sd(__m128d __a,
                                                       __m128d __b) {
  return __builtin_ia32_comisdge((__v2df)__a, (__v2df)__b);
}
# 1109 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_comineq_sd(__m128d __a,
                                                        __m128d __b) {
  return __builtin_ia32_comisdneq((__v2df)__a, (__v2df)__b);
}
# 1132 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_ucomieq_sd(__m128d __a,
                                                        __m128d __b) {
  return __builtin_ia32_ucomisdeq((__v2df)__a, (__v2df)__b);
}
# 1157 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_ucomilt_sd(__m128d __a,
                                                        __m128d __b) {
  return __builtin_ia32_ucomisdlt((__v2df)__a, (__v2df)__b);
}
# 1182 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_ucomile_sd(__m128d __a,
                                                        __m128d __b) {
  return __builtin_ia32_ucomisdle((__v2df)__a, (__v2df)__b);
}
# 1207 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_ucomigt_sd(__m128d __a,
                                                        __m128d __b) {
  return __builtin_ia32_ucomisdgt((__v2df)__a, (__v2df)__b);
}
# 1232 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_ucomige_sd(__m128d __a,
                                                        __m128d __b) {
  return __builtin_ia32_ucomisdge((__v2df)__a, (__v2df)__b);
}
# 1257 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_ucomineq_sd(__m128d __a,
                                                         __m128d __b) {
  return __builtin_ia32_ucomisdneq((__v2df)__a, (__v2df)__b);
}
# 1275 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cvtpd_ps(__m128d __a) {
  return __builtin_ia32_cvtpd2ps((__v2df)__a);
}
# 1293 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cvtps_pd(__m128 __a) {
  return (__m128d) __builtin_convertvector(
      __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 0, 1), __v2df);
}
# 1314 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cvtepi32_pd(__m128i __a) {
  return (__m128d) __builtin_convertvector(
      __builtin_shufflevector((__v4si)__a, (__v4si)__a, 0, 1), __v2df);
}
# 1332 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cvtpd_epi32(__m128d __a) {
  return __builtin_ia32_cvtpd2dq((__v2df)__a);
}
# 1347 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cvtsd_si32(__m128d __a) {
  return __builtin_ia32_cvtsd2si((__v2df)__a);
}
# 1370 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cvtsd_ss(__m128 __a,
                                                         __m128d __b) {
  return (__m128)__builtin_ia32_cvtsd2ss((__v4sf)__a, (__v2df)__b);
}
# 1392 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cvtsi32_sd(__m128d __a,
                                                            int __b) {
  __a[0] = __b;
  return __a;
}
# 1417 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cvtss_sd(__m128d __a,
                                                          __m128 __b) {
  __a[0] = __b[0];
  return __a;
}
# 1440 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cvttpd_epi32(__m128d __a) {
  return (__m128i)__builtin_ia32_cvttpd2dq((__v2df)__a);
}
# 1456 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cvttsd_si32(__m128d __a) {
  return __builtin_ia32_cvttsd2si((__v2df)__a);
}
# 1471 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse2"), __min_vector_width__(64))) _mm_cvtpd_pi32(__m128d __a) {
  return (__m64)__builtin_ia32_cvtpd2pi((__v2df)__a);
}
# 1489 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse2"), __min_vector_width__(64))) _mm_cvttpd_pi32(__m128d __a) {
  return (__m64)__builtin_ia32_cvttpd2pi((__v2df)__a);
}
# 1504 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse2"), __min_vector_width__(64))) _mm_cvtpi32_pd(__m64 __a) {
  return __builtin_ia32_cvtpi2pd((__v2si)__a);
}
# 1519 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cvtsd_f64(__m128d __a) {
  return __a[0];
}
# 1534 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_load_pd(double const *__dp) {
  return *(const __m128d *)__dp;
}
# 1550 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_load1_pd(double const *__dp) {
  struct __mm_load1_pd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  double __u = ((const struct __mm_load1_pd_struct *)__dp)->__u;
  return __extension__(__m128d){__u, __u};
}
# 1574 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_loadr_pd(double const *__dp) {
  __m128d __u = *(const __m128d *)__dp;
  return __builtin_shufflevector((__v2df)__u, (__v2df)__u, 1, 0);
}
# 1590 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_loadu_pd(double const *__dp) {
  struct __loadu_pd {
    __m128d_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_pd *)__dp)->__v;
}
# 1608 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_loadu_si64(void const *__a) {
  struct __loadu_si64 {
    long long __v;
  } __attribute__((__packed__, __may_alias__));
  long long __u = ((const struct __loadu_si64 *)__a)->__v;
  return __extension__(__m128i)(__v2di){__u, 0LL};
}
# 1627 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_loadu_si32(void const *__a) {
  struct __loadu_si32 {
    int __v;
  } __attribute__((__packed__, __may_alias__));
  int __u = ((const struct __loadu_si32 *)__a)->__v;
  return __extension__(__m128i)(__v4si){__u, 0, 0, 0};
}
# 1646 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_loadu_si16(void const *__a) {
  struct __loadu_si16 {
    short __v;
  } __attribute__((__packed__, __may_alias__));
  short __u = ((const struct __loadu_si16 *)__a)->__v;
  return __extension__(__m128i)(__v8hi){__u, 0, 0, 0, 0, 0, 0, 0};
}
# 1665 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_load_sd(double const *__dp) {
  struct __mm_load_sd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  double __u = ((const struct __mm_load_sd_struct *)__dp)->__u;
  return __extension__(__m128d){__u, 0};
}
# 1690 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_loadh_pd(__m128d __a,
                                                          double const *__dp) {
  struct __mm_loadh_pd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  double __u = ((const struct __mm_loadh_pd_struct *)__dp)->__u;
  return __extension__(__m128d){__a[0], __u};
}
# 1716 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_loadl_pd(__m128d __a,
                                                          double const *__dp) {
  struct __mm_loadl_pd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  double __u = ((const struct __mm_loadl_pd_struct *)__dp)->__u;
  return __extension__(__m128d){__u, __a[1]};
}
# 1736 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_undefined_pd(void) {
  return (__m128d)__builtin_ia32_undef128();
}
# 1754 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_set_sd(double __w) {
  return __extension__(__m128d){__w, 0};
}
# 1770 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_set1_pd(double __w) {
  return __extension__(__m128d){__w, __w};
}
# 1786 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_set_pd1(double __w) {
  return _mm_set1_pd(__w);
}
# 1804 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_set_pd(double __w,
                                                        double __x) {
  return __extension__(__m128d){__x, __w};
}
# 1824 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_setr_pd(double __w,
                                                         double __x) {
  return __extension__(__m128d){__w, __x};
}
# 1838 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_setzero_pd(void) {
  return __extension__(__m128d){0.0, 0.0};
}
# 1857 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_move_sd(__m128d __a,
                                                         __m128d __b) {
  __a[0] = __b[0];
  return __a;
}
# 1874 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_store_sd(double *__dp,
                                                       __m128d __a) {
  struct __mm_store_sd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_store_sd_struct *)__dp)->__u = __a[0];
}
# 1895 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_store_pd(double *__dp,
                                                       __m128d __a) {
  *(__m128d *)__dp = __a;
}
# 1914 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_store1_pd(double *__dp,
                                                        __m128d __a) {
  __a = __builtin_shufflevector((__v2df)__a, (__v2df)__a, 0, 0);
  _mm_store_pd(__dp, __a);
}
# 1934 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_store_pd1(double *__dp,
                                                        __m128d __a) {
  _mm_store1_pd(__dp, __a);
}
# 1951 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_storeu_pd(double *__dp,
                                                        __m128d __a) {
  struct __storeu_pd {
    __m128d_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_pd *)__dp)->__v = __a;
}
# 1973 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_storer_pd(double *__dp,
                                                        __m128d __a) {
  __a = __builtin_shufflevector((__v2df)__a, (__v2df)__a, 1, 0);
  *(__m128d *)__dp = __a;
}
# 1990 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_storeh_pd(double *__dp,
                                                        __m128d __a) {
  struct __mm_storeh_pd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_storeh_pd_struct *)__dp)->__u = __a[1];
}
# 2009 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_storel_pd(double *__dp,
                                                        __m128d __a) {
  struct __mm_storeh_pd_struct {
    double __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_storeh_pd_struct *)__dp)->__u = __a[0];
}
# 2033 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_add_epi8(__m128i __a,
                                                          __m128i __b) {
  return (__m128i)((__v16qu)__a + (__v16qu)__b);
}
# 2054 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_add_epi16(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)((__v8hu)__a + (__v8hu)__b);
}
# 2075 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_add_epi32(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)((__v4su)__a + (__v4su)__b);
}
# 2092 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse2"), __min_vector_width__(64))) _mm_add_si64(__m64 __a,
                                                            __m64 __b) {
  return (__m64)__builtin_ia32_paddq((__v1di)__a, (__v1di)__b);
}
# 2113 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_add_epi64(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)((__v2du)__a + (__v2du)__b);
}
# 2133 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_adds_epi8(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)__builtin_elementwise_add_sat((__v16qs)__a, (__v16qs)__b);
}
# 2154 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_adds_epi16(__m128i __a,
                                                            __m128i __b) {
  return (__m128i)__builtin_elementwise_add_sat((__v8hi)__a, (__v8hi)__b);
}
# 2174 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_adds_epu8(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)__builtin_elementwise_add_sat((__v16qu)__a, (__v16qu)__b);
}
# 2194 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_adds_epu16(__m128i __a,
                                                            __m128i __b) {
  return (__m128i)__builtin_elementwise_add_sat((__v8hu)__a, (__v8hu)__b);
}
# 2213 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_avg_epu8(__m128i __a,
                                                          __m128i __b) {
  return (__m128i)__builtin_ia32_pavgb128((__v16qi)__a, (__v16qi)__b);
}
# 2232 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_avg_epu16(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)__builtin_ia32_pavgw128((__v8hi)__a, (__v8hi)__b);
}
# 2257 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_madd_epi16(__m128i __a,
                                                            __m128i __b) {
  return (__m128i)__builtin_ia32_pmaddwd128((__v8hi)__a, (__v8hi)__b);
}
# 2276 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_max_epi16(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)__builtin_elementwise_max((__v8hi)__a, (__v8hi)__b);
}
# 2295 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_max_epu8(__m128i __a,
                                                          __m128i __b) {
  return (__m128i)__builtin_elementwise_max((__v16qu)__a, (__v16qu)__b);
}
# 2314 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_min_epi16(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)__builtin_elementwise_min((__v8hi)__a, (__v8hi)__b);
}
# 2333 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_min_epu8(__m128i __a,
                                                          __m128i __b) {
  return (__m128i)__builtin_elementwise_min((__v16qu)__a, (__v16qu)__b);
}
# 2352 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_mulhi_epi16(__m128i __a,
                                                             __m128i __b) {
  return (__m128i)__builtin_ia32_pmulhw128((__v8hi)__a, (__v8hi)__b);
}
# 2371 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_mulhi_epu16(__m128i __a,
                                                             __m128i __b) {
  return (__m128i)__builtin_ia32_pmulhuw128((__v8hi)__a, (__v8hi)__b);
}
# 2390 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_mullo_epi16(__m128i __a,
                                                             __m128i __b) {
  return (__m128i)((__v8hu)__a * (__v8hu)__b);
}
# 2408 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse2"), __min_vector_width__(64))) _mm_mul_su32(__m64 __a,
                                                            __m64 __b) {
  return __builtin_ia32_pmuludq((__v2si)__a, (__v2si)__b);
}
# 2426 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_mul_epu32(__m128i __a,
                                                           __m128i __b) {
  return __builtin_ia32_pmuludq128((__v4si)__a, (__v4si)__b);
}
# 2447 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_sad_epu8(__m128i __a,
                                                          __m128i __b) {
  return __builtin_ia32_psadbw128((__v16qi)__a, (__v16qi)__b);
}
# 2464 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_sub_epi8(__m128i __a,
                                                          __m128i __b) {
  return (__m128i)((__v16qu)__a - (__v16qu)__b);
}
# 2481 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_sub_epi16(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)((__v8hu)__a - (__v8hu)__b);
}
# 2498 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_sub_epi32(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)((__v4su)__a - (__v4su)__b);
}
# 2516 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,sse2"), __min_vector_width__(64))) _mm_sub_si64(__m64 __a,
                                                            __m64 __b) {
  return (__m64)__builtin_ia32_psubq((__v1di)__a, (__v1di)__b);
}
# 2533 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_sub_epi64(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)((__v2du)__a - (__v2du)__b);
}
# 2553 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_subs_epi8(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)__builtin_elementwise_sub_sat((__v16qs)__a, (__v16qs)__b);
}
# 2573 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_subs_epi16(__m128i __a,
                                                            __m128i __b) {
  return (__m128i)__builtin_elementwise_sub_sat((__v8hi)__a, (__v8hi)__b);
}
# 2592 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_subs_epu8(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)__builtin_elementwise_sub_sat((__v16qu)__a, (__v16qu)__b);
}
# 2611 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_subs_epu16(__m128i __a,
                                                            __m128i __b) {
  return (__m128i)__builtin_elementwise_sub_sat((__v8hu)__a, (__v8hu)__b);
}
# 2628 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_and_si128(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)((__v2du)__a & (__v2du)__b);
}
# 2647 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_andnot_si128(__m128i __a,
                                                              __m128i __b) {
  return (__m128i)(~(__v2du)__a & (__v2du)__b);
}
# 2663 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_or_si128(__m128i __a,
                                                          __m128i __b) {
  return (__m128i)((__v2du)__a | (__v2du)__b);
}
# 2680 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_xor_si128(__m128i __a,
                                                           __m128i __b) {
  return (__m128i)((__v2du)__a ^ (__v2du)__b);
}
# 2723 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_slli_epi16(__m128i __a,
                                                            int __count) {
  return (__m128i)__builtin_ia32_psllwi128((__v8hi)__a, __count);
}
# 2741 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_sll_epi16(__m128i __a,
                                                           __m128i __count) {
  return (__m128i)__builtin_ia32_psllw128((__v8hi)__a, (__v8hi)__count);
}
# 2759 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_slli_epi32(__m128i __a,
                                                            int __count) {
  return (__m128i)__builtin_ia32_pslldi128((__v4si)__a, __count);
}
# 2777 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_sll_epi32(__m128i __a,
                                                           __m128i __count) {
  return (__m128i)__builtin_ia32_pslld128((__v4si)__a, (__v4si)__count);
}
# 2795 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_slli_epi64(__m128i __a,
                                                            int __count) {
  return __builtin_ia32_psllqi128((__v2di)__a, __count);
}
# 2813 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_sll_epi64(__m128i __a,
                                                           __m128i __count) {
  return __builtin_ia32_psllq128((__v2di)__a, (__v2di)__count);
}
# 2832 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_srai_epi16(__m128i __a,
                                                            int __count) {
  return (__m128i)__builtin_ia32_psrawi128((__v8hi)__a, __count);
}
# 2851 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_sra_epi16(__m128i __a,
                                                           __m128i __count) {
  return (__m128i)__builtin_ia32_psraw128((__v8hi)__a, (__v8hi)__count);
}
# 2870 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_srai_epi32(__m128i __a,
                                                            int __count) {
  return (__m128i)__builtin_ia32_psradi128((__v4si)__a, __count);
}
# 2889 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_sra_epi32(__m128i __a,
                                                           __m128i __count) {
  return (__m128i)__builtin_ia32_psrad128((__v4si)__a, (__v4si)__count);
}
# 2932 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_srli_epi16(__m128i __a,
                                                            int __count) {
  return (__m128i)__builtin_ia32_psrlwi128((__v8hi)__a, __count);
}
# 2950 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_srl_epi16(__m128i __a,
                                                           __m128i __count) {
  return (__m128i)__builtin_ia32_psrlw128((__v8hi)__a, (__v8hi)__count);
}
# 2968 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_srli_epi32(__m128i __a,
                                                            int __count) {
  return (__m128i)__builtin_ia32_psrldi128((__v4si)__a, __count);
}
# 2986 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_srl_epi32(__m128i __a,
                                                           __m128i __count) {
  return (__m128i)__builtin_ia32_psrld128((__v4si)__a, (__v4si)__count);
}
# 3004 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_srli_epi64(__m128i __a,
                                                            int __count) {
  return __builtin_ia32_psrlqi128((__v2di)__a, __count);
}
# 3022 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_srl_epi64(__m128i __a,
                                                           __m128i __count) {
  return __builtin_ia32_psrlq128((__v2di)__a, (__v2di)__count);
}
# 3040 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpeq_epi8(__m128i __a,
                                                            __m128i __b) {
  return (__m128i)((__v16qi)__a == (__v16qi)__b);
}
# 3058 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpeq_epi16(__m128i __a,
                                                             __m128i __b) {
  return (__m128i)((__v8hi)__a == (__v8hi)__b);
}
# 3076 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpeq_epi32(__m128i __a,
                                                             __m128i __b) {
  return (__m128i)((__v4si)__a == (__v4si)__b);
}
# 3095 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpgt_epi8(__m128i __a,
                                                            __m128i __b) {


  return (__m128i)((__v16qs)__a > (__v16qs)__b);
}
# 3117 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpgt_epi16(__m128i __a,
                                                             __m128i __b) {
  return (__m128i)((__v8hi)__a > (__v8hi)__b);
}
# 3137 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmpgt_epi32(__m128i __a,
                                                             __m128i __b) {
  return (__m128i)((__v4si)__a > (__v4si)__b);
}
# 3157 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmplt_epi8(__m128i __a,
                                                            __m128i __b) {
  return _mm_cmpgt_epi8(__b, __a);
}
# 3177 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmplt_epi16(__m128i __a,
                                                             __m128i __b) {
  return _mm_cmpgt_epi16(__b, __a);
}
# 3197 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cmplt_epi32(__m128i __a,
                                                             __m128i __b) {
  return _mm_cmpgt_epi32(__b, __a);
}
# 3220 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cvtsi64_sd(__m128d __a,
                                                            long long __b) {
  __a[0] = __b;
  return __a;
}
# 3237 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cvtsd_si64(__m128d __a) {
  return __builtin_ia32_cvtsd2si64((__v2df)__a);
}
# 3253 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cvttsd_si64(__m128d __a) {
  return __builtin_ia32_cvttsd2si64((__v2df)__a);
}
# 3267 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cvtepi32_ps(__m128i __a) {
  return (__m128) __builtin_convertvector((__v4si)__a, __v4sf);
}
# 3281 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cvtps_epi32(__m128 __a) {
  return (__m128i)__builtin_ia32_cvtps2dq((__v4sf)__a);
}
# 3296 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cvttps_epi32(__m128 __a) {
  return (__m128i)__builtin_ia32_cvttps2dq((__v4sf)__a);
}
# 3310 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cvtsi32_si128(int __a) {
  return __extension__(__m128i)(__v4si){__a, 0, 0, 0};
}
# 3325 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cvtsi64_si128(long long __a) {
  return __extension__(__m128i)(__v2di){__a, 0};
}
# 3340 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cvtsi128_si32(__m128i __a) {
  __v4si __b = (__v4si)__a;
  return __b[0];
}
# 3356 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_cvtsi128_si64(__m128i __a) {
  return __a[0];
}
# 3370 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128)))
_mm_load_si128(__m128i const *__p) {
  return *__p;
}
# 3385 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128)))
_mm_loadu_si128(__m128i_u const *__p) {
  struct __loadu_si128 {
    __m128i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_si128 *)__p)->__v;
}
# 3405 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128)))
_mm_loadl_epi64(__m128i_u const *__p) {
  struct __mm_loadl_epi64_struct {
    long long __u;
  } __attribute__((__packed__, __may_alias__));
  return __extension__(__m128i){
      ((const struct __mm_loadl_epi64_struct *)__p)->__u, 0};
}
# 3423 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_undefined_si128(void) {
  return (__m128i)__builtin_ia32_undef128();
}
# 3443 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_set_epi64x(long long __q1,
                                                            long long __q0) {
  return __extension__(__m128i)(__v2di){__q0, __q1};
}
# 3464 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_set_epi64(__m64 __q1,
                                                           __m64 __q0) {
  return _mm_set_epi64x((long long)__q1, (long long)__q0);
}
# 3491 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_set_epi32(int __i3, int __i2,
                                                           int __i1, int __i0) {
  return __extension__(__m128i)(__v4si){__i0, __i1, __i2, __i3};
}
# 3530 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128)))
_mm_set_epi16(short __w7, short __w6, short __w5, short __w4, short __w3,
              short __w2, short __w1, short __w0) {
  return __extension__(__m128i)(__v8hi){__w0, __w1, __w2, __w3,
                                        __w4, __w5, __w6, __w7};
}
# 3579 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128)))
_mm_set_epi8(char __b15, char __b14, char __b13, char __b12, char __b11,
             char __b10, char __b9, char __b8, char __b7, char __b6, char __b5,
             char __b4, char __b3, char __b2, char __b1, char __b0) {
  return __extension__(__m128i)(__v16qi){
      __b0, __b1, __b2, __b3, __b4, __b5, __b6, __b7,
      __b8, __b9, __b10, __b11, __b12, __b13, __b14, __b15};
}
# 3601 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_set1_epi64x(long long __q) {
  return _mm_set_epi64x(__q, __q);
}
# 3618 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_set1_epi64(__m64 __q) {
  return _mm_set_epi64(__q, __q);
}
# 3635 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_set1_epi32(int __i) {
  return _mm_set_epi32(__i, __i, __i, __i);
}
# 3652 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_set1_epi16(short __w) {
  return _mm_set_epi16(__w, __w, __w, __w, __w, __w, __w, __w);
}
# 3669 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_set1_epi8(char __b) {
  return _mm_set_epi8(__b, __b, __b, __b, __b, __b, __b, __b, __b, __b, __b,
                      __b, __b, __b, __b, __b);
}
# 3688 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_setr_epi64(__m64 __q0,
                                                            __m64 __q1) {
  return _mm_set_epi64(__q1, __q0);
}
# 3710 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_setr_epi32(int __i0, int __i1,
                                                            int __i2,
                                                            int __i3) {
  return _mm_set_epi32(__i3, __i2, __i1, __i0);
}
# 3741 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128)))
_mm_setr_epi16(short __w0, short __w1, short __w2, short __w3, short __w4,
               short __w5, short __w6, short __w7) {
  return _mm_set_epi16(__w7, __w6, __w5, __w4, __w3, __w2, __w1, __w0);
}
# 3788 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128)))
_mm_setr_epi8(char __b0, char __b1, char __b2, char __b3, char __b4, char __b5,
              char __b6, char __b7, char __b8, char __b9, char __b10,
              char __b11, char __b12, char __b13, char __b14, char __b15) {
  return _mm_set_epi8(__b15, __b14, __b13, __b12, __b11, __b10, __b9, __b8,
                      __b7, __b6, __b5, __b4, __b3, __b2, __b1, __b0);
}
# 3804 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_setzero_si128(void) {
  return __extension__(__m128i)(__v2di){0LL, 0LL};
}
# 3820 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_store_si128(__m128i *__p,
                                                          __m128i __b) {
  *__p = __b;
}
# 3835 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_storeu_si128(__m128i_u *__p,
                                                           __m128i __b) {
  struct __storeu_si128 {
    __m128i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_si128 *)__p)->__v = __b;
}
# 3855 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_storeu_si64(void *__p,
                                                          __m128i __b) {
  struct __storeu_si64 {
    long long __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_si64 *)__p)->__v = ((__v2di)__b)[0];
}
# 3875 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_storeu_si32(void *__p,
                                                          __m128i __b) {
  struct __storeu_si32 {
    int __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_si32 *)__p)->__v = ((__v4si)__b)[0];
}
# 3895 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_storeu_si16(void *__p,
                                                          __m128i __b) {
  struct __storeu_si16 {
    short __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_si16 *)__p)->__v = ((__v8hi)__b)[0];
}
# 3924 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_maskmoveu_si128(__m128i __d,
                                                              __m128i __n,
                                                              char *__p) {
  __builtin_ia32_maskmovdqu((__v16qi)__d, (__v16qi)__n, __p);
}
# 3943 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_storel_epi64(__m128i_u *__p,
                                                           __m128i __a) {
  struct __mm_storel_epi64_struct {
    long long __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_storel_epi64_struct *)__p)->__u = __a[0];
}
# 3965 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_stream_pd(double *__p,
                                                        __m128d __a) {
  __builtin_nontemporal_store((__v2df)__a, (__v2df *)__p);
}
# 3983 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_stream_si128(__m128i *__p,
                                                           __m128i __a) {
  __builtin_nontemporal_store((__v2di)__a, (__v2di *)__p);
}
# 4001 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ void
    __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
    _mm_stream_si32(int *__p, int __a) {
  __builtin_ia32_movnti(__p, __a);
}
# 4021 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ void
    __attribute__((__always_inline__, __nodebug__, __target__("sse2")))
    _mm_stream_si64(long long *__p, long long __a) {
  __builtin_ia32_movnti64(__p, __a);
}
# 4042 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
void _mm_clflush(void const *__p);
# 4053 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
void _mm_lfence(void);
# 4064 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
void _mm_mfence(void);
# 4092 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_packs_epi16(__m128i __a,
                                                             __m128i __b) {
  return (__m128i)__builtin_ia32_packsswb128((__v8hi)__a, (__v8hi)__b);
}
# 4119 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_packs_epi32(__m128i __a,
                                                             __m128i __b) {
  return (__m128i)__builtin_ia32_packssdw128((__v4si)__a, (__v4si)__b);
}
# 4146 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_packus_epi16(__m128i __a,
                                                              __m128i __b) {
  return (__m128i)__builtin_ia32_packuswb128((__v8hi)__a, (__v8hi)__b);
}
# 4221 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_movemask_epi8(__m128i __a) {
  return __builtin_ia32_pmovmskb128((__v16qi)__a);
}
# 4354 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_unpackhi_epi8(__m128i __a,
                                                               __m128i __b) {
  return (__m128i)__builtin_shufflevector(
      (__v16qi)__a, (__v16qi)__b, 8, 16 + 8, 9, 16 + 9, 10, 16 + 10, 11,
      16 + 11, 12, 16 + 12, 13, 16 + 13, 14, 16 + 14, 15, 16 + 15);
}
# 4382 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_unpackhi_epi16(__m128i __a,
                                                                __m128i __b) {
  return (__m128i)__builtin_shufflevector((__v8hi)__a, (__v8hi)__b, 4, 8 + 4, 5,
                                          8 + 5, 6, 8 + 6, 7, 8 + 7);
}
# 4405 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_unpackhi_epi32(__m128i __a,
                                                                __m128i __b) {
  return (__m128i)__builtin_shufflevector((__v4si)__a, (__v4si)__b, 2, 4 + 2, 3,
                                          4 + 3);
}
# 4426 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_unpackhi_epi64(__m128i __a,
                                                                __m128i __b) {
  return (__m128i)__builtin_shufflevector((__v2di)__a, (__v2di)__b, 1, 2 + 1);
}
# 4460 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_unpacklo_epi8(__m128i __a,
                                                               __m128i __b) {
  return (__m128i)__builtin_shufflevector(
      (__v16qi)__a, (__v16qi)__b, 0, 16 + 0, 1, 16 + 1, 2, 16 + 2, 3, 16 + 3, 4,
      16 + 4, 5, 16 + 5, 6, 16 + 6, 7, 16 + 7);
}
# 4489 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_unpacklo_epi16(__m128i __a,
                                                                __m128i __b) {
  return (__m128i)__builtin_shufflevector((__v8hi)__a, (__v8hi)__b, 0, 8 + 0, 1,
                                          8 + 1, 2, 8 + 2, 3, 8 + 3);
}
# 4512 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_unpacklo_epi32(__m128i __a,
                                                                __m128i __b) {
  return (__m128i)__builtin_shufflevector((__v4si)__a, (__v4si)__b, 0, 4 + 0, 1,
                                          4 + 1);
}
# 4533 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_unpacklo_epi64(__m128i __a,
                                                                __m128i __b) {
  return (__m128i)__builtin_shufflevector((__v2di)__a, (__v2di)__b, 0, 2 + 0);
}
# 4549 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_movepi64_pi64(__m128i __a) {
  return (__m64)__a[0];
}
# 4564 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_movpi64_epi64(__m64 __a) {
  return __extension__(__m128i)(__v2di){(long long)__a, 0};
}
# 4580 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_move_epi64(__m128i __a) {
  return __builtin_shufflevector((__v2di)__a, _mm_setzero_si128(), 0, 2);
}
# 4599 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_unpackhi_pd(__m128d __a,
                                                             __m128d __b) {
  return __builtin_shufflevector((__v2df)__a, (__v2df)__b, 1, 2 + 1);
}
# 4619 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_unpacklo_pd(__m128d __a,
                                                             __m128d __b) {
  return __builtin_shufflevector((__v2df)__a, (__v2df)__b, 0, 2 + 0);
}
# 4637 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_movemask_pd(__m128d __a) {
  return __builtin_ia32_movmskpd((__v2df)__a);
}
# 4683 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_castpd_ps(__m128d __a) {
  return (__m128)__a;
}
# 4698 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_castpd_si128(__m128d __a) {
  return (__m128i)__a;
}
# 4713 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_castps_pd(__m128 __a) {
  return (__m128d)__a;
}
# 4728 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_castps_si128(__m128 __a) {
  return (__m128i)__a;
}
# 4743 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_castsi128_ps(__m128i __a) {
  return (__m128)__a;
}
# 4758 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse2"), __min_vector_width__(128))) _mm_castsi128_pd(__m128i __a) {
  return (__m128d)__a;
}
# 4773 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
void _mm_pause(void);
# 4792 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/emmintrin.h" 3
#pragma float_control(pop)
# 3032 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xmmintrin.h" 2 3


#pragma float_control(pop)
# 63 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3
# 73 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 3
# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/pmmintrin.h" 1 3
# 37 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/pmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse3"), __min_vector_width__(128)))
_mm_lddqu_si128(__m128i_u const *__p)
{
  return (__m128i)__builtin_ia32_lddqu((char const *)__p);
}
# 56 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/pmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse3"), __min_vector_width__(128)))
_mm_addsub_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_addsubps((__v4sf)__a, (__v4sf)__b);
}
# 79 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/pmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse3"), __min_vector_width__(128)))
_mm_hadd_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_haddps((__v4sf)__a, (__v4sf)__b);
}
# 102 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/pmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse3"), __min_vector_width__(128)))
_mm_hsub_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_hsubps((__v4sf)__a, (__v4sf)__b);
}
# 124 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/pmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse3"), __min_vector_width__(128)))
_mm_movehdup_ps(__m128 __a)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 1, 1, 3, 3);
}
# 145 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/pmmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse3"), __min_vector_width__(128)))
_mm_moveldup_ps(__m128 __a)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 0, 0, 2, 2);
}
# 164 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/pmmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse3"), __min_vector_width__(128)))
_mm_addsub_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_addsubpd((__v2df)__a, (__v2df)__b);
}
# 187 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/pmmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse3"), __min_vector_width__(128)))
_mm_hadd_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_haddpd((__v2df)__a, (__v2df)__b);
}
# 210 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/pmmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse3"), __min_vector_width__(128)))
_mm_hsub_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_hsubpd((__v2df)__a, (__v2df)__b);
}
# 246 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/pmmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse3"), __min_vector_width__(128)))
_mm_movedup_pd(__m128d __a)
{
  return __builtin_shufflevector((__v2df)__a, (__v2df)__a, 0, 0);
}
# 267 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/pmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse3"), __min_vector_width__(128)))
_mm_monitor(void const *__p, unsigned __extensions, unsigned __hints)
{
  __builtin_ia32_monitor(__p, __extensions, __hints);
}
# 286 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/pmmintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse3"), __min_vector_width__(128)))
_mm_mwait(unsigned __extensions, unsigned __hints)
{
  __builtin_ia32_mwait(__extensions, __hints);
}
# 74 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 1 3
# 35 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3"), __min_vector_width__(64)))
_mm_abs_pi8(__m64 __a)
{
    return (__m64)__builtin_ia32_pabsb((__v8qi)__a);
}
# 53 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3"), __min_vector_width__(64)))
_mm_abs_epi8(__m128i __a)
{
    return (__m128i)__builtin_elementwise_abs((__v16qs)__a);
}
# 71 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3"), __min_vector_width__(64)))
_mm_abs_pi16(__m64 __a)
{
    return (__m64)__builtin_ia32_pabsw((__v4hi)__a);
}
# 89 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3"), __min_vector_width__(64)))
_mm_abs_epi16(__m128i __a)
{
    return (__m128i)__builtin_elementwise_abs((__v8hi)__a);
}
# 107 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3"), __min_vector_width__(64)))
_mm_abs_pi32(__m64 __a)
{
    return (__m64)__builtin_ia32_pabsd((__v2si)__a);
}
# 125 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3"), __min_vector_width__(64)))
_mm_abs_epi32(__m128i __a)
{
    return (__m128i)__builtin_elementwise_abs((__v4si)__a);
}
# 194 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3"), __min_vector_width__(64)))
_mm_hadd_epi16(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_phaddw128((__v8hi)__a, (__v8hi)__b);
}
# 217 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3"), __min_vector_width__(64)))
_mm_hadd_epi32(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_phaddd128((__v4si)__a, (__v4si)__b);
}
# 240 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3"), __min_vector_width__(64)))
_mm_hadd_pi16(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_phaddw((__v4hi)__a, (__v4hi)__b);
}
# 263 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3"), __min_vector_width__(64)))
_mm_hadd_pi32(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_phaddd((__v2si)__a, (__v2si)__b);
}
# 288 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3"), __min_vector_width__(64)))
_mm_hadds_epi16(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_phaddsw128((__v8hi)__a, (__v8hi)__b);
}
# 313 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3"), __min_vector_width__(64)))
_mm_hadds_pi16(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_phaddsw((__v4hi)__a, (__v4hi)__b);
}
# 336 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3"), __min_vector_width__(64)))
_mm_hsub_epi16(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_phsubw128((__v8hi)__a, (__v8hi)__b);
}
# 359 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3"), __min_vector_width__(64)))
_mm_hsub_epi32(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_phsubd128((__v4si)__a, (__v4si)__b);
}
# 382 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3"), __min_vector_width__(64)))
_mm_hsub_pi16(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_phsubw((__v4hi)__a, (__v4hi)__b);
}
# 405 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3"), __min_vector_width__(64)))
_mm_hsub_pi32(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_phsubd((__v2si)__a, (__v2si)__b);
}
# 430 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3"), __min_vector_width__(64)))
_mm_hsubs_epi16(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_phsubsw128((__v8hi)__a, (__v8hi)__b);
}
# 455 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3"), __min_vector_width__(64)))
_mm_hsubs_pi16(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_phsubsw((__v4hi)__a, (__v4hi)__b);
}
# 489 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3"), __min_vector_width__(64)))
_mm_maddubs_epi16(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_pmaddubsw128((__v16qi)__a, (__v16qi)__b);
}
# 519 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3"), __min_vector_width__(64)))
_mm_maddubs_pi16(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_pmaddubsw((__v8qi)__a, (__v8qi)__b);
}
# 539 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3"), __min_vector_width__(64)))
_mm_mulhrs_epi16(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_pmulhrsw128((__v8hi)__a, (__v8hi)__b);
}
# 559 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3"), __min_vector_width__(64)))
_mm_mulhrs_pi16(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_pmulhrsw((__v4hi)__a, (__v4hi)__b);
}
# 585 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3"), __min_vector_width__(64)))
_mm_shuffle_epi8(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_pshufb128((__v16qi)__a, (__v16qi)__b);
}
# 610 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3"), __min_vector_width__(64)))
_mm_shuffle_pi8(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_pshufb((__v8qi)__a, (__v8qi)__b);
}
# 636 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3"), __min_vector_width__(64)))
_mm_sign_epi8(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_psignb128((__v16qi)__a, (__v16qi)__b);
}
# 662 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3"), __min_vector_width__(64)))
_mm_sign_epi16(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_psignw128((__v8hi)__a, (__v8hi)__b);
}
# 688 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("ssse3"), __min_vector_width__(64)))
_mm_sign_epi32(__m128i __a, __m128i __b)
{
    return (__m128i)__builtin_ia32_psignd128((__v4si)__a, (__v4si)__b);
}
# 714 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3"), __min_vector_width__(64)))
_mm_sign_pi8(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_psignb((__v8qi)__a, (__v8qi)__b);
}
# 740 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3"), __min_vector_width__(64)))
_mm_sign_pi16(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_psignw((__v4hi)__a, (__v4hi)__b);
}
# 766 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tmmintrin.h" 3
static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("mmx,ssse3"), __min_vector_width__(64)))
_mm_sign_pi32(__m64 __a, __m64 __b)
{
    return (__m64)__builtin_ia32_psignd((__v2si)__a, (__v2si)__b);
}
# 79 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3





# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 1 3
# 449 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_blendv_pd(__m128d __V1,
                                                           __m128d __V2,
                                                           __m128d __M) {
  return (__m128d)__builtin_ia32_blendvpd((__v2df)__V1, (__v2df)__V2,
                                          (__v2df)__M);
}
# 476 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_blendv_ps(__m128 __V1,
                                                          __m128 __V2,
                                                          __m128 __M) {
  return (__m128)__builtin_ia32_blendvps((__v4sf)__V1, (__v4sf)__V2,
                                         (__v4sf)__M);
}
# 503 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_blendv_epi8(__m128i __V1,
                                                             __m128i __V2,
                                                             __m128i __M) {
  return (__m128i)__builtin_ia32_pblendvb128((__v16qi)__V1, (__v16qi)__V2,
                                             (__v16qi)__M);
}
# 552 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_mullo_epi32(__m128i __V1,
                                                             __m128i __V2) {
  return (__m128i)((__v4su)__V1 * (__v4su)__V2);
}
# 571 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_mul_epi32(__m128i __V1,
                                                           __m128i __V2) {
  return (__m128i)__builtin_ia32_pmuldq128((__v4si)__V1, (__v4si)__V2);
}
# 660 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128)))
_mm_stream_load_si128(__m128i const *__V) {
  return (__m128i)__builtin_nontemporal_load((const __v2di *)__V);
}
# 679 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_min_epi8(__m128i __V1,
                                                          __m128i __V2) {
  return (__m128i)__builtin_elementwise_min((__v16qs)__V1, (__v16qs)__V2);
}
# 697 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_max_epi8(__m128i __V1,
                                                          __m128i __V2) {
  return (__m128i)__builtin_elementwise_max((__v16qs)__V1, (__v16qs)__V2);
}
# 715 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_min_epu16(__m128i __V1,
                                                           __m128i __V2) {
  return (__m128i)__builtin_elementwise_min((__v8hu)__V1, (__v8hu)__V2);
}
# 733 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_max_epu16(__m128i __V1,
                                                           __m128i __V2) {
  return (__m128i)__builtin_elementwise_max((__v8hu)__V1, (__v8hu)__V2);
}
# 751 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_min_epi32(__m128i __V1,
                                                           __m128i __V2) {
  return (__m128i)__builtin_elementwise_min((__v4si)__V1, (__v4si)__V2);
}
# 769 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_max_epi32(__m128i __V1,
                                                           __m128i __V2) {
  return (__m128i)__builtin_elementwise_max((__v4si)__V1, (__v4si)__V2);
}
# 787 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_min_epu32(__m128i __V1,
                                                           __m128i __V2) {
  return (__m128i)__builtin_elementwise_min((__v4su)__V1, (__v4su)__V2);
}
# 805 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_max_epu32(__m128i __V1,
                                                           __m128i __V2) {
  return (__m128i)__builtin_elementwise_max((__v4su)__V1, (__v4su)__V2);
}
# 1106 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_testz_si128(__m128i __M,
                                                         __m128i __V) {
  return __builtin_ia32_ptestz128((__v2di)__M, (__v2di)__V);
}
# 1123 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_testc_si128(__m128i __M,
                                                         __m128i __V) {
  return __builtin_ia32_ptestc128((__v2di)__M, (__v2di)__V);
}
# 1141 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_testnzc_si128(__m128i __M,
                                                           __m128i __V) {
  return __builtin_ia32_ptestnzc128((__v2di)__M, (__v2di)__V);
}
# 1213 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_cmpeq_epi64(__m128i __V1,
                                                             __m128i __V2) {
  return (__m128i)((__v2di)__V1 == (__v2di)__V2);
}
# 1232 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_cvtepi8_epi16(__m128i __V) {


  return (__m128i) __builtin_convertvector(
      __builtin_shufflevector((__v16qs)__V, (__v16qs)__V, 0, 1, 2, 3, 4, 5, 6,
                              7),
      __v8hi);
}
# 1254 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_cvtepi8_epi32(__m128i __V) {


  return (__m128i) __builtin_convertvector(
      __builtin_shufflevector((__v16qs)__V, (__v16qs)__V, 0, 1, 2, 3), __v4si);
}
# 1274 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_cvtepi8_epi64(__m128i __V) {


  return (__m128i) __builtin_convertvector(
      __builtin_shufflevector((__v16qs)__V, (__v16qs)__V, 0, 1), __v2di);
}
# 1294 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_cvtepi16_epi32(__m128i __V) {
  return (__m128i) __builtin_convertvector(
      __builtin_shufflevector((__v8hi)__V, (__v8hi)__V, 0, 1, 2, 3), __v4si);
}
# 1312 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_cvtepi16_epi64(__m128i __V) {
  return (__m128i) __builtin_convertvector(
      __builtin_shufflevector((__v8hi)__V, (__v8hi)__V, 0, 1), __v2di);
}
# 1330 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_cvtepi32_epi64(__m128i __V) {
  return (__m128i) __builtin_convertvector(
      __builtin_shufflevector((__v4si)__V, (__v4si)__V, 0, 1), __v2di);
}
# 1349 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_cvtepu8_epi16(__m128i __V) {
  return (__m128i) __builtin_convertvector(
      __builtin_shufflevector((__v16qu)__V, (__v16qu)__V, 0, 1, 2, 3, 4, 5, 6,
                              7),
      __v8hi);
}
# 1369 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_cvtepu8_epi32(__m128i __V) {
  return (__m128i) __builtin_convertvector(
      __builtin_shufflevector((__v16qu)__V, (__v16qu)__V, 0, 1, 2, 3), __v4si);
}
# 1387 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_cvtepu8_epi64(__m128i __V) {
  return (__m128i) __builtin_convertvector(
      __builtin_shufflevector((__v16qu)__V, (__v16qu)__V, 0, 1), __v2di);
}
# 1405 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_cvtepu16_epi32(__m128i __V) {
  return (__m128i) __builtin_convertvector(
      __builtin_shufflevector((__v8hu)__V, (__v8hu)__V, 0, 1, 2, 3), __v4si);
}
# 1423 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_cvtepu16_epi64(__m128i __V) {
  return (__m128i) __builtin_convertvector(
      __builtin_shufflevector((__v8hu)__V, (__v8hu)__V, 0, 1), __v2di);
}
# 1441 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_cvtepu32_epi64(__m128i __V) {
  return (__m128i) __builtin_convertvector(
      __builtin_shufflevector((__v4su)__V, (__v4su)__V, 0, 1), __v2di);
}
# 1469 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_packus_epi32(__m128i __V1,
                                                              __m128i __V2) {
  return (__m128i)__builtin_ia32_packusdw128((__v4si)__V1, (__v4si)__V2);
}
# 1527 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.1"), __min_vector_width__(128))) _mm_minpos_epu16(__m128i __V) {
  return (__m128i)__builtin_ia32_phminposuw128((__v8hi)__V);
}
# 2330 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4.2"))) _mm_cmpgt_epi64(__m128i __V1,
                                                             __m128i __V2) {
  return (__m128i)((__v2di)__V1 > (__v2di)__V2);
}



# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/popcntintrin.h" 1 3
# 32 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/popcntintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("popcnt")))
_mm_popcnt_u32(unsigned int __A)
{
  return __builtin_popcount(__A);
}
# 49 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/popcntintrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("popcnt")))
_mm_popcnt_u64(unsigned long long __A)
{
  return __builtin_popcountll(__A);
}
# 2338 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/smmintrin.h" 2 3
# 85 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3





# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/wmmintrin.h" 1 3
# 19 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/wmmintrin.h" 3
# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/__wmmintrin_aes.h" 1 3
# 34 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/__wmmintrin_aes.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("aes"), __min_vector_width__(128)))
_mm_aesenc_si128(__m128i __V, __m128i __R)
{
  return (__m128i)__builtin_ia32_aesenc128((__v2di)__V, (__v2di)__R);
}
# 54 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/__wmmintrin_aes.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("aes"), __min_vector_width__(128)))
_mm_aesenclast_si128(__m128i __V, __m128i __R)
{
  return (__m128i)__builtin_ia32_aesenclast128((__v2di)__V, (__v2di)__R);
}
# 74 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/__wmmintrin_aes.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("aes"), __min_vector_width__(128)))
_mm_aesdec_si128(__m128i __V, __m128i __R)
{
  return (__m128i)__builtin_ia32_aesdec128((__v2di)__V, (__v2di)__R);
}
# 94 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/__wmmintrin_aes.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("aes"), __min_vector_width__(128)))
_mm_aesdeclast_si128(__m128i __V, __m128i __R)
{
  return (__m128i)__builtin_ia32_aesdeclast128((__v2di)__V, (__v2di)__R);
}
# 111 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/__wmmintrin_aes.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("aes"), __min_vector_width__(128)))
_mm_aesimc_si128(__m128i __V)
{
  return (__m128i)__builtin_ia32_aesimc128((__v2di)__V);
}
# 20 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/wmmintrin.h" 2 3

# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/__wmmintrin_pclmul.h" 1 3
# 22 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/wmmintrin.h" 2 3
# 91 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/clflushoptintrin.h" 1 3
# 20 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/clflushoptintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("clflushopt")))
_mm_clflushopt(void const * __m) {
  __builtin_ia32_clflushopt(__m);
}
# 96 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/clwbintrin.h" 1 3
# 31 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/clwbintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("clwb")))
_mm_clwb(void const *__p) {
  __builtin_ia32_clwb(__p);
}
# 101 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 1 3
# 17 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
typedef double __v4df __attribute__ ((__vector_size__ (32)));
typedef float __v8sf __attribute__ ((__vector_size__ (32)));
typedef long long __v4di __attribute__ ((__vector_size__ (32)));
typedef int __v8si __attribute__ ((__vector_size__ (32)));
typedef short __v16hi __attribute__ ((__vector_size__ (32)));
typedef char __v32qi __attribute__ ((__vector_size__ (32)));


typedef unsigned long long __v4du __attribute__ ((__vector_size__ (32)));
typedef unsigned int __v8su __attribute__ ((__vector_size__ (32)));
typedef unsigned short __v16hu __attribute__ ((__vector_size__ (32)));
typedef unsigned char __v32qu __attribute__ ((__vector_size__ (32)));



typedef signed char __v32qs __attribute__((__vector_size__(32)));

typedef float __m256 __attribute__ ((__vector_size__ (32), __aligned__(32)));
typedef double __m256d __attribute__((__vector_size__(32), __aligned__(32)));
typedef long long __m256i __attribute__((__vector_size__(32), __aligned__(32)));

typedef float __m256_u __attribute__ ((__vector_size__ (32), __aligned__(1)));
typedef double __m256d_u __attribute__((__vector_size__(32), __aligned__(1)));
typedef long long __m256i_u __attribute__((__vector_size__(32), __aligned__(1)));



typedef _Float16 __v16hf __attribute__((__vector_size__(32), __aligned__(32)));
typedef _Float16 __m256h __attribute__((__vector_size__(32), __aligned__(32)));
typedef _Float16 __m256h_u __attribute__((__vector_size__(32), __aligned__(1)));

typedef __bf16 __v16bf __attribute__((__vector_size__(32), __aligned__(32)));
typedef __bf16 __m256bh __attribute__((__vector_size__(32), __aligned__(32)));
# 69 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_add_pd(__m256d __a, __m256d __b)
{
  return (__m256d)((__v4df)__a+(__v4df)__b);
}
# 87 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_add_ps(__m256 __a, __m256 __b)
{
  return (__m256)((__v8sf)__a+(__v8sf)__b);
}
# 105 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_sub_pd(__m256d __a, __m256d __b)
{
  return (__m256d)((__v4df)__a-(__v4df)__b);
}
# 123 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_sub_ps(__m256 __a, __m256 __b)
{
  return (__m256)((__v8sf)__a-(__v8sf)__b);
}
# 142 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_addsub_pd(__m256d __a, __m256d __b)
{
  return (__m256d)__builtin_ia32_addsubpd256((__v4df)__a, (__v4df)__b);
}
# 161 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_addsub_ps(__m256 __a, __m256 __b)
{
  return (__m256)__builtin_ia32_addsubps256((__v8sf)__a, (__v8sf)__b);
}
# 179 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_div_pd(__m256d __a, __m256d __b)
{
  return (__m256d)((__v4df)__a/(__v4df)__b);
}
# 197 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_div_ps(__m256 __a, __m256 __b)
{
  return (__m256)((__v8sf)__a/(__v8sf)__b);
}
# 216 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_max_pd(__m256d __a, __m256d __b)
{
  return (__m256d)__builtin_ia32_maxpd256((__v4df)__a, (__v4df)__b);
}
# 235 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_max_ps(__m256 __a, __m256 __b)
{
  return (__m256)__builtin_ia32_maxps256((__v8sf)__a, (__v8sf)__b);
}
# 254 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_min_pd(__m256d __a, __m256d __b)
{
  return (__m256d)__builtin_ia32_minpd256((__v4df)__a, (__v4df)__b);
}
# 273 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_min_ps(__m256 __a, __m256 __b)
{
  return (__m256)__builtin_ia32_minps256((__v8sf)__a, (__v8sf)__b);
}
# 291 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_mul_pd(__m256d __a, __m256d __b)
{
  return (__m256d)((__v4df)__a * (__v4df)__b);
}
# 309 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_mul_ps(__m256 __a, __m256 __b)
{
  return (__m256)((__v8sf)__a * (__v8sf)__b);
}
# 326 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_sqrt_pd(__m256d __a)
{
  return (__m256d)__builtin_ia32_sqrtpd256((__v4df)__a);
}
# 343 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_sqrt_ps(__m256 __a)
{
  return (__m256)__builtin_ia32_sqrtps256((__v8sf)__a);
}
# 360 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_rsqrt_ps(__m256 __a)
{
  return (__m256)__builtin_ia32_rsqrtps256((__v8sf)__a);
}
# 377 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_rcp_ps(__m256 __a)
{
  return (__m256)__builtin_ia32_rcpps256((__v8sf)__a);
}
# 529 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_and_pd(__m256d __a, __m256d __b)
{
  return (__m256d)((__v4du)__a & (__v4du)__b);
}
# 547 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_and_ps(__m256 __a, __m256 __b)
{
  return (__m256)((__v8su)__a & (__v8su)__b);
}
# 568 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_andnot_pd(__m256d __a, __m256d __b)
{
  return (__m256d)(~(__v4du)__a & (__v4du)__b);
}
# 589 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_andnot_ps(__m256 __a, __m256 __b)
{
  return (__m256)(~(__v8su)__a & (__v8su)__b);
}
# 607 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_or_pd(__m256d __a, __m256d __b)
{
  return (__m256d)((__v4du)__a | (__v4du)__b);
}
# 625 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_or_ps(__m256 __a, __m256 __b)
{
  return (__m256)((__v8su)__a | (__v8su)__b);
}
# 643 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_xor_pd(__m256d __a, __m256d __b)
{
  return (__m256d)((__v4du)__a ^ (__v4du)__b);
}
# 661 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_xor_ps(__m256 __a, __m256 __b)
{
  return (__m256)((__v8su)__a ^ (__v8su)__b);
}
# 685 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_hadd_pd(__m256d __a, __m256d __b)
{
  return (__m256d)__builtin_ia32_haddpd256((__v4df)__a, (__v4df)__b);
}
# 708 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_hadd_ps(__m256 __a, __m256 __b)
{
  return (__m256)__builtin_ia32_haddps256((__v8sf)__a, (__v8sf)__b);
}
# 731 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_hsub_pd(__m256d __a, __m256d __b)
{
  return (__m256d)__builtin_ia32_hsubpd256((__v4df)__a, (__v4df)__b);
}
# 754 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_hsub_ps(__m256 __a, __m256 __b)
{
  return (__m256)__builtin_ia32_hsubps256((__v8sf)__a, (__v8sf)__b);
}
# 784 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(128)))
_mm_permutevar_pd(__m128d __a, __m128i __c)
{
  return (__m128d)__builtin_ia32_vpermilvarpd((__v2df)__a, (__v2di)__c);
}
# 823 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_permutevar_pd(__m256d __a, __m256i __c)
{
  return (__m256d)__builtin_ia32_vpermilvarpd256((__v4df)__a, (__v4di)__c);
}
# 877 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(128)))
_mm_permutevar_ps(__m128 __a, __m128i __c)
{
  return (__m128)__builtin_ia32_vpermilvarps((__v4sf)__a, (__v4si)__c);
}
# 968 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_permutevar_ps(__m256 __a, __m256i __c)
{
  return (__m256)__builtin_ia32_vpermilvarps256((__v8sf)__a, (__v8si)__c);
}
# 1392 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_blendv_pd(__m256d __a, __m256d __b, __m256d __c)
{
  return (__m256d)__builtin_ia32_blendvpd256(
    (__v4df)__a, (__v4df)__b, (__v4df)__c);
}
# 1420 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_blendv_ps(__m256 __a, __m256 __b, __m256 __c)
{
  return (__m256)__builtin_ia32_blendvps256(
    (__v8sf)__a, (__v8sf)__b, (__v8sf)__c);
}
# 2173 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_cvtepi32_pd(__m128i __a)
{
  return (__m256d)__builtin_convertvector((__v4si)__a, __v4df);
}
# 2188 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_cvtepi32_ps(__m256i __a)
{
  return (__m256)__builtin_convertvector((__v8si)__a, __v8sf);
}
# 2204 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_cvtpd_ps(__m256d __a)
{
  return (__m128)__builtin_ia32_cvtpd2ps256((__v4df) __a);
}
# 2219 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_cvtps_epi32(__m256 __a)
{
  return (__m256i)__builtin_ia32_cvtps2dq256((__v8sf) __a);
}
# 2235 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_cvtps_pd(__m128 __a)
{
  return (__m256d)__builtin_convertvector((__v4sf)__a, __v4df);
}
# 2252 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_cvttpd_epi32(__m256d __a)
{
  return (__m128i)__builtin_ia32_cvttpd2dq256((__v4df) __a);
}
# 2269 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_cvtpd_epi32(__m256d __a)
{
  return (__m128i)__builtin_ia32_cvtpd2dq256((__v4df) __a);
}
# 2285 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_cvttps_epi32(__m256 __a)
{
  return (__m256i)__builtin_ia32_cvttps2dq256((__v8sf) __a);
}
# 2301 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline double __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_cvtsd_f64(__m256d __a)
{
 return __a[0];
}
# 2317 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_cvtsi256_si32(__m256i __a)
{
 __v8si __b = (__v8si)__a;
 return __b[0];
}
# 2334 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline float __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_cvtss_f32(__m256 __a)
{
 return __a[0];
}
# 2360 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_movehdup_ps(__m256 __a)
{
  return __builtin_shufflevector((__v8sf)__a, (__v8sf)__a, 1, 1, 3, 3, 5, 5, 7, 7);
}
# 2385 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_moveldup_ps(__m256 __a)
{
  return __builtin_shufflevector((__v8sf)__a, (__v8sf)__a, 0, 0, 2, 2, 4, 4, 6, 6);
}
# 2407 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_movedup_pd(__m256d __a)
{
  return __builtin_shufflevector((__v4df)__a, (__v4df)__a, 0, 0, 2, 2);
}
# 2430 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_unpackhi_pd(__m256d __a, __m256d __b)
{
  return __builtin_shufflevector((__v4df)__a, (__v4df)__b, 1, 5, 1+2, 5+2);
}
# 2452 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_unpacklo_pd(__m256d __a, __m256d __b)
{
  return __builtin_shufflevector((__v4df)__a, (__v4df)__b, 0, 4, 0+2, 4+2);
}
# 2479 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_unpackhi_ps(__m256 __a, __m256 __b)
{
  return __builtin_shufflevector((__v8sf)__a, (__v8sf)__b, 2, 10, 2+1, 10+1, 6, 14, 6+1, 14+1);
}
# 2506 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_unpacklo_ps(__m256 __a, __m256 __b)
{
  return __builtin_shufflevector((__v8sf)__a, (__v8sf)__b, 0, 8, 0+1, 8+1, 4, 12, 4+1, 12+1);
}
# 2536 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(128)))
_mm_testz_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_vtestzpd((__v2df)__a, (__v2df)__b);
}
# 2565 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(128)))
_mm_testc_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_vtestcpd((__v2df)__a, (__v2df)__b);
}
# 2595 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(128)))
_mm_testnzc_pd(__m128d __a, __m128d __b)
{
  return __builtin_ia32_vtestnzcpd((__v2df)__a, (__v2df)__b);
}
# 2624 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(128)))
_mm_testz_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_vtestzps((__v4sf)__a, (__v4sf)__b);
}
# 2653 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(128)))
_mm_testc_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_vtestcps((__v4sf)__a, (__v4sf)__b);
}
# 2683 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(128)))
_mm_testnzc_ps(__m128 __a, __m128 __b)
{
  return __builtin_ia32_vtestnzcps((__v4sf)__a, (__v4sf)__b);
}
# 2712 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_testz_pd(__m256d __a, __m256d __b)
{
  return __builtin_ia32_vtestzpd256((__v4df)__a, (__v4df)__b);
}
# 2741 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_testc_pd(__m256d __a, __m256d __b)
{
  return __builtin_ia32_vtestcpd256((__v4df)__a, (__v4df)__b);
}
# 2771 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_testnzc_pd(__m256d __a, __m256d __b)
{
  return __builtin_ia32_vtestnzcpd256((__v4df)__a, (__v4df)__b);
}
# 2800 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_testz_ps(__m256 __a, __m256 __b)
{
  return __builtin_ia32_vtestzps256((__v8sf)__a, (__v8sf)__b);
}
# 2829 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_testc_ps(__m256 __a, __m256 __b)
{
  return __builtin_ia32_vtestcps256((__v8sf)__a, (__v8sf)__b);
}
# 2859 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_testnzc_ps(__m256 __a, __m256 __b)
{
  return __builtin_ia32_vtestnzcps256((__v8sf)__a, (__v8sf)__b);
}
# 2885 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_testz_si256(__m256i __a, __m256i __b)
{
  return __builtin_ia32_ptestz256((__v4di)__a, (__v4di)__b);
}
# 2911 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_testc_si256(__m256i __a, __m256i __b)
{
  return __builtin_ia32_ptestc256((__v4di)__a, (__v4di)__b);
}
# 2938 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_testnzc_si256(__m256i __a, __m256i __b)
{
  return __builtin_ia32_ptestnzc256((__v4di)__a, (__v4di)__b);
}
# 2957 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_movemask_pd(__m256d __a)
{
  return __builtin_ia32_movmskpd256((__v4df)__a);
}
# 2975 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline int __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_movemask_ps(__m256 __a)
{
  return __builtin_ia32_movmskps256((__v8sf)__a);
}







static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_zeroall(void)
{
  __builtin_ia32_vzeroall();
}






static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx")))
_mm256_zeroupper(void)
{
  __builtin_ia32_vzeroupper();
}
# 3017 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(128)))
_mm_broadcast_ss(float const *__a)
{
  float __f = *__a;
  return __extension__ (__m128)(__v4sf){ __f, __f, __f, __f };
}
# 3036 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_broadcast_sd(double const *__a)
{
  double __d = *__a;
  return __extension__ (__m256d)(__v4df){ __d, __d, __d, __d };
}
# 3055 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_broadcast_ss(float const *__a)
{
  float __f = *__a;
  return __extension__ (__m256)(__v8sf){ __f, __f, __f, __f, __f, __f, __f, __f };
}
# 3074 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_broadcast_pd(__m128d const *__a)
{
  __m128d __b = _mm_loadu_pd((const double *)__a);
  return (__m256d)__builtin_shufflevector((__v2df)__b, (__v2df)__b,
                                          0, 1, 0, 1);
}
# 3094 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_broadcast_ps(__m128 const *__a)
{
  __m128 __b = _mm_loadu_ps((const float *)__a);
  return (__m256)__builtin_shufflevector((__v4sf)__b, (__v4sf)__b,
                                         0, 1, 2, 3, 0, 1, 2, 3);
}
# 3114 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_load_pd(double const *__p)
{
  return *(const __m256d *)__p;
}
# 3130 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_load_ps(float const *__p)
{
  return *(const __m256 *)__p;
}
# 3147 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_loadu_pd(double const *__p)
{
  struct __loadu_pd {
    __m256d_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_pd*)__p)->__v;
}
# 3167 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_loadu_ps(float const *__p)
{
  struct __loadu_ps {
    __m256_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_ps*)__p)->__v;
}
# 3187 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_load_si256(__m256i const *__p)
{
  return *__p;
}
# 3203 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_loadu_si256(__m256i_u const *__p)
{
  struct __loadu_si256 {
    __m256i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_si256*)__p)->__v;
}
# 3224 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_lddqu_si256(__m256i_u const *__p)
{
  return (__m256i)__builtin_ia32_lddqu256((char const *)__p);
}
# 3244 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_store_pd(double *__p, __m256d __a)
{
  *(__m256d *)__p = __a;
}
# 3262 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_store_ps(float *__p, __m256 __a)
{
  *(__m256 *)__p = __a;
}
# 3280 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_storeu_pd(double *__p, __m256d __a)
{
  struct __storeu_pd {
    __m256d_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_pd*)__p)->__v = __a;
}
# 3300 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_storeu_ps(float *__p, __m256 __a)
{
  struct __storeu_ps {
    __m256_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_ps*)__p)->__v = __a;
}
# 3321 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_store_si256(__m256i *__p, __m256i __a)
{
  *__p = __a;
}
# 3338 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_storeu_si256(__m256i_u *__p, __m256i __a)
{
  struct __storeu_si256 {
    __m256i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_si256*)__p)->__v = __a;
}
# 3366 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(128)))
_mm_maskload_pd(double const *__p, __m128i __m)
{
  return (__m128d)__builtin_ia32_maskloadpd((const __v2df *)__p, (__v2di)__m);
}
# 3390 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_maskload_pd(double const *__p, __m256i __m)
{
  return (__m256d)__builtin_ia32_maskloadpd256((const __v4df *)__p,
                                               (__v4di)__m);
}
# 3415 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(128)))
_mm_maskload_ps(float const *__p, __m128i __m)
{
  return (__m128)__builtin_ia32_maskloadps((const __v4sf *)__p, (__v4si)__m);
}
# 3439 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_maskload_ps(float const *__p, __m256i __m)
{
  return (__m256)__builtin_ia32_maskloadps256((const __v8sf *)__p, (__v8si)__m);
}
# 3464 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_maskstore_ps(float *__p, __m256i __m, __m256 __a)
{
  __builtin_ia32_maskstoreps256((__v8sf *)__p, (__v8si)__m, (__v8sf)__a);
}
# 3488 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(128)))
_mm_maskstore_pd(double *__p, __m128i __m, __m128d __a)
{
  __builtin_ia32_maskstorepd((__v2df *)__p, (__v2di)__m, (__v2df)__a);
}
# 3512 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_maskstore_pd(double *__p, __m256i __m, __m256d __a)
{
  __builtin_ia32_maskstorepd256((__v4df *)__p, (__v4di)__m, (__v4df)__a);
}
# 3536 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(128)))
_mm_maskstore_ps(float *__p, __m128i __m, __m128 __a)
{
  __builtin_ia32_maskstoreps((__v4sf *)__p, (__v4si)__m, (__v4sf)__a);
}
# 3556 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_stream_si256(__m256i *__a, __m256i __b)
{
  typedef __v4di __v4di_aligned __attribute__((aligned(32)));
  __builtin_nontemporal_store((__v4di_aligned)__b, (__v4di_aligned*)__a);
}
# 3576 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_stream_pd(double *__a, __m256d __b)
{
  typedef __v4df __v4df_aligned __attribute__((aligned(32)));
  __builtin_nontemporal_store((__v4df_aligned)__b, (__v4df_aligned*)__a);
}
# 3597 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_stream_ps(float *__p, __m256 __a)
{
  typedef __v8sf __v8sf_aligned __attribute__((aligned(32)));
  __builtin_nontemporal_store((__v8sf_aligned)__a, (__v8sf_aligned*)__p);
}
# 3612 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_undefined_pd(void)
{
  return (__m256d)__builtin_ia32_undef256();
}
# 3625 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_undefined_ps(void)
{
  return (__m256)__builtin_ia32_undef256();
}
# 3638 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_undefined_si256(void)
{
  return (__m256i)__builtin_ia32_undef256();
}
# 3665 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_set_pd(double __a, double __b, double __c, double __d)
{
  return __extension__ (__m256d){ __d, __c, __b, __a };
}
# 3704 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_set_ps(float __a, float __b, float __c, float __d,
              float __e, float __f, float __g, float __h)
{
  return __extension__ (__m256){ __h, __g, __f, __e, __d, __c, __b, __a };
}
# 3736 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_set_epi32(int __i0, int __i1, int __i2, int __i3,
                 int __i4, int __i5, int __i6, int __i7)
{
  return __extension__ (__m256i)(__v8si){ __i7, __i6, __i5, __i4, __i3, __i2, __i1, __i0 };
}
# 3784 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_set_epi16(short __w15, short __w14, short __w13, short __w12,
                 short __w11, short __w10, short __w09, short __w08,
                 short __w07, short __w06, short __w05, short __w04,
                 short __w03, short __w02, short __w01, short __w00)
{
  return __extension__ (__m256i)(__v16hi){ __w00, __w01, __w02, __w03, __w04, __w05, __w06,
    __w07, __w08, __w09, __w10, __w11, __w12, __w13, __w14, __w15 };
}
# 3867 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_set_epi8(char __b31, char __b30, char __b29, char __b28,
                char __b27, char __b26, char __b25, char __b24,
                char __b23, char __b22, char __b21, char __b20,
                char __b19, char __b18, char __b17, char __b16,
                char __b15, char __b14, char __b13, char __b12,
                char __b11, char __b10, char __b09, char __b08,
                char __b07, char __b06, char __b05, char __b04,
                char __b03, char __b02, char __b01, char __b00)
{
  return __extension__ (__m256i)(__v32qi){
    __b00, __b01, __b02, __b03, __b04, __b05, __b06, __b07,
    __b08, __b09, __b10, __b11, __b12, __b13, __b14, __b15,
    __b16, __b17, __b18, __b19, __b20, __b21, __b22, __b23,
    __b24, __b25, __b26, __b27, __b28, __b29, __b30, __b31
  };
}
# 3902 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_set_epi64x(long long __a, long long __b, long long __c, long long __d)
{
  return __extension__ (__m256i)(__v4di){ __d, __c, __b, __a };
}
# 3931 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_setr_pd(double __a, double __b, double __c, double __d)
{
  return _mm256_set_pd(__d, __c, __b, __a);
}
# 3971 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_setr_ps(float __a, float __b, float __c, float __d,
               float __e, float __f, float __g, float __h)
{
  return _mm256_set_ps(__h, __g, __f, __e, __d, __c, __b, __a);
}
# 4003 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_setr_epi32(int __i0, int __i1, int __i2, int __i3,
                  int __i4, int __i5, int __i6, int __i7)
{
  return _mm256_set_epi32(__i7, __i6, __i5, __i4, __i3, __i2, __i1, __i0);
}
# 4051 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_setr_epi16(short __w15, short __w14, short __w13, short __w12,
       short __w11, short __w10, short __w09, short __w08,
       short __w07, short __w06, short __w05, short __w04,
       short __w03, short __w02, short __w01, short __w00)
{
  return _mm256_set_epi16(__w00, __w01, __w02, __w03,
                          __w04, __w05, __w06, __w07,
                          __w08, __w09, __w10, __w11,
                          __w12, __w13, __w14, __w15);
}
# 4136 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_setr_epi8(char __b31, char __b30, char __b29, char __b28,
                 char __b27, char __b26, char __b25, char __b24,
                 char __b23, char __b22, char __b21, char __b20,
                 char __b19, char __b18, char __b17, char __b16,
                 char __b15, char __b14, char __b13, char __b12,
                 char __b11, char __b10, char __b09, char __b08,
                 char __b07, char __b06, char __b05, char __b04,
                 char __b03, char __b02, char __b01, char __b00)
{
  return _mm256_set_epi8(__b00, __b01, __b02, __b03, __b04, __b05, __b06, __b07,
                         __b08, __b09, __b10, __b11, __b12, __b13, __b14, __b15,
                         __b16, __b17, __b18, __b19, __b20, __b21, __b22, __b23,
                         __b24, __b25, __b26, __b27, __b28, __b29, __b30, __b31);
}
# 4169 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_setr_epi64x(long long __a, long long __b, long long __c, long long __d)
{
  return _mm256_set_epi64x(__d, __c, __b, __a);
}
# 4188 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_set1_pd(double __w)
{
  return _mm256_set_pd(__w, __w, __w, __w);
}
# 4207 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_set1_ps(float __w)
{
  return _mm256_set_ps(__w, __w, __w, __w, __w, __w, __w, __w);
}
# 4226 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_set1_epi32(int __i)
{
  return _mm256_set_epi32(__i, __i, __i, __i, __i, __i, __i, __i);
}
# 4244 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_set1_epi16(short __w)
{
  return _mm256_set_epi16(__w, __w, __w, __w, __w, __w, __w, __w,
                          __w, __w, __w, __w, __w, __w, __w, __w);
}
# 4262 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_set1_epi8(char __b)
{
  return _mm256_set_epi8(__b, __b, __b, __b, __b, __b, __b, __b,
                         __b, __b, __b, __b, __b, __b, __b, __b,
                         __b, __b, __b, __b, __b, __b, __b, __b,
                         __b, __b, __b, __b, __b, __b, __b, __b);
}
# 4283 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_set1_epi64x(long long __q)
{
  return _mm256_set_epi64x(__q, __q, __q, __q);
}
# 4298 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_setzero_pd(void)
{
  return __extension__ (__m256d){ 0.0, 0.0, 0.0, 0.0 };
}
# 4312 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_setzero_ps(void)
{
  return __extension__ (__m256){ 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f };
}
# 4325 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_setzero_si256(void)
{
  return __extension__ (__m256i)(__v4di){ 0, 0, 0, 0 };
}
# 4343 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_castpd_ps(__m256d __a)
{
  return (__m256)__a;
}
# 4360 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_castpd_si256(__m256d __a)
{
  return (__m256i)__a;
}
# 4377 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_castps_pd(__m256 __a)
{
  return (__m256d)__a;
}
# 4394 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_castps_si256(__m256 __a)
{
  return (__m256i)__a;
}
# 4411 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_castsi256_ps(__m256i __a)
{
  return (__m256)__a;
}
# 4428 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_castsi256_pd(__m256i __a)
{
  return (__m256d)__a;
}
# 4445 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_castpd256_pd128(__m256d __a)
{
  return __builtin_shufflevector((__v4df)__a, (__v4df)__a, 0, 1);
}
# 4462 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_castps256_ps128(__m256 __a)
{
  return __builtin_shufflevector((__v8sf)__a, (__v8sf)__a, 0, 1, 2, 3);
}
# 4478 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_castsi256_si128(__m256i __a)
{
  return __builtin_shufflevector((__v4di)__a, (__v4di)__a, 0, 1);
}
# 4499 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_castpd128_pd256(__m128d __a)
{
  return __builtin_shufflevector((__v2df)__a, (__v2df)__a, 0, 1, -1, -1);
}
# 4520 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_castps128_ps256(__m128 __a)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)__a, 0, 1, 2, 3, -1, -1, -1, -1);
}
# 4539 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_castsi128_si256(__m128i __a)
{
  return __builtin_shufflevector((__v2di)__a, (__v2di)__a, 0, 1, -1, -1);
}
# 4558 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_zextpd128_pd256(__m128d __a)
{
  return __builtin_shufflevector((__v2df)__a, (__v2df)_mm_setzero_pd(), 0, 1, 2, 3);
}
# 4576 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_zextps128_ps256(__m128 __a)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)_mm_setzero_ps(), 0, 1, 2, 3, 4, 5, 6, 7);
}
# 4594 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_zextsi128_si256(__m128i __a)
{
  return __builtin_shufflevector((__v2di)__a, (__v2di)_mm_setzero_si128(), 0, 1, 2, 3);
}
# 4811 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_set_m128 (__m128 __hi, __m128 __lo)
{
  return (__m256) __builtin_shufflevector((__v4sf)__lo, (__v4sf)__hi, 0, 1, 2, 3, 4, 5, 6, 7);
}
# 4832 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_set_m128d (__m128d __hi, __m128d __lo)
{
  return (__m256d) __builtin_shufflevector((__v2df)__lo, (__v2df)__hi, 0, 1, 2, 3);
}
# 4852 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_set_m128i (__m128i __hi, __m128i __lo)
{
  return (__m256i) __builtin_shufflevector((__v2di)__lo, (__v2di)__hi, 0, 1, 2, 3);
}
# 4875 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_setr_m128 (__m128 __lo, __m128 __hi)
{
  return _mm256_set_m128(__hi, __lo);
}
# 4898 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_setr_m128d (__m128d __lo, __m128d __hi)
{
  return (__m256d)_mm256_set_m128d(__hi, __lo);
}
# 4919 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_setr_m128i (__m128i __lo, __m128i __hi)
{
  return (__m256i)_mm256_set_m128i(__hi, __lo);
}
# 4947 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_loadu2_m128(float const *__addr_hi, float const *__addr_lo)
{
  return _mm256_set_m128(_mm_loadu_ps(__addr_hi), _mm_loadu_ps(__addr_lo));
}
# 4974 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_loadu2_m128d(double const *__addr_hi, double const *__addr_lo)
{
  return _mm256_set_m128d(_mm_loadu_pd(__addr_hi), _mm_loadu_pd(__addr_lo));
}
# 4998 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_loadu2_m128i(__m128i_u const *__addr_hi, __m128i_u const *__addr_lo)
{
   return _mm256_set_m128i(_mm_loadu_si128(__addr_hi), _mm_loadu_si128(__addr_lo));
}
# 5023 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_storeu2_m128(float *__addr_hi, float *__addr_lo, __m256 __a)
{
  __m128 __v128;

  __v128 = _mm256_castps256_ps128(__a);
  _mm_storeu_ps(__addr_lo, __v128);
  __v128 = ((__m128)__builtin_ia32_vextractf128_ps256((__v8sf)(__m256)(__a), (int)(1)));
  _mm_storeu_ps(__addr_hi, __v128);
}
# 5052 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_storeu2_m128d(double *__addr_hi, double *__addr_lo, __m256d __a)
{
  __m128d __v128;

  __v128 = _mm256_castpd256_pd128(__a);
  _mm_storeu_pd(__addr_lo, __v128);
  __v128 = ((__m128d)__builtin_ia32_vextractf128_pd256((__v4df)(__m256d)(__a), (int)(1)));
  _mm_storeu_pd(__addr_hi, __v128);
}
# 5081 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxintrin.h" 3
static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx"), __min_vector_width__(256)))
_mm256_storeu2_m128i(__m128i_u *__addr_hi, __m128i_u *__addr_lo, __m256i __a)
{
  __m128i __v128;

  __v128 = _mm256_castsi256_si128(__a);
  _mm_storeu_si128(__addr_lo, __v128);
  __v128 = ((__m128i)__builtin_ia32_vextractf128_si256((__v8si)(__m256i)(__a), (int)(1)));
  _mm_storeu_si128(__addr_hi, __v128);
}
# 106 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx2intrin.h" 1 3
# 31 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx2intrin.h" 3
typedef enum
{
  _MM_SCALE_1 = 1,
  _MM_SCALE_2 = 2,
  _MM_SCALE_4 = 4,
  _MM_SCALE_8 = 8
} _MM_INDEX_SCALE_ENUM;
# 48 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_abs_epi8(__m256i __a)
{
    return (__m256i)__builtin_elementwise_abs((__v32qs)__a);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_abs_epi16(__m256i __a)
{
    return (__m256i)__builtin_elementwise_abs((__v16hi)__a);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_abs_epi32(__m256i __a)
{
    return (__m256i)__builtin_elementwise_abs((__v8si)__a);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_packs_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_packsswb256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_packs_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_packssdw256((__v8si)__a, (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_packus_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_packuswb256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_packus_epi32(__m256i __V1, __m256i __V2)
{
  return (__m256i) __builtin_ia32_packusdw256((__v8si)__V1, (__v8si)__V2);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_add_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)((__v32qu)__a + (__v32qu)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_add_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)((__v16hu)__a + (__v16hu)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_add_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)((__v8su)__a + (__v8su)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_add_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4du)__a + (__v4du)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_adds_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_add_sat((__v32qs)__a, (__v32qs)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_adds_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_add_sat((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_adds_epu8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_add_sat((__v32qu)__a, (__v32qu)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_adds_epu16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_add_sat((__v16hu)__a, (__v16hu)__b);
}





static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_and_si256(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4du)__a & (__v4du)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_andnot_si256(__m256i __a, __m256i __b)
{
  return (__m256i)(~(__v4du)__a & (__v4du)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_avg_epu8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pavgb256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_avg_epu16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pavgw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_blendv_epi8(__m256i __V1, __m256i __V2, __m256i __M)
{
  return (__m256i)__builtin_ia32_pblendvb256((__v32qi)__V1, (__v32qi)__V2,
                                              (__v32qi)__M);
}





static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_cmpeq_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)((__v32qi)__a == (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_cmpeq_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)((__v16hi)__a == (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_cmpeq_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)((__v8si)__a == (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_cmpeq_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4di)__a == (__v4di)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_cmpgt_epi8(__m256i __a, __m256i __b)
{


  return (__m256i)((__v32qs)__a > (__v32qs)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_cmpgt_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)((__v16hi)__a > (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_cmpgt_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)((__v8si)__a > (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_cmpgt_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4di)__a > (__v4di)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_hadd_epi16(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_phaddw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_hadd_epi32(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_phaddd256((__v8si)__a, (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_hadds_epi16(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_phaddsw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_hsub_epi16(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_phsubw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_hsub_epi32(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_phsubd256((__v8si)__a, (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_hsubs_epi16(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_phsubsw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_maddubs_epi16(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_pmaddubsw256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_madd_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmaddwd256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_max_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_max((__v32qs)__a, (__v32qs)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_max_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_max((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_max_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_max((__v8si)__a, (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_max_epu8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_max((__v32qu)__a, (__v32qu)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_max_epu16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_max((__v16hu)__a, (__v16hu)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_max_epu32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_max((__v8su)__a, (__v8su)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_min_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_min((__v32qs)__a, (__v32qs)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_min_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_min((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_min_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_min((__v8si)__a, (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_min_epu8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_min((__v32qu)__a, (__v32qu)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_min_epu16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_min((__v16hu)__a, (__v16hu)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_min_epu32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_min((__v8su)__a, (__v8su)__b);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_movemask_epi8(__m256i __a)
{
  return __builtin_ia32_pmovmskb256((__v32qi)__a);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_cvtepi8_epi16(__m128i __V)
{


  return (__m256i)__builtin_convertvector((__v16qs)__V, __v16hi);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_cvtepi8_epi32(__m128i __V)
{


  return (__m256i)__builtin_convertvector(__builtin_shufflevector((__v16qs)__V, (__v16qs)__V, 0, 1, 2, 3, 4, 5, 6, 7), __v8si);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_cvtepi8_epi64(__m128i __V)
{


  return (__m256i)__builtin_convertvector(__builtin_shufflevector((__v16qs)__V, (__v16qs)__V, 0, 1, 2, 3), __v4di);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_cvtepi16_epi32(__m128i __V)
{
  return (__m256i)__builtin_convertvector((__v8hi)__V, __v8si);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_cvtepi16_epi64(__m128i __V)
{
  return (__m256i)__builtin_convertvector(__builtin_shufflevector((__v8hi)__V, (__v8hi)__V, 0, 1, 2, 3), __v4di);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_cvtepi32_epi64(__m128i __V)
{
  return (__m256i)__builtin_convertvector((__v4si)__V, __v4di);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_cvtepu8_epi16(__m128i __V)
{
  return (__m256i)__builtin_convertvector((__v16qu)__V, __v16hi);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_cvtepu8_epi32(__m128i __V)
{
  return (__m256i)__builtin_convertvector(__builtin_shufflevector((__v16qu)__V, (__v16qu)__V, 0, 1, 2, 3, 4, 5, 6, 7), __v8si);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_cvtepu8_epi64(__m128i __V)
{
  return (__m256i)__builtin_convertvector(__builtin_shufflevector((__v16qu)__V, (__v16qu)__V, 0, 1, 2, 3), __v4di);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_cvtepu16_epi32(__m128i __V)
{
  return (__m256i)__builtin_convertvector((__v8hu)__V, __v8si);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_cvtepu16_epi64(__m128i __V)
{
  return (__m256i)__builtin_convertvector(__builtin_shufflevector((__v8hu)__V, (__v8hu)__V, 0, 1, 2, 3), __v4di);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_cvtepu32_epi64(__m128i __V)
{
  return (__m256i)__builtin_convertvector((__v4su)__V, __v4di);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_mul_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmuldq256((__v8si)__a, (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_mulhrs_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmulhrsw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_mulhi_epu16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmulhuw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_mulhi_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pmulhw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_mullo_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)((__v16hu)__a * (__v16hu)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_mullo_epi32 (__m256i __a, __m256i __b)
{
  return (__m256i)((__v8su)__a * (__v8su)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_mul_epu32(__m256i __a, __m256i __b)
{
  return __builtin_ia32_pmuludq256((__v8si)__a, (__v8si)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_or_si256(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4du)__a | (__v4du)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_sad_epu8(__m256i __a, __m256i __b)
{
  return __builtin_ia32_psadbw256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_shuffle_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_pshufb256((__v32qi)__a, (__v32qi)__b);
}
# 500 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_sign_epi8(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_psignb256((__v32qi)__a, (__v32qi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_sign_epi16(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_psignw256((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_sign_epi32(__m256i __a, __m256i __b)
{
    return (__m256i)__builtin_ia32_psignd256((__v8si)__a, (__v8si)__b);
}







static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_slli_epi16(__m256i __a, int __count)
{
  return (__m256i)__builtin_ia32_psllwi256((__v16hi)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_sll_epi16(__m256i __a, __m128i __count)
{
  return (__m256i)__builtin_ia32_psllw256((__v16hi)__a, (__v8hi)__count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_slli_epi32(__m256i __a, int __count)
{
  return (__m256i)__builtin_ia32_pslldi256((__v8si)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_sll_epi32(__m256i __a, __m128i __count)
{
  return (__m256i)__builtin_ia32_pslld256((__v8si)__a, (__v4si)__count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_slli_epi64(__m256i __a, int __count)
{
  return __builtin_ia32_psllqi256((__v4di)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_sll_epi64(__m256i __a, __m128i __count)
{
  return __builtin_ia32_psllq256((__v4di)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_srai_epi16(__m256i __a, int __count)
{
  return (__m256i)__builtin_ia32_psrawi256((__v16hi)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_sra_epi16(__m256i __a, __m128i __count)
{
  return (__m256i)__builtin_ia32_psraw256((__v16hi)__a, (__v8hi)__count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_srai_epi32(__m256i __a, int __count)
{
  return (__m256i)__builtin_ia32_psradi256((__v8si)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_sra_epi32(__m256i __a, __m128i __count)
{
  return (__m256i)__builtin_ia32_psrad256((__v8si)__a, (__v4si)__count);
}







static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_srli_epi16(__m256i __a, int __count)
{
  return (__m256i)__builtin_ia32_psrlwi256((__v16hi)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_srl_epi16(__m256i __a, __m128i __count)
{
  return (__m256i)__builtin_ia32_psrlw256((__v16hi)__a, (__v8hi)__count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_srli_epi32(__m256i __a, int __count)
{
  return (__m256i)__builtin_ia32_psrldi256((__v8si)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_srl_epi32(__m256i __a, __m128i __count)
{
  return (__m256i)__builtin_ia32_psrld256((__v8si)__a, (__v4si)__count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_srli_epi64(__m256i __a, int __count)
{
  return __builtin_ia32_psrlqi256((__v4di)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_srl_epi64(__m256i __a, __m128i __count)
{
  return __builtin_ia32_psrlq256((__v4di)__a, __count);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_sub_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)((__v32qu)__a - (__v32qu)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_sub_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)((__v16hu)__a - (__v16hu)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_sub_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)((__v8su)__a - (__v8su)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_sub_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4du)__a - (__v4du)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_subs_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_sub_sat((__v32qs)__a, (__v32qs)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_subs_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_sub_sat((__v16hi)__a, (__v16hi)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_subs_epu8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_sub_sat((__v32qu)__a, (__v32qu)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_subs_epu16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_elementwise_sub_sat((__v16hu)__a, (__v16hu)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_unpackhi_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v32qi)__a, (__v32qi)__b, 8, 32+8, 9, 32+9, 10, 32+10, 11, 32+11, 12, 32+12, 13, 32+13, 14, 32+14, 15, 32+15, 24, 32+24, 25, 32+25, 26, 32+26, 27, 32+27, 28, 32+28, 29, 32+29, 30, 32+30, 31, 32+31);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_unpackhi_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v16hi)__a, (__v16hi)__b, 4, 16+4, 5, 16+5, 6, 16+6, 7, 16+7, 12, 16+12, 13, 16+13, 14, 16+14, 15, 16+15);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_unpackhi_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v8si)__a, (__v8si)__b, 2, 8+2, 3, 8+3, 6, 8+6, 7, 8+7);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_unpackhi_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v4di)__a, (__v4di)__b, 1, 4+1, 3, 4+3);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_unpacklo_epi8(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v32qi)__a, (__v32qi)__b, 0, 32+0, 1, 32+1, 2, 32+2, 3, 32+3, 4, 32+4, 5, 32+5, 6, 32+6, 7, 32+7, 16, 32+16, 17, 32+17, 18, 32+18, 19, 32+19, 20, 32+20, 21, 32+21, 22, 32+22, 23, 32+23);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_unpacklo_epi16(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v16hi)__a, (__v16hi)__b, 0, 16+0, 1, 16+1, 2, 16+2, 3, 16+3, 8, 16+8, 9, 16+9, 10, 16+10, 11, 16+11);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_unpacklo_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v8si)__a, (__v8si)__b, 0, 8+0, 1, 8+1, 4, 8+4, 5, 8+5);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_unpacklo_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_shufflevector((__v4di)__a, (__v4di)__b, 0, 4+0, 2, 4+2);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_xor_si256(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4du)__a ^ (__v4du)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_stream_load_si256(__m256i const *__V)
{
  typedef __v4di __v4di_aligned __attribute__((aligned(32)));
  return (__m256i)__builtin_nontemporal_load((const __v4di_aligned *)__V);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(128)))
_mm_broadcastss_ps(__m128 __X)
{
  return (__m128)__builtin_shufflevector((__v4sf)__X, (__v4sf)__X, 0, 0, 0, 0);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(128)))
_mm_broadcastsd_pd(__m128d __a)
{
  return __builtin_shufflevector((__v2df)__a, (__v2df)__a, 0, 0);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_broadcastss_ps(__m128 __X)
{
  return (__m256)__builtin_shufflevector((__v4sf)__X, (__v4sf)__X, 0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_broadcastsd_pd(__m128d __X)
{
  return (__m256d)__builtin_shufflevector((__v2df)__X, (__v2df)__X, 0, 0, 0, 0);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_broadcastsi128_si256(__m128i __X)
{
  return (__m256i)__builtin_shufflevector((__v2di)__X, (__v2di)__X, 0, 1, 0, 1);
}
# 775 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_broadcastb_epi8(__m128i __X)
{
  return (__m256i)__builtin_shufflevector((__v16qi)__X, (__v16qi)__X, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_broadcastw_epi16(__m128i __X)
{
  return (__m256i)__builtin_shufflevector((__v8hi)__X, (__v8hi)__X, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_broadcastd_epi32(__m128i __X)
{
  return (__m256i)__builtin_shufflevector((__v4si)__X, (__v4si)__X, 0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_broadcastq_epi64(__m128i __X)
{
  return (__m256i)__builtin_shufflevector((__v2di)__X, (__v2di)__X, 0, 0, 0, 0);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(128)))
_mm_broadcastb_epi8(__m128i __X)
{
  return (__m128i)__builtin_shufflevector((__v16qi)__X, (__v16qi)__X, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(128)))
_mm_broadcastw_epi16(__m128i __X)
{
  return (__m128i)__builtin_shufflevector((__v8hi)__X, (__v8hi)__X, 0, 0, 0, 0, 0, 0, 0, 0);
}


static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(128)))
_mm_broadcastd_epi32(__m128i __X)
{
  return (__m128i)__builtin_shufflevector((__v4si)__X, (__v4si)__X, 0, 0, 0, 0);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(128)))
_mm_broadcastq_epi64(__m128i __X)
{
  return (__m128i)__builtin_shufflevector((__v2di)__X, (__v2di)__X, 0, 0);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_permutevar8x32_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)__builtin_ia32_permvarsi256((__v8si)__a, (__v8si)__b);
}




static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_permutevar8x32_ps(__m256 __a, __m256i __b)
{
  return (__m256)__builtin_ia32_permvarsf256((__v8sf)__a, (__v8si)__b);
}
# 852 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_maskload_epi32(int const *__X, __m256i __M)
{
  return (__m256i)__builtin_ia32_maskloadd256((const __v8si *)__X, (__v8si)__M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_maskload_epi64(long long const *__X, __m256i __M)
{
  return (__m256i)__builtin_ia32_maskloadq256((const __v4di *)__X, (__v4di)__M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(128)))
_mm_maskload_epi32(int const *__X, __m128i __M)
{
  return (__m128i)__builtin_ia32_maskloadd((const __v4si *)__X, (__v4si)__M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(128)))
_mm_maskload_epi64(long long const *__X, __m128i __M)
{
  return (__m128i)__builtin_ia32_maskloadq((const __v2di *)__X, (__v2di)__M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_maskstore_epi32(int *__X, __m256i __M, __m256i __Y)
{
  __builtin_ia32_maskstored256((__v8si *)__X, (__v8si)__M, (__v8si)__Y);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_maskstore_epi64(long long *__X, __m256i __M, __m256i __Y)
{
  __builtin_ia32_maskstoreq256((__v4di *)__X, (__v4di)__M, (__v4di)__Y);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(128)))
_mm_maskstore_epi32(int *__X, __m128i __M, __m128i __Y)
{
  __builtin_ia32_maskstored((__v4si *)__X, (__v4si)__M, (__v4si)__Y);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(128)))
_mm_maskstore_epi64(long long *__X, __m128i __M, __m128i __Y)
{
  __builtin_ia32_maskstoreq(( __v2di *)__X, (__v2di)__M, (__v2di)__Y);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_sllv_epi32(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_psllv8si((__v8si)__X, (__v8si)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(128)))
_mm_sllv_epi32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_psllv4si((__v4si)__X, (__v4si)__Y);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_sllv_epi64(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_psllv4di((__v4di)__X, (__v4di)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(128)))
_mm_sllv_epi64(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_psllv2di((__v2di)__X, (__v2di)__Y);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_srav_epi32(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_psrav8si((__v8si)__X, (__v8si)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(128)))
_mm_srav_epi32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_psrav4si((__v4si)__X, (__v4si)__Y);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_srlv_epi32(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_psrlv8si((__v8si)__X, (__v8si)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(128)))
_mm_srlv_epi32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_psrlv4si((__v4si)__X, (__v4si)__Y);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(256)))
_mm256_srlv_epi64(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_psrlv4di((__v4di)__X, (__v4di)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx2"), __min_vector_width__(128)))
_mm_srlv_epi64(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_psrlv2di((__v2di)__X, (__v2di)__Y);
}
# 111 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/f16cintrin.h" 1 3
# 38 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/f16cintrin.h" 3
static __inline float __attribute__((__always_inline__, __nodebug__, __target__("f16c"), __min_vector_width__(128)))
_cvtsh_ss(unsigned short __a)
{
  __v8hi __v = {(short)__a, 0, 0, 0, 0, 0, 0, 0};
  __v4sf __r = __builtin_ia32_vcvtph2ps(__v);
  return __r[0];
}
# 109 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/f16cintrin.h" 3
static __inline __m128 __attribute__((__always_inline__, __nodebug__, __target__("f16c"), __min_vector_width__(128)))
_mm_cvtph_ps(__m128i __a)
{
  return (__m128)__builtin_ia32_vcvtph2ps((__v8hi)__a);
}
# 153 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/f16cintrin.h" 3
static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("f16c"), __min_vector_width__(256)))
_mm256_cvtph_ps(__m128i __a)
{
  return (__m256)__builtin_ia32_vcvtph2ps256((__v8hi)__a);
}
# 116 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3



# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/bmiintrin.h" 1 3
# 47 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/bmiintrin.h" 3
static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__))
__tzcnt_u16(unsigned short __X)
{
  return __builtin_ia32_tzcnt_u16(__X);
}
# 64 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/bmiintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__))
__tzcnt_u32(unsigned int __X)
{
  return __builtin_ia32_tzcnt_u32(__X);
}
# 81 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/bmiintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__))
_mm_tzcnt_32(unsigned int __X)
{
  return (int)__builtin_ia32_tzcnt_u32(__X);
}
# 102 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/bmiintrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__tzcnt_u64(unsigned long long __X)
{
  return __builtin_ia32_tzcnt_u64(__X);
}
# 119 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/bmiintrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__))
_mm_tzcnt_64(unsigned long long __X)
{
  return (long long)__builtin_ia32_tzcnt_u64(__X);
}
# 159 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/bmiintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__andn_u32(unsigned int __X, unsigned int __Y)
{
  return ~__X & __Y;
}
# 182 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/bmiintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__bextr_u32(unsigned int __X, unsigned int __Y)
{
  return __builtin_ia32_bextr_u32(__X, __Y);
}
# 207 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/bmiintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
_bextr_u32(unsigned int __X, unsigned int __Y, unsigned int __Z)
{
  return __builtin_ia32_bextr_u32 (__X, ((__Y & 0xff) | ((__Z & 0xff) << 8)));
}
# 230 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/bmiintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
_bextr2_u32(unsigned int __X, unsigned int __Y) {
  return __builtin_ia32_bextr_u32(__X, __Y);
}
# 246 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/bmiintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__blsi_u32(unsigned int __X)
{
  return __X & -__X;
}
# 263 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/bmiintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__blsmsk_u32(unsigned int __X)
{
  return __X ^ (__X - 1);
}
# 280 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/bmiintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__blsr_u32(unsigned int __X)
{
  return __X & (__X - 1);
}
# 310 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/bmiintrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__andn_u64 (unsigned long long __X, unsigned long long __Y)
{
  return ~__X & __Y;
}
# 333 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/bmiintrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__bextr_u64(unsigned long long __X, unsigned long long __Y)
{
  return __builtin_ia32_bextr_u64(__X, __Y);
}
# 358 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/bmiintrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
_bextr_u64(unsigned long long __X, unsigned int __Y, unsigned int __Z)
{
  return __builtin_ia32_bextr_u64 (__X, ((__Y & 0xff) | ((__Z & 0xff) << 8)));
}
# 381 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/bmiintrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
_bextr2_u64(unsigned long long __X, unsigned long long __Y) {
  return __builtin_ia32_bextr_u64(__X, __Y);
}
# 397 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/bmiintrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__blsi_u64(unsigned long long __X)
{
  return __X & -__X;
}
# 414 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/bmiintrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__blsmsk_u64(unsigned long long __X)
{
  return __X ^ (__X - 1);
}
# 431 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/bmiintrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi")))
__blsr_u64(unsigned long long __X)
{
  return __X & (__X - 1);
}
# 120 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3



# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/bmi2intrin.h" 1 3
# 20 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/bmi2intrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi2")))
_bzhi_u32(unsigned int __X, unsigned int __Y)
{
  return __builtin_ia32_bzhi_si(__X, __Y);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi2")))
_pdep_u32(unsigned int __X, unsigned int __Y)
{
  return __builtin_ia32_pdep_si(__X, __Y);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("bmi2")))
_pext_u32(unsigned int __X, unsigned int __Y)
{
  return __builtin_ia32_pext_si(__X, __Y);
}



static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi2")))
_bzhi_u64(unsigned long long __X, unsigned long long __Y)
{
  return __builtin_ia32_bzhi_di(__X, __Y);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi2")))
_pdep_u64(unsigned long long __X, unsigned long long __Y)
{
  return __builtin_ia32_pdep_di(__X, __Y);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi2")))
_pext_u64(unsigned long long __X, unsigned long long __Y)
{
  return __builtin_ia32_pext_di(__X, __Y);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("bmi2")))
_mulx_u64 (unsigned long long __X, unsigned long long __Y,
    unsigned long long *__P)
{
  unsigned __int128 __res = (unsigned __int128) __X * __Y;
  *__P = (unsigned long long) (__res >> 64);
  return (unsigned long long) __res;
}
# 124 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/lzcntintrin.h" 1 3
# 45 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/lzcntintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("lzcnt")))
__lzcnt32(unsigned int __X)
{
  return __builtin_ia32_lzcnt_u32(__X);
}
# 62 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/lzcntintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("lzcnt")))
_lzcnt_u32(unsigned int __X)
{
  return __builtin_ia32_lzcnt_u32(__X);
}
# 95 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/lzcntintrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("lzcnt")))
_lzcnt_u64(unsigned long long __X)
{
  return __builtin_ia32_lzcnt_u64(__X);
}
# 129 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3
# 138 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 3
# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/fmaintrin.h" 1 3
# 21 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/fmaintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fmadd_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddps((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fmadd_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddpd((__v2df)__A, (__v2df)__B, (__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fmadd_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddss3((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fmadd_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsd3((__v2df)__A, (__v2df)__B, (__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fmsub_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddps((__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fmsub_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddpd((__v2df)__A, (__v2df)__B, -(__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fmsub_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddss3((__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fmsub_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsd3((__v2df)__A, (__v2df)__B, -(__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fnmadd_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddps(-(__v4sf)__A, (__v4sf)__B, (__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fnmadd_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddpd(-(__v2df)__A, (__v2df)__B, (__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fnmadd_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddss3((__v4sf)__A, -(__v4sf)__B, (__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fnmadd_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsd3((__v2df)__A, -(__v2df)__B, (__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fnmsub_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddps(-(__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fnmsub_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddpd(-(__v2df)__A, (__v2df)__B, -(__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fnmsub_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddss3((__v4sf)__A, -(__v4sf)__B, -(__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fnmsub_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsd3((__v2df)__A, -(__v2df)__B, -(__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fmaddsub_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddsubps((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fmaddsub_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsubpd((__v2df)__A, (__v2df)__B, (__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fmsubadd_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddsubps((__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(128)))
_mm_fmsubadd_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsubpd((__v2df)__A, (__v2df)__B, -(__v2df)__C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(256)))
_mm256_fmadd_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddps256((__v8sf)__A, (__v8sf)__B, (__v8sf)__C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(256)))
_mm256_fmadd_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddpd256((__v4df)__A, (__v4df)__B, (__v4df)__C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(256)))
_mm256_fmsub_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddps256((__v8sf)__A, (__v8sf)__B, -(__v8sf)__C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(256)))
_mm256_fmsub_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddpd256((__v4df)__A, (__v4df)__B, -(__v4df)__C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(256)))
_mm256_fnmadd_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddps256(-(__v8sf)__A, (__v8sf)__B, (__v8sf)__C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(256)))
_mm256_fnmadd_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddpd256(-(__v4df)__A, (__v4df)__B, (__v4df)__C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(256)))
_mm256_fnmsub_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddps256(-(__v8sf)__A, (__v8sf)__B, -(__v8sf)__C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(256)))
_mm256_fnmsub_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddpd256(-(__v4df)__A, (__v4df)__B, -(__v4df)__C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(256)))
_mm256_fmaddsub_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddsubps256((__v8sf)__A, (__v8sf)__B, (__v8sf)__C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(256)))
_mm256_fmaddsub_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddsubpd256((__v4df)__A, (__v4df)__B, (__v4df)__C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(256)))
_mm256_fmsubadd_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddsubps256((__v8sf)__A, (__v8sf)__B, -(__v8sf)__C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma"), __min_vector_width__(256)))
_mm256_fmsubadd_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddsubpd256((__v4df)__A, (__v4df)__B, -(__v4df)__C);
}
# 139 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 1 3
# 29 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
typedef char __v64qi __attribute__((__vector_size__(64)));
typedef short __v32hi __attribute__((__vector_size__(64)));
typedef double __v8df __attribute__((__vector_size__(64)));
typedef float __v16sf __attribute__((__vector_size__(64)));
typedef long long __v8di __attribute__((__vector_size__(64)));
typedef int __v16si __attribute__((__vector_size__(64)));


typedef unsigned char __v64qu __attribute__((__vector_size__(64)));
typedef unsigned short __v32hu __attribute__((__vector_size__(64)));
typedef unsigned long long __v8du __attribute__((__vector_size__(64)));
typedef unsigned int __v16su __attribute__((__vector_size__(64)));



typedef signed char __v64qs __attribute__((__vector_size__(64)));

typedef float __m512 __attribute__((__vector_size__(64), __aligned__(64)));
typedef double __m512d __attribute__((__vector_size__(64), __aligned__(64)));
typedef long long __m512i __attribute__((__vector_size__(64), __aligned__(64)));

typedef float __m512_u __attribute__((__vector_size__(64), __aligned__(1)));
typedef double __m512d_u __attribute__((__vector_size__(64), __aligned__(1)));
typedef long long __m512i_u __attribute__((__vector_size__(64), __aligned__(1)));

typedef unsigned char __mmask8;
typedef unsigned short __mmask16;


typedef _Float16 __v32hf __attribute__((__vector_size__(64), __aligned__(64)));
typedef _Float16 __m512h __attribute__((__vector_size__(64), __aligned__(64)));
typedef _Float16 __m512h_u __attribute__((__vector_size__(64), __aligned__(1)));
# 71 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
typedef enum {
    _MM_CMPINT_EQ,
    _MM_CMPINT_LT,
    _MM_CMPINT_LE,
    _MM_CMPINT_UNUSED,
    _MM_CMPINT_NE,
    _MM_CMPINT_NLT,

    _MM_CMPINT_NLE

} _MM_CMPINT_ENUM;

typedef enum
{
  _MM_PERM_AAAA = 0x00, _MM_PERM_AAAB = 0x01, _MM_PERM_AAAC = 0x02,
  _MM_PERM_AAAD = 0x03, _MM_PERM_AABA = 0x04, _MM_PERM_AABB = 0x05,
  _MM_PERM_AABC = 0x06, _MM_PERM_AABD = 0x07, _MM_PERM_AACA = 0x08,
  _MM_PERM_AACB = 0x09, _MM_PERM_AACC = 0x0A, _MM_PERM_AACD = 0x0B,
  _MM_PERM_AADA = 0x0C, _MM_PERM_AADB = 0x0D, _MM_PERM_AADC = 0x0E,
  _MM_PERM_AADD = 0x0F, _MM_PERM_ABAA = 0x10, _MM_PERM_ABAB = 0x11,
  _MM_PERM_ABAC = 0x12, _MM_PERM_ABAD = 0x13, _MM_PERM_ABBA = 0x14,
  _MM_PERM_ABBB = 0x15, _MM_PERM_ABBC = 0x16, _MM_PERM_ABBD = 0x17,
  _MM_PERM_ABCA = 0x18, _MM_PERM_ABCB = 0x19, _MM_PERM_ABCC = 0x1A,
  _MM_PERM_ABCD = 0x1B, _MM_PERM_ABDA = 0x1C, _MM_PERM_ABDB = 0x1D,
  _MM_PERM_ABDC = 0x1E, _MM_PERM_ABDD = 0x1F, _MM_PERM_ACAA = 0x20,
  _MM_PERM_ACAB = 0x21, _MM_PERM_ACAC = 0x22, _MM_PERM_ACAD = 0x23,
  _MM_PERM_ACBA = 0x24, _MM_PERM_ACBB = 0x25, _MM_PERM_ACBC = 0x26,
  _MM_PERM_ACBD = 0x27, _MM_PERM_ACCA = 0x28, _MM_PERM_ACCB = 0x29,
  _MM_PERM_ACCC = 0x2A, _MM_PERM_ACCD = 0x2B, _MM_PERM_ACDA = 0x2C,
  _MM_PERM_ACDB = 0x2D, _MM_PERM_ACDC = 0x2E, _MM_PERM_ACDD = 0x2F,
  _MM_PERM_ADAA = 0x30, _MM_PERM_ADAB = 0x31, _MM_PERM_ADAC = 0x32,
  _MM_PERM_ADAD = 0x33, _MM_PERM_ADBA = 0x34, _MM_PERM_ADBB = 0x35,
  _MM_PERM_ADBC = 0x36, _MM_PERM_ADBD = 0x37, _MM_PERM_ADCA = 0x38,
  _MM_PERM_ADCB = 0x39, _MM_PERM_ADCC = 0x3A, _MM_PERM_ADCD = 0x3B,
  _MM_PERM_ADDA = 0x3C, _MM_PERM_ADDB = 0x3D, _MM_PERM_ADDC = 0x3E,
  _MM_PERM_ADDD = 0x3F, _MM_PERM_BAAA = 0x40, _MM_PERM_BAAB = 0x41,
  _MM_PERM_BAAC = 0x42, _MM_PERM_BAAD = 0x43, _MM_PERM_BABA = 0x44,
  _MM_PERM_BABB = 0x45, _MM_PERM_BABC = 0x46, _MM_PERM_BABD = 0x47,
  _MM_PERM_BACA = 0x48, _MM_PERM_BACB = 0x49, _MM_PERM_BACC = 0x4A,
  _MM_PERM_BACD = 0x4B, _MM_PERM_BADA = 0x4C, _MM_PERM_BADB = 0x4D,
  _MM_PERM_BADC = 0x4E, _MM_PERM_BADD = 0x4F, _MM_PERM_BBAA = 0x50,
  _MM_PERM_BBAB = 0x51, _MM_PERM_BBAC = 0x52, _MM_PERM_BBAD = 0x53,
  _MM_PERM_BBBA = 0x54, _MM_PERM_BBBB = 0x55, _MM_PERM_BBBC = 0x56,
  _MM_PERM_BBBD = 0x57, _MM_PERM_BBCA = 0x58, _MM_PERM_BBCB = 0x59,
  _MM_PERM_BBCC = 0x5A, _MM_PERM_BBCD = 0x5B, _MM_PERM_BBDA = 0x5C,
  _MM_PERM_BBDB = 0x5D, _MM_PERM_BBDC = 0x5E, _MM_PERM_BBDD = 0x5F,
  _MM_PERM_BCAA = 0x60, _MM_PERM_BCAB = 0x61, _MM_PERM_BCAC = 0x62,
  _MM_PERM_BCAD = 0x63, _MM_PERM_BCBA = 0x64, _MM_PERM_BCBB = 0x65,
  _MM_PERM_BCBC = 0x66, _MM_PERM_BCBD = 0x67, _MM_PERM_BCCA = 0x68,
  _MM_PERM_BCCB = 0x69, _MM_PERM_BCCC = 0x6A, _MM_PERM_BCCD = 0x6B,
  _MM_PERM_BCDA = 0x6C, _MM_PERM_BCDB = 0x6D, _MM_PERM_BCDC = 0x6E,
  _MM_PERM_BCDD = 0x6F, _MM_PERM_BDAA = 0x70, _MM_PERM_BDAB = 0x71,
  _MM_PERM_BDAC = 0x72, _MM_PERM_BDAD = 0x73, _MM_PERM_BDBA = 0x74,
  _MM_PERM_BDBB = 0x75, _MM_PERM_BDBC = 0x76, _MM_PERM_BDBD = 0x77,
  _MM_PERM_BDCA = 0x78, _MM_PERM_BDCB = 0x79, _MM_PERM_BDCC = 0x7A,
  _MM_PERM_BDCD = 0x7B, _MM_PERM_BDDA = 0x7C, _MM_PERM_BDDB = 0x7D,
  _MM_PERM_BDDC = 0x7E, _MM_PERM_BDDD = 0x7F, _MM_PERM_CAAA = 0x80,
  _MM_PERM_CAAB = 0x81, _MM_PERM_CAAC = 0x82, _MM_PERM_CAAD = 0x83,
  _MM_PERM_CABA = 0x84, _MM_PERM_CABB = 0x85, _MM_PERM_CABC = 0x86,
  _MM_PERM_CABD = 0x87, _MM_PERM_CACA = 0x88, _MM_PERM_CACB = 0x89,
  _MM_PERM_CACC = 0x8A, _MM_PERM_CACD = 0x8B, _MM_PERM_CADA = 0x8C,
  _MM_PERM_CADB = 0x8D, _MM_PERM_CADC = 0x8E, _MM_PERM_CADD = 0x8F,
  _MM_PERM_CBAA = 0x90, _MM_PERM_CBAB = 0x91, _MM_PERM_CBAC = 0x92,
  _MM_PERM_CBAD = 0x93, _MM_PERM_CBBA = 0x94, _MM_PERM_CBBB = 0x95,
  _MM_PERM_CBBC = 0x96, _MM_PERM_CBBD = 0x97, _MM_PERM_CBCA = 0x98,
  _MM_PERM_CBCB = 0x99, _MM_PERM_CBCC = 0x9A, _MM_PERM_CBCD = 0x9B,
  _MM_PERM_CBDA = 0x9C, _MM_PERM_CBDB = 0x9D, _MM_PERM_CBDC = 0x9E,
  _MM_PERM_CBDD = 0x9F, _MM_PERM_CCAA = 0xA0, _MM_PERM_CCAB = 0xA1,
  _MM_PERM_CCAC = 0xA2, _MM_PERM_CCAD = 0xA3, _MM_PERM_CCBA = 0xA4,
  _MM_PERM_CCBB = 0xA5, _MM_PERM_CCBC = 0xA6, _MM_PERM_CCBD = 0xA7,
  _MM_PERM_CCCA = 0xA8, _MM_PERM_CCCB = 0xA9, _MM_PERM_CCCC = 0xAA,
  _MM_PERM_CCCD = 0xAB, _MM_PERM_CCDA = 0xAC, _MM_PERM_CCDB = 0xAD,
  _MM_PERM_CCDC = 0xAE, _MM_PERM_CCDD = 0xAF, _MM_PERM_CDAA = 0xB0,
  _MM_PERM_CDAB = 0xB1, _MM_PERM_CDAC = 0xB2, _MM_PERM_CDAD = 0xB3,
  _MM_PERM_CDBA = 0xB4, _MM_PERM_CDBB = 0xB5, _MM_PERM_CDBC = 0xB6,
  _MM_PERM_CDBD = 0xB7, _MM_PERM_CDCA = 0xB8, _MM_PERM_CDCB = 0xB9,
  _MM_PERM_CDCC = 0xBA, _MM_PERM_CDCD = 0xBB, _MM_PERM_CDDA = 0xBC,
  _MM_PERM_CDDB = 0xBD, _MM_PERM_CDDC = 0xBE, _MM_PERM_CDDD = 0xBF,
  _MM_PERM_DAAA = 0xC0, _MM_PERM_DAAB = 0xC1, _MM_PERM_DAAC = 0xC2,
  _MM_PERM_DAAD = 0xC3, _MM_PERM_DABA = 0xC4, _MM_PERM_DABB = 0xC5,
  _MM_PERM_DABC = 0xC6, _MM_PERM_DABD = 0xC7, _MM_PERM_DACA = 0xC8,
  _MM_PERM_DACB = 0xC9, _MM_PERM_DACC = 0xCA, _MM_PERM_DACD = 0xCB,
  _MM_PERM_DADA = 0xCC, _MM_PERM_DADB = 0xCD, _MM_PERM_DADC = 0xCE,
  _MM_PERM_DADD = 0xCF, _MM_PERM_DBAA = 0xD0, _MM_PERM_DBAB = 0xD1,
  _MM_PERM_DBAC = 0xD2, _MM_PERM_DBAD = 0xD3, _MM_PERM_DBBA = 0xD4,
  _MM_PERM_DBBB = 0xD5, _MM_PERM_DBBC = 0xD6, _MM_PERM_DBBD = 0xD7,
  _MM_PERM_DBCA = 0xD8, _MM_PERM_DBCB = 0xD9, _MM_PERM_DBCC = 0xDA,
  _MM_PERM_DBCD = 0xDB, _MM_PERM_DBDA = 0xDC, _MM_PERM_DBDB = 0xDD,
  _MM_PERM_DBDC = 0xDE, _MM_PERM_DBDD = 0xDF, _MM_PERM_DCAA = 0xE0,
  _MM_PERM_DCAB = 0xE1, _MM_PERM_DCAC = 0xE2, _MM_PERM_DCAD = 0xE3,
  _MM_PERM_DCBA = 0xE4, _MM_PERM_DCBB = 0xE5, _MM_PERM_DCBC = 0xE6,
  _MM_PERM_DCBD = 0xE7, _MM_PERM_DCCA = 0xE8, _MM_PERM_DCCB = 0xE9,
  _MM_PERM_DCCC = 0xEA, _MM_PERM_DCCD = 0xEB, _MM_PERM_DCDA = 0xEC,
  _MM_PERM_DCDB = 0xED, _MM_PERM_DCDC = 0xEE, _MM_PERM_DCDD = 0xEF,
  _MM_PERM_DDAA = 0xF0, _MM_PERM_DDAB = 0xF1, _MM_PERM_DDAC = 0xF2,
  _MM_PERM_DDAD = 0xF3, _MM_PERM_DDBA = 0xF4, _MM_PERM_DDBB = 0xF5,
  _MM_PERM_DDBC = 0xF6, _MM_PERM_DDBD = 0xF7, _MM_PERM_DDCA = 0xF8,
  _MM_PERM_DDCB = 0xF9, _MM_PERM_DDCC = 0xFA, _MM_PERM_DDCD = 0xFB,
  _MM_PERM_DDDA = 0xFC, _MM_PERM_DDDB = 0xFD, _MM_PERM_DDDC = 0xFE,
  _MM_PERM_DDDD = 0xFF
} _MM_PERM_ENUM;

typedef enum
{
  _MM_MANT_NORM_1_2,
  _MM_MANT_NORM_p5_2,
  _MM_MANT_NORM_p5_1,
  _MM_MANT_NORM_p75_1p5
} _MM_MANTISSA_NORM_ENUM;

typedef enum
{
  _MM_MANT_SIGN_src,
  _MM_MANT_SIGN_zero,
  _MM_MANT_SIGN_nan
} _MM_MANTISSA_SIGN_ENUM;
# 196 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_setzero_si512(void)
{
  return __extension__ (__m512i)(__v8di){ 0, 0, 0, 0, 0, 0, 0, 0 };
}



static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_undefined_pd(void)
{
  return (__m512d)__builtin_ia32_undef512();
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_undefined(void)
{
  return (__m512)__builtin_ia32_undef512();
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_undefined_ps(void)
{
  return (__m512)__builtin_ia32_undef512();
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_undefined_epi32(void)
{
  return (__m512i)__builtin_ia32_undef512();
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_broadcastd_epi32 (__m128i __A)
{
  return (__m512i)__builtin_shufflevector((__v4si) __A, (__v4si) __A,
                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_broadcastd_epi32 (__m512i __O, __mmask16 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512(__M,
                                             (__v16si) _mm512_broadcastd_epi32(__A),
                                             (__v16si) __O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_broadcastd_epi32 (__mmask16 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512(__M,
                                             (__v16si) _mm512_broadcastd_epi32(__A),
                                             (__v16si) _mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_broadcastq_epi64 (__m128i __A)
{
  return (__m512i)__builtin_shufflevector((__v2di) __A, (__v2di) __A,
                                          0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_broadcastq_epi64 (__m512i __O, __mmask8 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512(__M,
                                             (__v8di) _mm512_broadcastq_epi64(__A),
                                             (__v8di) __O);

}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_broadcastq_epi64 (__mmask8 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512(__M,
                                             (__v8di) _mm512_broadcastq_epi64(__A),
                                             (__v8di) _mm512_setzero_si512());
}


static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_setzero_ps(void)
{
  return __extension__ (__m512){ 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f,
                                 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f };
}



static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_setzero_pd(void)
{
  return __extension__ (__m512d){ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 };
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_set1_ps(float __w)
{
  return __extension__ (__m512){ __w, __w, __w, __w, __w, __w, __w, __w,
                                 __w, __w, __w, __w, __w, __w, __w, __w };
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_set1_pd(double __w)
{
  return __extension__ (__m512d){ __w, __w, __w, __w, __w, __w, __w, __w };
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_set1_epi8(char __w)
{
  return __extension__ (__m512i)(__v64qi){
    __w, __w, __w, __w, __w, __w, __w, __w,
    __w, __w, __w, __w, __w, __w, __w, __w,
    __w, __w, __w, __w, __w, __w, __w, __w,
    __w, __w, __w, __w, __w, __w, __w, __w,
    __w, __w, __w, __w, __w, __w, __w, __w,
    __w, __w, __w, __w, __w, __w, __w, __w,
    __w, __w, __w, __w, __w, __w, __w, __w,
    __w, __w, __w, __w, __w, __w, __w, __w };
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_set1_epi16(short __w)
{
  return __extension__ (__m512i)(__v32hi){
    __w, __w, __w, __w, __w, __w, __w, __w,
    __w, __w, __w, __w, __w, __w, __w, __w,
    __w, __w, __w, __w, __w, __w, __w, __w,
    __w, __w, __w, __w, __w, __w, __w, __w };
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_set1_epi32(int __s)
{
  return __extension__ (__m512i)(__v16si){
    __s, __s, __s, __s, __s, __s, __s, __s,
    __s, __s, __s, __s, __s, __s, __s, __s };
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_set1_epi32(__mmask16 __M, int __A)
{
  return (__m512i)__builtin_ia32_selectd_512(__M,
                                             (__v16si)_mm512_set1_epi32(__A),
                                             (__v16si)_mm512_setzero_si512());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_set1_epi64(long long __d)
{
  return __extension__(__m512i)(__v8di){ __d, __d, __d, __d, __d, __d, __d, __d };
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_set1_epi64(__mmask8 __M, long long __A)
{
  return (__m512i)__builtin_ia32_selectq_512(__M,
                                             (__v8di)_mm512_set1_epi64(__A),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_broadcastss_ps(__m128 __A)
{
  return (__m512)__builtin_shufflevector((__v4sf) __A, (__v4sf) __A,
                                         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_set4_epi32 (int __A, int __B, int __C, int __D)
{
  return __extension__ (__m512i)(__v16si)
   { __D, __C, __B, __A, __D, __C, __B, __A,
     __D, __C, __B, __A, __D, __C, __B, __A };
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_set4_epi64 (long long __A, long long __B, long long __C,
       long long __D)
{
  return __extension__ (__m512i) (__v8di)
   { __D, __C, __B, __A, __D, __C, __B, __A };
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_set4_pd (double __A, double __B, double __C, double __D)
{
  return __extension__ (__m512d)
   { __D, __C, __B, __A, __D, __C, __B, __A };
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_set4_ps (float __A, float __B, float __C, float __D)
{
  return __extension__ (__m512)
   { __D, __C, __B, __A, __D, __C, __B, __A,
     __D, __C, __B, __A, __D, __C, __B, __A };
}
# 408 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_broadcastsd_pd(__m128d __A)
{
  return (__m512d)__builtin_shufflevector((__v2df) __A, (__v2df) __A,
                                          0, 0, 0, 0, 0, 0, 0, 0);
}



static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_castpd256_pd512(__m256d __a)
{
  return __builtin_shufflevector(__a, __a, 0, 1, 2, 3, -1, -1, -1, -1);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_castps256_ps512(__m256 __a)
{
  return __builtin_shufflevector(__a, __a, 0, 1, 2, 3, 4, 5, 6, 7,
                                          -1, -1, -1, -1, -1, -1, -1, -1);
}

static __inline __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_castpd512_pd128(__m512d __a)
{
  return __builtin_shufflevector(__a, __a, 0, 1);
}

static __inline __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_castpd512_pd256 (__m512d __A)
{
  return __builtin_shufflevector(__A, __A, 0, 1, 2, 3);
}

static __inline __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_castps512_ps128(__m512 __a)
{
  return __builtin_shufflevector(__a, __a, 0, 1, 2, 3);
}

static __inline __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_castps512_ps256 (__m512 __A)
{
  return __builtin_shufflevector(__A, __A, 0, 1, 2, 3, 4, 5, 6, 7);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_castpd_ps (__m512d __A)
{
  return (__m512) (__A);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_castpd_si512 (__m512d __A)
{
  return (__m512i) (__A);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_castpd128_pd512 (__m128d __A)
{
  return __builtin_shufflevector( __A, __A, 0, 1, -1, -1, -1, -1, -1, -1);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_castps_pd (__m512 __A)
{
  return (__m512d) (__A);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_castps_si512 (__m512 __A)
{
  return (__m512i) (__A);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_castps128_ps512 (__m128 __A)
{
    return __builtin_shufflevector( __A, __A, 0, 1, 2, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_castsi128_si512 (__m128i __A)
{
   return __builtin_shufflevector( __A, __A, 0, 1, -1, -1, -1, -1, -1, -1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_castsi256_si512 (__m256i __A)
{
   return __builtin_shufflevector( __A, __A, 0, 1, 2, 3, -1, -1, -1, -1);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_castsi512_ps (__m512i __A)
{
  return (__m512) (__A);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_castsi512_pd (__m512i __A)
{
  return (__m512d) (__A);
}

static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_castsi512_si128 (__m512i __A)
{
  return (__m128i)__builtin_shufflevector(__A, __A , 0, 1);
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_castsi512_si256 (__m512i __A)
{
  return (__m256i)__builtin_shufflevector(__A, __A , 0, 1, 2, 3);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_int2mask(int __a)
{
  return (__mmask16)__a;
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_mask2int(__mmask16 __a)
{
  return (int)__a;
}
# 551 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_zextpd128_pd512(__m128d __a)
{
  return __builtin_shufflevector((__v2df)__a, (__v2df)_mm_setzero_pd(), 0, 1, 2, 3, 2, 3, 2, 3);
}
# 570 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_zextpd256_pd512(__m256d __a)
{
  return __builtin_shufflevector((__v4df)__a, (__v4df)_mm256_setzero_pd(), 0, 1, 2, 3, 4, 5, 6, 7);
}
# 588 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_zextps128_ps512(__m128 __a)
{
  return __builtin_shufflevector((__v4sf)__a, (__v4sf)_mm_setzero_ps(), 0, 1, 2, 3, 4, 5, 6, 7, 4, 5, 6, 7, 4, 5, 6, 7);
}
# 606 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_zextps256_ps512(__m256 __a)
{
  return __builtin_shufflevector((__v8sf)__a, (__v8sf)_mm256_setzero_ps(), 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);
}
# 624 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_zextsi128_si512(__m128i __a)
{
  return __builtin_shufflevector((__v2di)__a, (__v2di)_mm_setzero_si128(), 0, 1, 2, 3, 2, 3, 2, 3);
}
# 642 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_zextsi256_si512(__m256i __a)
{
  return __builtin_shufflevector((__v4di)__a, (__v4di)_mm256_setzero_si256(), 0, 1, 2, 3, 4, 5, 6, 7);
}


static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_and_epi32(__m512i __a, __m512i __b)
{
  return (__m512i)((__v16su)__a & (__v16su)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_and_epi32(__m512i __src, __mmask16 __k, __m512i __a, __m512i __b)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__k,
                (__v16si) _mm512_and_epi32(__a, __b),
                (__v16si) __src);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_and_epi32(__mmask16 __k, __m512i __a, __m512i __b)
{
  return (__m512i) _mm512_mask_and_epi32(_mm512_setzero_si512 (),
                                         __k, __a, __b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_and_epi64(__m512i __a, __m512i __b)
{
  return (__m512i)((__v8du)__a & (__v8du)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_and_epi64(__m512i __src, __mmask8 __k, __m512i __a, __m512i __b)
{
    return (__m512i) __builtin_ia32_selectq_512 ((__mmask8) __k,
                (__v8di) _mm512_and_epi64(__a, __b),
                (__v8di) __src);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_and_epi64(__mmask8 __k, __m512i __a, __m512i __b)
{
  return (__m512i) _mm512_mask_and_epi64(_mm512_setzero_si512 (),
                                         __k, __a, __b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_andnot_si512 (__m512i __A, __m512i __B)
{
  return (__m512i)(~(__v8du)__A & (__v8du)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_andnot_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i)(~(__v16su)__A & (__v16su)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_andnot_epi32(__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                         (__v16si)_mm512_andnot_epi32(__A, __B),
                                         (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_andnot_epi32(__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)_mm512_mask_andnot_epi32(_mm512_setzero_si512(),
                                           __U, __A, __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_andnot_epi64(__m512i __A, __m512i __B)
{
  return (__m512i)(~(__v8du)__A & (__v8du)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_andnot_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                          (__v8di)_mm512_andnot_epi64(__A, __B),
                                          (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_andnot_epi64(__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)_mm512_mask_andnot_epi64(_mm512_setzero_si512(),
                                           __U, __A, __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_or_epi32(__m512i __a, __m512i __b)
{
  return (__m512i)((__v16su)__a | (__v16su)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_or_epi32(__m512i __src, __mmask16 __k, __m512i __a, __m512i __b)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__k,
                                             (__v16si)_mm512_or_epi32(__a, __b),
                                             (__v16si)__src);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_or_epi32(__mmask16 __k, __m512i __a, __m512i __b)
{
  return (__m512i)_mm512_mask_or_epi32(_mm512_setzero_si512(), __k, __a, __b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_or_epi64(__m512i __a, __m512i __b)
{
  return (__m512i)((__v8du)__a | (__v8du)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_or_epi64(__m512i __src, __mmask8 __k, __m512i __a, __m512i __b)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__k,
                                             (__v8di)_mm512_or_epi64(__a, __b),
                                             (__v8di)__src);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_or_epi64(__mmask8 __k, __m512i __a, __m512i __b)
{
  return (__m512i)_mm512_mask_or_epi64(_mm512_setzero_si512(), __k, __a, __b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_xor_epi32(__m512i __a, __m512i __b)
{
  return (__m512i)((__v16su)__a ^ (__v16su)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_xor_epi32(__m512i __src, __mmask16 __k, __m512i __a, __m512i __b)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__k,
                                            (__v16si)_mm512_xor_epi32(__a, __b),
                                            (__v16si)__src);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_xor_epi32(__mmask16 __k, __m512i __a, __m512i __b)
{
  return (__m512i)_mm512_mask_xor_epi32(_mm512_setzero_si512(), __k, __a, __b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_xor_epi64(__m512i __a, __m512i __b)
{
  return (__m512i)((__v8du)__a ^ (__v8du)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_xor_epi64(__m512i __src, __mmask8 __k, __m512i __a, __m512i __b)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__k,
                                             (__v8di)_mm512_xor_epi64(__a, __b),
                                             (__v8di)__src);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_xor_epi64(__mmask8 __k, __m512i __a, __m512i __b)
{
  return (__m512i)_mm512_mask_xor_epi64(_mm512_setzero_si512(), __k, __a, __b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_and_si512(__m512i __a, __m512i __b)
{
  return (__m512i)((__v8du)__a & (__v8du)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_or_si512(__m512i __a, __m512i __b)
{
  return (__m512i)((__v8du)__a | (__v8du)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_xor_si512(__m512i __a, __m512i __b)
{
  return (__m512i)((__v8du)__a ^ (__v8du)__b);
}



static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_add_pd(__m512d __a, __m512d __b)
{
  return (__m512d)((__v8df)__a + (__v8df)__b);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_add_ps(__m512 __a, __m512 __b)
{
  return (__m512)((__v16sf)__a + (__v16sf)__b);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mul_pd(__m512d __a, __m512d __b)
{
  return (__m512d)((__v8df)__a * (__v8df)__b);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mul_ps(__m512 __a, __m512 __b)
{
  return (__m512)((__v16sf)__a * (__v16sf)__b);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_sub_pd(__m512d __a, __m512d __b)
{
  return (__m512d)((__v8df)__a - (__v8df)__b);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_sub_ps(__m512 __a, __m512 __b)
{
  return (__m512)((__v16sf)__a - (__v16sf)__b);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_add_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v8du) __A + (__v8du) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_add_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_add_epi64(__A, __B),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_add_epi64(__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_add_epi64(__A, __B),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_sub_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v8du) __A - (__v8du) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_sub_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_sub_epi64(__A, __B),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_sub_epi64(__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_sub_epi64(__A, __B),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_add_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v16su) __A + (__v16su) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_add_epi32(__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_add_epi32(__A, __B),
                                             (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_add_epi32 (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_add_epi32(__A, __B),
                                             (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_sub_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v16su) __A - (__v16su) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_sub_epi32(__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_sub_epi32(__A, __B),
                                             (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_sub_epi32(__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_sub_epi32(__A, __B),
                                             (__v16si)_mm512_setzero_si512());
}
# 977 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_max_pd(__m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_maxpd512((__v8df) __A, (__v8df) __B,
                                           0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_max_pd (__m512d __W, __mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512(__U,
                                              (__v8df)_mm512_max_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_max_pd (__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512(__U,
                                              (__v8df)_mm512_max_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}
# 1014 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_max_ps(__m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_maxps512((__v16sf) __A, (__v16sf) __B,
                                          0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_max_ps (__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512(__U,
                                             (__v16sf)_mm512_max_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_max_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512(__U,
                                             (__v16sf)_mm512_max_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_max_ss(__m128 __W, __mmask8 __U,__m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_maxss_round_mask ((__v4sf) __A,
                (__v4sf) __B,
                (__v4sf) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_max_ss(__mmask8 __U,__m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_maxss_round_mask ((__v4sf) __A,
                (__v4sf) __B,
                (__v4sf) _mm_setzero_ps (),
                (__mmask8) __U,
                0x04);
}
# 1073 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_max_sd(__m128d __W, __mmask8 __U,__m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_maxsd_round_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_max_sd(__mmask8 __U,__m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_maxsd_round_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df) _mm_setzero_pd (),
                (__mmask8) __U,
                0x04);
}
# 1109 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline __m512i
__attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_max_epi32(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_max((__v16si)__A, (__v16si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_max_epi32 (__m512i __W, __mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                            (__v16si)_mm512_max_epi32(__A, __B),
                                            (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_max_epi32 (__mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                            (__v16si)_mm512_max_epi32(__A, __B),
                                            (__v16si)_mm512_setzero_si512());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_max_epu32(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_max((__v16su)__A, (__v16su)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_max_epu32 (__m512i __W, __mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                            (__v16si)_mm512_max_epu32(__A, __B),
                                            (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_max_epu32 (__mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                            (__v16si)_mm512_max_epu32(__A, __B),
                                            (__v16si)_mm512_setzero_si512());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_max_epi64(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_max((__v8di)__A, (__v8di)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_max_epi64 (__m512i __W, __mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_max_epi64(__A, __B),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_max_epi64 (__mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_max_epi64(__A, __B),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_max_epu64(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_max((__v8du)__A, (__v8du)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_max_epu64 (__m512i __W, __mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_max_epu64(__A, __B),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_max_epu64 (__mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_max_epu64(__A, __B),
                                             (__v8di)_mm512_setzero_si512());
}
# 1212 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_min_pd(__m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_minpd512((__v8df) __A, (__v8df) __B,
                                           0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_min_pd (__m512d __W, __mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512(__U,
                                              (__v8df)_mm512_min_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_min_pd (__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512(__U,
                                              (__v8df)_mm512_min_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}
# 1249 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_min_ps(__m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_minps512((__v16sf) __A, (__v16sf) __B,
                                          0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_min_ps (__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512(__U,
                                             (__v16sf)_mm512_min_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_min_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512(__U,
                                             (__v16sf)_mm512_min_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_min_ss(__m128 __W, __mmask8 __U,__m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_minss_round_mask ((__v4sf) __A,
                (__v4sf) __B,
                (__v4sf) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_min_ss(__mmask8 __U,__m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_minss_round_mask ((__v4sf) __A,
                (__v4sf) __B,
                (__v4sf) _mm_setzero_ps (),
                (__mmask8) __U,
                0x04);
}
# 1308 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_min_sd(__m128d __W, __mmask8 __U,__m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_minsd_round_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_min_sd(__mmask8 __U,__m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_minsd_round_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df) _mm_setzero_pd (),
                (__mmask8) __U,
                0x04);
}
# 1344 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline __m512i
__attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_min_epi32(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_min((__v16si)__A, (__v16si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_min_epi32 (__m512i __W, __mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                            (__v16si)_mm512_min_epi32(__A, __B),
                                            (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_min_epi32 (__mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                            (__v16si)_mm512_min_epi32(__A, __B),
                                            (__v16si)_mm512_setzero_si512());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_min_epu32(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_min((__v16su)__A, (__v16su)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_min_epu32 (__m512i __W, __mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                            (__v16si)_mm512_min_epu32(__A, __B),
                                            (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_min_epu32 (__mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                            (__v16si)_mm512_min_epu32(__A, __B),
                                            (__v16si)_mm512_setzero_si512());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_min_epi64(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_min((__v8di)__A, (__v8di)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_min_epi64 (__m512i __W, __mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_min_epi64(__A, __B),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_min_epi64 (__mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_min_epi64(__A, __B),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_min_epu64(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_min((__v8du)__A, (__v8du)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_min_epu64 (__m512i __W, __mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_min_epu64(__A, __B),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_min_epu64 (__mmask8 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_min_epu64(__A, __B),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mul_epi32(__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_pmuldq512((__v16si)__X, (__v16si) __Y);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_mul_epi32(__m512i __W, __mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_mul_epi32(__X, __Y),
                                             (__v8di)__W);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_mul_epi32(__mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_mul_epi32(__X, __Y),
                                             (__v8di)_mm512_setzero_si512 ());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mul_epu32(__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_pmuludq512((__v16si)__X, (__v16si)__Y);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_mul_epu32(__m512i __W, __mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_mul_epu32(__X, __Y),
                                             (__v8di)__W);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_mul_epu32(__mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                             (__v8di)_mm512_mul_epu32(__X, __Y),
                                             (__v8di)_mm512_setzero_si512 ());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mullo_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i) ((__v16su) __A * (__v16su) __B);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_mullo_epi32(__mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                             (__v16si)_mm512_mullo_epi32(__A, __B),
                                             (__v16si)_mm512_setzero_si512());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_mullo_epi32(__m512i __W, __mmask16 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                             (__v16si)_mm512_mullo_epi32(__A, __B),
                                             (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mullox_epi64 (__m512i __A, __m512i __B) {
  return (__m512i) ((__v8du) __A * (__v8du) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_mullox_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_mullox_epi64(__A, __B),
                                             (__v8di)__W);
}
# 1524 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_sqrt_pd(__m512d __A)
{
  return (__m512d)__builtin_ia32_sqrtpd512((__v8df)__A,
                                           0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_sqrt_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512(__U,
                                              (__v8df)_mm512_sqrt_pd(__A),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_sqrt_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512(__U,
                                              (__v8df)_mm512_sqrt_pd(__A),
                                              (__v8df)_mm512_setzero_pd());
}
# 1560 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_sqrt_ps(__m512 __A)
{
  return (__m512)__builtin_ia32_sqrtps512((__v16sf)__A,
                                          0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_sqrt_ps(__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512)__builtin_ia32_selectps_512(__U,
                                             (__v16sf)_mm512_sqrt_ps(__A),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_sqrt_ps( __mmask16 __U, __m512 __A)
{
  return (__m512)__builtin_ia32_selectps_512(__U,
                                             (__v16sf)_mm512_sqrt_ps(__A),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_rsqrt14_pd(__m512d __A)
{
  return (__m512d) __builtin_ia32_rsqrt14pd512_mask ((__v8df) __A,
                 (__v8df)
                 _mm512_setzero_pd (),
                 (__mmask8) -1);}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_rsqrt14_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_rsqrt14pd512_mask ((__v8df) __A,
                  (__v8df) __W,
                  (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_rsqrt14_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_rsqrt14pd512_mask ((__v8df) __A,
                  (__v8df)
                  _mm512_setzero_pd (),
                  (__mmask8) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_rsqrt14_ps(__m512 __A)
{
  return (__m512) __builtin_ia32_rsqrt14ps512_mask ((__v16sf) __A,
                (__v16sf)
                _mm512_setzero_ps (),
                (__mmask16) -1);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_rsqrt14_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_rsqrt14ps512_mask ((__v16sf) __A,
                 (__v16sf) __W,
                 (__mmask16) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_rsqrt14_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_rsqrt14ps512_mask ((__v16sf) __A,
                 (__v16sf)
                 _mm512_setzero_ps (),
                 (__mmask16) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_rsqrt14_ss(__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_rsqrt14ss_mask ((__v4sf) __A,
             (__v4sf) __B,
             (__v4sf)
             _mm_setzero_ps (),
             (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_rsqrt14_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_rsqrt14ss_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) __W,
          (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_rsqrt14_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_rsqrt14ss_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) _mm_setzero_ps (),
          (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_rsqrt14_sd(__m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_rsqrt14sd_mask ((__v2df) __A,
              (__v2df) __B,
              (__v2df)
              _mm_setzero_pd (),
              (__mmask8) -1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_rsqrt14_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_rsqrt14sd_mask ( (__v2df) __A,
          (__v2df) __B,
          (__v2df) __W,
          (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_rsqrt14_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_rsqrt14sd_mask ( (__v2df) __A,
          (__v2df) __B,
          (__v2df) _mm_setzero_pd (),
          (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_rcp14_pd(__m512d __A)
{
  return (__m512d) __builtin_ia32_rcp14pd512_mask ((__v8df) __A,
               (__v8df)
               _mm512_setzero_pd (),
               (__mmask8) -1);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_rcp14_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_rcp14pd512_mask ((__v8df) __A,
                (__v8df) __W,
                (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_rcp14_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_rcp14pd512_mask ((__v8df) __A,
                (__v8df)
                _mm512_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_rcp14_ps(__m512 __A)
{
  return (__m512) __builtin_ia32_rcp14ps512_mask ((__v16sf) __A,
              (__v16sf)
              _mm512_setzero_ps (),
              (__mmask16) -1);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_rcp14_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_rcp14ps512_mask ((__v16sf) __A,
                   (__v16sf) __W,
                   (__mmask16) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_rcp14_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_rcp14ps512_mask ((__v16sf) __A,
                   (__v16sf)
                   _mm512_setzero_ps (),
                   (__mmask16) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_rcp14_ss(__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_rcp14ss_mask ((__v4sf) __A,
                 (__v4sf) __B,
                 (__v4sf)
                 _mm_setzero_ps (),
                 (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_rcp14_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_rcp14ss_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) __W,
          (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_rcp14_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_rcp14ss_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) _mm_setzero_ps (),
          (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_rcp14_sd(__m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_rcp14sd_mask ((__v2df) __A,
            (__v2df) __B,
            (__v2df)
            _mm_setzero_pd (),
            (__mmask8) -1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_rcp14_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_rcp14sd_mask ( (__v2df) __A,
          (__v2df) __B,
          (__v2df) __W,
          (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_rcp14_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_rcp14sd_mask ( (__v2df) __A,
          (__v2df) __B,
          (__v2df) _mm_setzero_pd (),
          (__mmask8) __U);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_floor_ps(__m512 __A)
{
  return (__m512) __builtin_ia32_rndscaleps_mask ((__v16sf) __A,
                                                  (0x00 | 0x01),
                                                  (__v16sf) __A, (unsigned short)-1,
                                                  0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_floor_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_rndscaleps_mask ((__v16sf) __A,
                   (0x00 | 0x01),
                   (__v16sf) __W, __U,
                   0x04);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_floor_pd(__m512d __A)
{
  return (__m512d) __builtin_ia32_rndscalepd_mask ((__v8df) __A,
                                                   (0x00 | 0x01),
                                                   (__v8df) __A, (unsigned char)-1,
                                                   0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_floor_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_rndscalepd_mask ((__v8df) __A,
                (0x00 | 0x01),
                (__v8df) __W, __U,
                0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_ceil_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_rndscaleps_mask ((__v16sf) __A,
                   (0x00 | 0x02),
                   (__v16sf) __W, __U,
                   0x04);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_ceil_ps(__m512 __A)
{
  return (__m512) __builtin_ia32_rndscaleps_mask ((__v16sf) __A,
                                                  (0x00 | 0x02),
                                                  (__v16sf) __A, (unsigned short)-1,
                                                  0x04);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_ceil_pd(__m512d __A)
{
  return (__m512d) __builtin_ia32_rndscalepd_mask ((__v8df) __A,
                                                   (0x00 | 0x02),
                                                   (__v8df) __A, (unsigned char)-1,
                                                   0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_ceil_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_rndscalepd_mask ((__v8df) __A,
                (0x00 | 0x02),
                (__v8df) __W, __U,
                0x04);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_abs_epi64(__m512i __A)
{
  return (__m512i)__builtin_elementwise_abs((__v8di)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_abs_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_abs_epi64(__A),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_abs_epi64 (__mmask8 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_abs_epi64(__A),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_abs_epi32(__m512i __A)
{
  return (__m512i)__builtin_elementwise_abs((__v16si) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_abs_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                             (__v16si)_mm512_abs_epi32(__A),
                                             (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_abs_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                             (__v16si)_mm512_abs_epi32(__A),
                                             (__v16si)_mm512_setzero_si512());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_add_ss(__m128 __W, __mmask8 __U,__m128 __A, __m128 __B) {
  __A = _mm_add_ss(__A, __B);
  return __builtin_ia32_selectss_128(__U, __A, __W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_add_ss(__mmask8 __U,__m128 __A, __m128 __B) {
  __A = _mm_add_ss(__A, __B);
  return __builtin_ia32_selectss_128(__U, __A, _mm_setzero_ps());
}
# 1944 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_add_sd(__m128d __W, __mmask8 __U,__m128d __A, __m128d __B) {
  __A = _mm_add_sd(__A, __B);
  return __builtin_ia32_selectsd_128(__U, __A, __W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_add_sd(__mmask8 __U,__m128d __A, __m128d __B) {
  __A = _mm_add_sd(__A, __B);
  return __builtin_ia32_selectsd_128(__U, __A, _mm_setzero_pd());
}
# 1973 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_add_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_add_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_add_pd(__mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_add_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_add_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_add_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_add_ps(__mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_add_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}
# 2029 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_sub_ss(__m128 __W, __mmask8 __U,__m128 __A, __m128 __B) {
  __A = _mm_sub_ss(__A, __B);
  return __builtin_ia32_selectss_128(__U, __A, __W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_sub_ss(__mmask8 __U,__m128 __A, __m128 __B) {
  __A = _mm_sub_ss(__A, __B);
  return __builtin_ia32_selectss_128(__U, __A, _mm_setzero_ps());
}
# 2058 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_sub_sd(__m128d __W, __mmask8 __U,__m128d __A, __m128d __B) {
  __A = _mm_sub_sd(__A, __B);
  return __builtin_ia32_selectsd_128(__U, __A, __W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_sub_sd(__mmask8 __U,__m128d __A, __m128d __B) {
  __A = _mm_sub_sd(__A, __B);
  return __builtin_ia32_selectsd_128(__U, __A, _mm_setzero_pd());
}
# 2088 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_sub_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_sub_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_sub_pd(__mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_sub_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_sub_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_sub_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_sub_ps(__mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_sub_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}
# 2144 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_mul_ss(__m128 __W, __mmask8 __U,__m128 __A, __m128 __B) {
  __A = _mm_mul_ss(__A, __B);
  return __builtin_ia32_selectss_128(__U, __A, __W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_mul_ss(__mmask8 __U,__m128 __A, __m128 __B) {
  __A = _mm_mul_ss(__A, __B);
  return __builtin_ia32_selectss_128(__U, __A, _mm_setzero_ps());
}
# 2173 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_mul_sd(__m128d __W, __mmask8 __U,__m128d __A, __m128d __B) {
  __A = _mm_mul_sd(__A, __B);
  return __builtin_ia32_selectsd_128(__U, __A, __W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_mul_sd(__mmask8 __U,__m128d __A, __m128d __B) {
  __A = _mm_mul_sd(__A, __B);
  return __builtin_ia32_selectsd_128(__U, __A, _mm_setzero_pd());
}
# 2203 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_mul_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_mul_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_mul_pd(__mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_mul_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_mul_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_mul_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_mul_ps(__mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_mul_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}
# 2259 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_div_ss(__m128 __W, __mmask8 __U,__m128 __A, __m128 __B) {
  __A = _mm_div_ss(__A, __B);
  return __builtin_ia32_selectss_128(__U, __A, __W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_div_ss(__mmask8 __U,__m128 __A, __m128 __B) {
  __A = _mm_div_ss(__A, __B);
  return __builtin_ia32_selectss_128(__U, __A, _mm_setzero_ps());
}
# 2289 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_div_sd(__m128d __W, __mmask8 __U,__m128d __A, __m128d __B) {
  __A = _mm_div_sd(__A, __B);
  return __builtin_ia32_selectsd_128(__U, __A, __W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_div_sd(__mmask8 __U,__m128d __A, __m128d __B) {
  __A = _mm_div_sd(__A, __B);
  return __builtin_ia32_selectsd_128(__U, __A, _mm_setzero_pd());
}
# 2319 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_div_pd(__m512d __a, __m512d __b)
{
  return (__m512d)((__v8df)__a/(__v8df)__b);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_div_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_div_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_div_pd(__mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_div_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_div_ps(__m512 __a, __m512 __b)
{
  return (__m512)((__v16sf)__a/(__v16sf)__b);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_div_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_div_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_div_ps(__mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_div_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}
# 2535 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_fmadd_pd(__m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask ((__v8df) __A,
                                                    (__v8df) __B,
                                                    (__v8df) __C,
                                                    (__mmask8) -1,
                                                    0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_fmadd_pd(__m512d __A, __mmask8 __U, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask ((__v8df) __A,
                                                    (__v8df) __B,
                                                    (__v8df) __C,
                                                    (__mmask8) __U,
                                                    0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask3_fmadd_pd(__m512d __A, __m512d __B, __m512d __C, __mmask8 __U)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask3 ((__v8df) __A,
                                                     (__v8df) __B,
                                                     (__v8df) __C,
                                                     (__mmask8) __U,
                                                     0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_fmadd_pd(__mmask8 __U, __m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_maskz ((__v8df) __A,
                                                     (__v8df) __B,
                                                     (__v8df) __C,
                                                     (__mmask8) __U,
                                                     0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_fmsub_pd(__m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask ((__v8df) __A,
                                                    (__v8df) __B,
                                                    -(__v8df) __C,
                                                    (__mmask8) -1,
                                                    0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_fmsub_pd(__m512d __A, __mmask8 __U, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask ((__v8df) __A,
                                                    (__v8df) __B,
                                                    -(__v8df) __C,
                                                    (__mmask8) __U,
                                                    0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_fmsub_pd(__mmask8 __U, __m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_maskz ((__v8df) __A,
                                                     (__v8df) __B,
                                                     -(__v8df) __C,
                                                     (__mmask8) __U,
                                                     0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_fnmadd_pd(__m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask ((__v8df) __A,
                                                    -(__v8df) __B,
                                                    (__v8df) __C,
                                                    (__mmask8) -1,
                                                    0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask3_fnmadd_pd(__m512d __A, __m512d __B, __m512d __C, __mmask8 __U)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask3 (-(__v8df) __A,
                                                     (__v8df) __B,
                                                     (__v8df) __C,
                                                     (__mmask8) __U,
                                                     0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_fnmadd_pd(__mmask8 __U, __m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_maskz (-(__v8df) __A,
                                                     (__v8df) __B,
                                                     (__v8df) __C,
                                                     (__mmask8) __U,
                                                     0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_fnmsub_pd(__m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask ((__v8df) __A,
                                                    -(__v8df) __B,
                                                    -(__v8df) __C,
                                                    (__mmask8) -1,
                                                    0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_fnmsub_pd(__mmask8 __U, __m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_maskz (-(__v8df) __A,
                                                     (__v8df) __B,
                                                     -(__v8df) __C,
                                                     (__mmask8) __U,
                                                     0x04);
}
# 2739 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_fmadd_ps(__m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask ((__v16sf) __A,
                                                   (__v16sf) __B,
                                                   (__v16sf) __C,
                                                   (__mmask16) -1,
                                                   0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_fmadd_ps(__m512 __A, __mmask16 __U, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask ((__v16sf) __A,
                                                   (__v16sf) __B,
                                                   (__v16sf) __C,
                                                   (__mmask16) __U,
                                                   0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask3_fmadd_ps(__m512 __A, __m512 __B, __m512 __C, __mmask16 __U)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask3 ((__v16sf) __A,
                                                    (__v16sf) __B,
                                                    (__v16sf) __C,
                                                    (__mmask16) __U,
                                                    0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_fmadd_ps(__mmask16 __U, __m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_maskz ((__v16sf) __A,
                                                    (__v16sf) __B,
                                                    (__v16sf) __C,
                                                    (__mmask16) __U,
                                                    0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_fmsub_ps(__m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask ((__v16sf) __A,
                                                   (__v16sf) __B,
                                                   -(__v16sf) __C,
                                                   (__mmask16) -1,
                                                   0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_fmsub_ps(__m512 __A, __mmask16 __U, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask ((__v16sf) __A,
                                                   (__v16sf) __B,
                                                   -(__v16sf) __C,
                                                   (__mmask16) __U,
                                                   0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_fmsub_ps(__mmask16 __U, __m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_maskz ((__v16sf) __A,
                                                    (__v16sf) __B,
                                                    -(__v16sf) __C,
                                                    (__mmask16) __U,
                                                    0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_fnmadd_ps(__m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask ((__v16sf) __A,
                                                   -(__v16sf) __B,
                                                   (__v16sf) __C,
                                                   (__mmask16) -1,
                                                   0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask3_fnmadd_ps(__m512 __A, __m512 __B, __m512 __C, __mmask16 __U)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask3 (-(__v16sf) __A,
                                                    (__v16sf) __B,
                                                    (__v16sf) __C,
                                                    (__mmask16) __U,
                                                    0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_fnmadd_ps(__mmask16 __U, __m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_maskz (-(__v16sf) __A,
                                                    (__v16sf) __B,
                                                    (__v16sf) __C,
                                                    (__mmask16) __U,
                                                    0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_fnmsub_ps(__m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask ((__v16sf) __A,
                                                   -(__v16sf) __B,
                                                   -(__v16sf) __C,
                                                   (__mmask16) -1,
                                                   0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_fnmsub_ps(__mmask16 __U, __m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_maskz (-(__v16sf) __A,
                                                    (__v16sf) __B,
                                                    -(__v16sf) __C,
                                                    (__mmask16) __U,
                                                    0x04);
}
# 2908 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_fmaddsub_pd(__m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask ((__v8df) __A,
                                                      (__v8df) __B,
                                                      (__v8df) __C,
                                                      (__mmask8) -1,
                                                      0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_fmaddsub_pd(__m512d __A, __mmask8 __U, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask ((__v8df) __A,
                                                      (__v8df) __B,
                                                      (__v8df) __C,
                                                      (__mmask8) __U,
                                                      0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask3_fmaddsub_pd(__m512d __A, __m512d __B, __m512d __C, __mmask8 __U)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask3 ((__v8df) __A,
                                                       (__v8df) __B,
                                                       (__v8df) __C,
                                                       (__mmask8) __U,
                                                       0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_fmaddsub_pd(__mmask8 __U, __m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_maskz ((__v8df) __A,
                                                       (__v8df) __B,
                                                       (__v8df) __C,
                                                       (__mmask8) __U,
                                                       0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_fmsubadd_pd(__m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask ((__v8df) __A,
                                                       (__v8df) __B,
                                                       -(__v8df) __C,
                                                       (__mmask8) -1,
                                                       0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_fmsubadd_pd(__m512d __A, __mmask8 __U, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_mask ((__v8df) __A,
                                                       (__v8df) __B,
                                                       -(__v8df) __C,
                                                       (__mmask8) __U,
                                                       0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_fmsubadd_pd(__mmask8 __U, __m512d __A, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddsubpd512_maskz ((__v8df) __A,
                                                        (__v8df) __B,
                                                        -(__v8df) __C,
                                                        (__mmask8) __U,
                                                        0x04);
}
# 3027 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_fmaddsub_ps(__m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask ((__v16sf) __A,
                                                      (__v16sf) __B,
                                                      (__v16sf) __C,
                                                      (__mmask16) -1,
                                                      0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_fmaddsub_ps(__m512 __A, __mmask16 __U, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask ((__v16sf) __A,
                                                      (__v16sf) __B,
                                                      (__v16sf) __C,
                                                      (__mmask16) __U,
                                                      0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask3_fmaddsub_ps(__m512 __A, __m512 __B, __m512 __C, __mmask16 __U)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask3 ((__v16sf) __A,
                                                       (__v16sf) __B,
                                                       (__v16sf) __C,
                                                       (__mmask16) __U,
                                                       0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_fmaddsub_ps(__mmask16 __U, __m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_maskz ((__v16sf) __A,
                                                       (__v16sf) __B,
                                                       (__v16sf) __C,
                                                       (__mmask16) __U,
                                                       0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_fmsubadd_ps(__m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask ((__v16sf) __A,
                                                      (__v16sf) __B,
                                                      -(__v16sf) __C,
                                                      (__mmask16) -1,
                                                      0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_fmsubadd_ps(__m512 __A, __mmask16 __U, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_mask ((__v16sf) __A,
                                                      (__v16sf) __B,
                                                      -(__v16sf) __C,
                                                      (__mmask16) __U,
                                                      0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_fmsubadd_ps(__mmask16 __U, __m512 __A, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddsubps512_maskz ((__v16sf) __A,
                                                       (__v16sf) __B,
                                                       -(__v16sf) __C,
                                                       (__mmask16) __U,
                                                       0x04);
}
# 3104 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask3_fmsub_pd(__m512d __A, __m512d __B, __m512d __C, __mmask8 __U)
{
  return (__m512d)__builtin_ia32_vfmsubpd512_mask3 ((__v8df) __A,
                                                    (__v8df) __B,
                                                    (__v8df) __C,
                                                    (__mmask8) __U,
                                                    0x04);
}







static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask3_fmsub_ps(__m512 __A, __m512 __B, __m512 __C, __mmask16 __U)
{
  return (__m512)__builtin_ia32_vfmsubps512_mask3 ((__v16sf) __A,
                                                   (__v16sf) __B,
                                                   (__v16sf) __C,
                                                   (__mmask16) __U,
                                                   0x04);
}
# 3137 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask3_fmsubadd_pd(__m512d __A, __m512d __B, __m512d __C, __mmask8 __U)
{
  return (__m512d)__builtin_ia32_vfmsubaddpd512_mask3 ((__v8df) __A,
                                                       (__v8df) __B,
                                                       (__v8df) __C,
                                                       (__mmask8) __U,
                                                       0x04);
}
# 3154 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask3_fmsubadd_ps(__m512 __A, __m512 __B, __m512 __C, __mmask16 __U)
{
  return (__m512)__builtin_ia32_vfmsubaddps512_mask3 ((__v16sf) __A,
                                                      (__v16sf) __B,
                                                      (__v16sf) __C,
                                                      (__mmask16) __U,
                                                      0x04);
}
# 3171 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_fnmadd_pd(__m512d __A, __mmask8 __U, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask ((__v8df) __A,
                                                    -(__v8df) __B,
                                                    (__v8df) __C,
                                                    (__mmask8) __U,
                                                    0x04);
}
# 3188 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_fnmadd_ps(__m512 __A, __mmask16 __U, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask ((__v16sf) __A,
                                                   -(__v16sf) __B,
                                                   (__v16sf) __C,
                                                   (__mmask16) __U,
                                                   0x04);
}
# 3212 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_fnmsub_pd(__m512d __A, __mmask8 __U, __m512d __B, __m512d __C)
{
  return (__m512d) __builtin_ia32_vfmaddpd512_mask ((__v8df) __A,
                                                    -(__v8df) __B,
                                                    -(__v8df) __C,
                                                    (__mmask8) __U,
                                                    0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask3_fnmsub_pd(__m512d __A, __m512d __B, __m512d __C, __mmask8 __U)
{
  return (__m512d) __builtin_ia32_vfmsubpd512_mask3 (-(__v8df) __A,
                                                     (__v8df) __B,
                                                     (__v8df) __C,
                                                     (__mmask8) __U,
                                                     0x04);
}
# 3246 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_fnmsub_ps(__m512 __A, __mmask16 __U, __m512 __B, __m512 __C)
{
  return (__m512) __builtin_ia32_vfmaddps512_mask ((__v16sf) __A,
                                                   -(__v16sf) __B,
                                                   -(__v16sf) __C,
                                                   (__mmask16) __U,
                                                   0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask3_fnmsub_ps(__m512 __A, __m512 __B, __m512 __C, __mmask16 __U)
{
  return (__m512) __builtin_ia32_vfmsubps512_mask3 (-(__v16sf) __A,
                                                    (__v16sf) __B,
                                                    (__v16sf) __C,
                                                    (__mmask16) __U,
                                                    0x04);
}





static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_permutex2var_epi32(__m512i __A, __m512i __I, __m512i __B)
{
  return (__m512i)__builtin_ia32_vpermi2vard512((__v16si)__A, (__v16si) __I,
                                                (__v16si) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_permutex2var_epi32(__m512i __A, __mmask16 __U, __m512i __I,
                               __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                              (__v16si)_mm512_permutex2var_epi32(__A, __I, __B),
                              (__v16si)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask2_permutex2var_epi32(__m512i __A, __m512i __I, __mmask16 __U,
                                __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                              (__v16si)_mm512_permutex2var_epi32(__A, __I, __B),
                              (__v16si)__I);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_permutex2var_epi32(__mmask16 __U, __m512i __A, __m512i __I,
                                __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                              (__v16si)_mm512_permutex2var_epi32(__A, __I, __B),
                              (__v16si)_mm512_setzero_si512());
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_permutex2var_epi64(__m512i __A, __m512i __I, __m512i __B)
{
  return (__m512i)__builtin_ia32_vpermi2varq512((__v8di)__A, (__v8di) __I,
                                                (__v8di) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_permutex2var_epi64(__m512i __A, __mmask8 __U, __m512i __I,
                               __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512(__U,
                               (__v8di)_mm512_permutex2var_epi64(__A, __I, __B),
                               (__v8di)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask2_permutex2var_epi64(__m512i __A, __m512i __I, __mmask8 __U,
                                __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512(__U,
                               (__v8di)_mm512_permutex2var_epi64(__A, __I, __B),
                               (__v8di)__I);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_permutex2var_epi64(__mmask8 __U, __m512i __A, __m512i __I,
                                __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512(__U,
                               (__v8di)_mm512_permutex2var_epi64(__A, __I, __B),
                               (__v8di)_mm512_setzero_si512());
}
# 3399 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_blend_pd(__mmask8 __U, __m512d __A, __m512d __W)
{
  return (__m512d) __builtin_ia32_selectpd_512 ((__mmask8) __U,
                 (__v8df) __W,
                 (__v8df) __A);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_blend_ps(__mmask16 __U, __m512 __A, __m512 __W)
{
  return (__m512) __builtin_ia32_selectps_512 ((__mmask16) __U,
                (__v16sf) __W,
                (__v16sf) __A);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_blend_epi64(__mmask8 __U, __m512i __A, __m512i __W)
{
  return (__m512i) __builtin_ia32_selectq_512 ((__mmask8) __U,
                (__v8di) __W,
                (__v8di) __A);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_blend_epi32(__mmask16 __U, __m512i __A, __m512i __W)
{
  return (__m512i) __builtin_ia32_selectd_512 ((__mmask16) __U,
                (__v16si) __W,
                (__v16si) __A);
}
# 3561 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvttps_epu32(__m512 __A)
{
  return (__m512i) __builtin_ia32_cvttps2udq512_mask ((__v16sf) __A,
                  (__v16si)
                  _mm512_setzero_si512 (),
                  (__mmask16) -1,
                  0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvttps_epu32 (__m512i __W, __mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvttps2udq512_mask ((__v16sf) __A,
                   (__v16si) __W,
                   (__mmask16) __U,
                   0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvttps_epu32 (__mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvttps2udq512_mask ((__v16sf) __A,
                   (__v16si) _mm512_setzero_si512 (),
                   (__mmask16) __U,
                   0x04);
}
# 3619 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtepu32_ps (__m512i __A)
{
  return (__m512)__builtin_convertvector((__v16su)__A, __v16sf);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepu32_ps (__m512 __W, __mmask16 __U, __m512i __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_cvtepu32_ps(__A),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtepu32_ps (__mmask16 __U, __m512i __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_cvtepu32_ps(__A),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtepi32_pd(__m256i __A)
{
  return (__m512d)__builtin_convertvector((__v8si)__A, __v8df);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepi32_pd (__m512d __W, __mmask8 __U, __m256i __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8) __U,
                                              (__v8df)_mm512_cvtepi32_pd(__A),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtepi32_pd (__mmask8 __U, __m256i __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8) __U,
                                              (__v8df)_mm512_cvtepi32_pd(__A),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtepi32lo_pd(__m512i __A)
{
  return (__m512d) _mm512_cvtepi32_pd(_mm512_castsi512_si256(__A));
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepi32lo_pd(__m512d __W, __mmask8 __U,__m512i __A)
{
  return (__m512d) _mm512_mask_cvtepi32_pd(__W, __U, _mm512_castsi512_si256(__A));
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtepi32_ps (__m512i __A)
{
  return (__m512)__builtin_convertvector((__v16si)__A, __v16sf);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepi32_ps (__m512 __W, __mmask16 __U, __m512i __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_cvtepi32_ps(__A),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtepi32_ps (__mmask16 __U, __m512i __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_cvtepi32_ps(__A),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtepu32_pd(__m256i __A)
{
  return (__m512d)__builtin_convertvector((__v8su)__A, __v8df);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepu32_pd (__m512d __W, __mmask8 __U, __m256i __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8) __U,
                                              (__v8df)_mm512_cvtepu32_pd(__A),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtepu32_pd (__mmask8 __U, __m256i __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8) __U,
                                              (__v8df)_mm512_cvtepu32_pd(__A),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtepu32lo_pd(__m512i __A)
{
  return (__m512d) _mm512_cvtepu32_pd(_mm512_castsi512_si256(__A));
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepu32lo_pd(__m512d __W, __mmask8 __U,__m512i __A)
{
  return (__m512d) _mm512_mask_cvtepu32_pd(__W, __U, _mm512_castsi512_si256(__A));
}
# 3746 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtpd_ps (__m512d __A)
{
  return (__m256) __builtin_ia32_cvtpd2ps512_mask ((__v8df) __A,
                (__v8sf) _mm256_undefined_ps (),
                (__mmask8) -1,
                0x04);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtpd_ps (__m256 __W, __mmask8 __U, __m512d __A)
{
  return (__m256) __builtin_ia32_cvtpd2ps512_mask ((__v8df) __A,
                (__v8sf) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtpd_ps (__mmask8 __U, __m512d __A)
{
  return (__m256) __builtin_ia32_cvtpd2ps512_mask ((__v8df) __A,
                (__v8sf) _mm256_setzero_ps (),
                (__mmask8) __U,
                0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtpd_pslo (__m512d __A)
{
  return (__m512) __builtin_shufflevector((__v8sf) _mm512_cvtpd_ps(__A),
                (__v8sf) _mm256_setzero_ps (),
                0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtpd_pslo (__m512 __W, __mmask8 __U,__m512d __A)
{
  return (__m512) __builtin_shufflevector (
                (__v8sf) _mm512_mask_cvtpd_ps (_mm512_castps512_ps256(__W),
                                               __U, __A),
                (__v8sf) _mm256_setzero_ps (),
                0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);
}
# 3826 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtph_ps(__m256i __A)
{
  return (__m512) __builtin_ia32_vcvtph2ps512_mask ((__v16hi) __A,
                (__v16sf)
                _mm512_setzero_ps (),
                (__mmask16) -1,
                0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtph_ps (__m512 __W, __mmask16 __U, __m256i __A)
{
  return (__m512) __builtin_ia32_vcvtph2ps512_mask ((__v16hi) __A,
                 (__v16sf) __W,
                 (__mmask16) __U,
                 0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtph_ps (__mmask16 __U, __m256i __A)
{
  return (__m512) __builtin_ia32_vcvtph2ps512_mask ((__v16hi) __A,
                 (__v16sf) _mm512_setzero_ps (),
                 (__mmask16) __U,
                 0x04);
}
# 3869 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvttpd_epi32(__m512d __a)
{
  return (__m256i)__builtin_ia32_cvttpd2dq512_mask((__v8df) __a,
                                                   (__v8si)_mm256_setzero_si256(),
                                                   (__mmask8) -1,
                                                    0x04);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvttpd_epi32 (__m256i __W, __mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2dq512_mask ((__v8df) __A,
                  (__v8si) __W,
                  (__mmask8) __U,
                  0x04);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvttpd_epi32 (__mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2dq512_mask ((__v8df) __A,
                  (__v8si) _mm256_setzero_si256 (),
                  (__mmask8) __U,
                  0x04);
}
# 3911 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvttps_epi32(__m512 __a)
{
  return (__m512i)
    __builtin_ia32_cvttps2dq512_mask((__v16sf) __a,
                                     (__v16si) _mm512_setzero_si512 (),
                                     (__mmask16) -1, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvttps_epi32 (__m512i __W, __mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvttps2dq512_mask ((__v16sf) __A,
                  (__v16si) __W,
                  (__mmask16) __U,
                  0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvttps_epi32 (__mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvttps2dq512_mask ((__v16sf) __A,
                  (__v16si) _mm512_setzero_si512 (),
                  (__mmask16) __U,
                  0x04);
}
# 3953 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtps_epi32 (__m512 __A)
{
  return (__m512i) __builtin_ia32_cvtps2dq512_mask ((__v16sf) __A,
                 (__v16si) _mm512_undefined_epi32 (),
                 (__mmask16) -1,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtps_epi32 (__m512i __W, __mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvtps2dq512_mask ((__v16sf) __A,
                 (__v16si) __W,
                 (__mmask16) __U,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtps_epi32 (__mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvtps2dq512_mask ((__v16sf) __A,
                 (__v16si)
                 _mm512_setzero_si512 (),
                 (__mmask16) __U,
                 0x04);
}
# 3996 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtpd_epi32 (__m512d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2dq512_mask ((__v8df) __A,
                 (__v8si)
                 _mm256_undefined_si256 (),
                 (__mmask8) -1,
                 0x04);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtpd_epi32 (__m256i __W, __mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2dq512_mask ((__v8df) __A,
                 (__v8si) __W,
                 (__mmask8) __U,
                 0x04);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtpd_epi32 (__mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2dq512_mask ((__v8df) __A,
                 (__v8si)
                 _mm256_setzero_si256 (),
                 (__mmask8) __U,
                 0x04);
}
# 4040 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtps_epu32 ( __m512 __A)
{
  return (__m512i) __builtin_ia32_cvtps2udq512_mask ((__v16sf) __A, (__v16si) _mm512_undefined_epi32 (),


                  (__mmask16) -1, 0x04);

}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtps_epu32 (__m512i __W, __mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvtps2udq512_mask ((__v16sf) __A,
                  (__v16si) __W,
                  (__mmask16) __U,
                  0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtps_epu32 ( __mmask16 __U, __m512 __A)
{
  return (__m512i) __builtin_ia32_cvtps2udq512_mask ((__v16sf) __A,
                  (__v16si)
                  _mm512_setzero_si512 (),
                  (__mmask16) __U ,
                  0x04);
}
# 4084 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtpd_epu32 (__m512d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2udq512_mask ((__v8df) __A,
                  (__v8si)
                  _mm256_undefined_si256 (),
                  (__mmask8) -1,
                  0x04);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtpd_epu32 (__m256i __W, __mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2udq512_mask ((__v8df) __A,
                  (__v8si) __W,
                  (__mmask8) __U,
                  0x04);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtpd_epu32 (__mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvtpd2udq512_mask ((__v8df) __A,
                  (__v8si)
                  _mm256_setzero_si256 (),
                  (__mmask8) __U,
                  0x04);
}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtsd_f64(__m512d __a)
{
  return __a[0];
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtss_f32(__m512 __a)
{
  return __a[0];
}



static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_unpackhi_pd(__m512d __a, __m512d __b)
{
  return (__m512d)__builtin_shufflevector((__v8df)__a, (__v8df)__b,
                                          1, 9, 1+2, 9+2, 1+4, 9+4, 1+6, 9+6);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_unpackhi_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8) __U,
                                           (__v8df)_mm512_unpackhi_pd(__A, __B),
                                           (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_unpackhi_pd(__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8) __U,
                                           (__v8df)_mm512_unpackhi_pd(__A, __B),
                                           (__v8df)_mm512_setzero_pd());
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_unpacklo_pd(__m512d __a, __m512d __b)
{
  return (__m512d)__builtin_shufflevector((__v8df)__a, (__v8df)__b,
                                          0, 8, 0+2, 8+2, 0+4, 8+4, 0+6, 8+6);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_unpacklo_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8) __U,
                                           (__v8df)_mm512_unpacklo_pd(__A, __B),
                                           (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_unpacklo_pd (__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8) __U,
                                           (__v8df)_mm512_unpacklo_pd(__A, __B),
                                           (__v8df)_mm512_setzero_pd());
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_unpackhi_ps(__m512 __a, __m512 __b)
{
  return (__m512)__builtin_shufflevector((__v16sf)__a, (__v16sf)__b,
                                         2, 18, 3, 19,
                                         2+4, 18+4, 3+4, 19+4,
                                         2+8, 18+8, 3+8, 19+8,
                                         2+12, 18+12, 3+12, 19+12);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_unpackhi_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16) __U,
                                          (__v16sf)_mm512_unpackhi_ps(__A, __B),
                                          (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_unpackhi_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16) __U,
                                          (__v16sf)_mm512_unpackhi_ps(__A, __B),
                                          (__v16sf)_mm512_setzero_ps());
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_unpacklo_ps(__m512 __a, __m512 __b)
{
  return (__m512)__builtin_shufflevector((__v16sf)__a, (__v16sf)__b,
                                         0, 16, 1, 17,
                                         0+4, 16+4, 1+4, 17+4,
                                         0+8, 16+8, 1+8, 17+8,
                                         0+12, 16+12, 1+12, 17+12);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_unpacklo_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16) __U,
                                          (__v16sf)_mm512_unpacklo_ps(__A, __B),
                                          (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_unpacklo_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16) __U,
                                          (__v16sf)_mm512_unpacklo_ps(__A, __B),
                                          (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_unpackhi_epi32(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_shufflevector((__v16si)__A, (__v16si)__B,
                                          2, 18, 3, 19,
                                          2+4, 18+4, 3+4, 19+4,
                                          2+8, 18+8, 3+8, 19+8,
                                          2+12, 18+12, 3+12, 19+12);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_unpackhi_epi32(__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16) __U,
                                       (__v16si)_mm512_unpackhi_epi32(__A, __B),
                                       (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_unpackhi_epi32(__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16) __U,
                                       (__v16si)_mm512_unpackhi_epi32(__A, __B),
                                       (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_unpacklo_epi32(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_shufflevector((__v16si)__A, (__v16si)__B,
                                          0, 16, 1, 17,
                                          0+4, 16+4, 1+4, 17+4,
                                          0+8, 16+8, 1+8, 17+8,
                                          0+12, 16+12, 1+12, 17+12);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_unpacklo_epi32(__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16) __U,
                                       (__v16si)_mm512_unpacklo_epi32(__A, __B),
                                       (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_unpacklo_epi32(__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16) __U,
                                       (__v16si)_mm512_unpacklo_epi32(__A, __B),
                                       (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_unpackhi_epi64(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_shufflevector((__v8di)__A, (__v8di)__B,
                                          1, 9, 1+2, 9+2, 1+4, 9+4, 1+6, 9+6);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_unpackhi_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8) __U,
                                        (__v8di)_mm512_unpackhi_epi64(__A, __B),
                                        (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_unpackhi_epi64(__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8) __U,
                                        (__v8di)_mm512_unpackhi_epi64(__A, __B),
                                        (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_unpacklo_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_shufflevector((__v8di)__A, (__v8di)__B,
                                          0, 8, 0+2, 8+2, 0+4, 8+4, 0+6, 8+6);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_unpacklo_epi64 (__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8) __U,
                                        (__v8di)_mm512_unpacklo_epi64(__A, __B),
                                        (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_unpacklo_epi64 (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8) __U,
                                        (__v8di)_mm512_unpacklo_epi64(__A, __B),
                                        (__v8di)_mm512_setzero_si512());
}




static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_loadu_si512 (void const *__P)
{
  struct __loadu_si512 {
    __m512i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_si512*)__P)->__v;
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_loadu_epi32 (void const *__P)
{
  struct __loadu_epi32 {
    __m512i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_epi32*)__P)->__v;
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_loadu_epi32 (__m512i __W, __mmask16 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_loaddqusi512_mask ((const int *) __P,
                  (__v16si) __W,
                  (__mmask16) __U);
}


static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_loadu_epi32(__mmask16 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_loaddqusi512_mask ((const int *)__P,
                                                     (__v16si)
                                                     _mm512_setzero_si512 (),
                                                     (__mmask16) __U);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_loadu_epi64 (void const *__P)
{
  struct __loadu_epi64 {
    __m512i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_epi64*)__P)->__v;
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_loadu_epi64 (__m512i __W, __mmask8 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_loaddqudi512_mask ((const long long *) __P,
                  (__v8di) __W,
                  (__mmask8) __U);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_loadu_epi64(__mmask8 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_loaddqudi512_mask ((const long long *)__P,
                                                     (__v8di)
                                                     _mm512_setzero_si512 (),
                                                     (__mmask8) __U);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_loadu_ps (__m512 __W, __mmask16 __U, void const *__P)
{
  return (__m512) __builtin_ia32_loadups512_mask ((const float *) __P,
                   (__v16sf) __W,
                   (__mmask16) __U);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_loadu_ps(__mmask16 __U, void const *__P)
{
  return (__m512) __builtin_ia32_loadups512_mask ((const float *)__P,
                                                  (__v16sf)
                                                  _mm512_setzero_ps (),
                                                  (__mmask16) __U);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_loadu_pd (__m512d __W, __mmask8 __U, void const *__P)
{
  return (__m512d) __builtin_ia32_loadupd512_mask ((const double *) __P,
                (__v8df) __W,
                (__mmask8) __U);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_loadu_pd(__mmask8 __U, void const *__P)
{
  return (__m512d) __builtin_ia32_loadupd512_mask ((const double *)__P,
                                                   (__v8df)
                                                   _mm512_setzero_pd (),
                                                   (__mmask8) __U);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_loadu_pd(void const *__p)
{
  struct __loadu_pd {
    __m512d_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_pd*)__p)->__v;
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_loadu_ps(void const *__p)
{
  struct __loadu_ps {
    __m512_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_ps*)__p)->__v;
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_load_ps(void const *__p)
{
  return *(const __m512*)__p;
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_load_ps (__m512 __W, __mmask16 __U, void const *__P)
{
  return (__m512) __builtin_ia32_loadaps512_mask ((const __v16sf *) __P,
                   (__v16sf) __W,
                   (__mmask16) __U);
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_load_ps(__mmask16 __U, void const *__P)
{
  return (__m512) __builtin_ia32_loadaps512_mask ((const __v16sf *)__P,
                                                  (__v16sf)
                                                  _mm512_setzero_ps (),
                                                  (__mmask16) __U);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_load_pd(void const *__p)
{
  return *(const __m512d*)__p;
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_load_pd (__m512d __W, __mmask8 __U, void const *__P)
{
  return (__m512d) __builtin_ia32_loadapd512_mask ((const __v8df *) __P,
                          (__v8df) __W,
                          (__mmask8) __U);
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_load_pd(__mmask8 __U, void const *__P)
{
  return (__m512d) __builtin_ia32_loadapd512_mask ((const __v8df *)__P,
                                                   (__v8df)
                                                   _mm512_setzero_pd (),
                                                   (__mmask8) __U);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_load_si512 (void const *__P)
{
  return *(const __m512i *) __P;
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_load_epi32 (void const *__P)
{
  return *(const __m512i *) __P;
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_load_epi64 (void const *__P)
{
  return *(const __m512i *) __P;
}



static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_storeu_epi64 (void *__P, __m512i __A)
{
  struct __storeu_epi64 {
    __m512i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_epi64*)__P)->__v = __A;
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_storeu_epi64(void *__P, __mmask8 __U, __m512i __A)
{
  __builtin_ia32_storedqudi512_mask ((long long *)__P, (__v8di) __A,
                                     (__mmask8) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_storeu_si512 (void *__P, __m512i __A)
{
  struct __storeu_si512 {
    __m512i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_si512*)__P)->__v = __A;
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_storeu_epi32 (void *__P, __m512i __A)
{
  struct __storeu_epi32 {
    __m512i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_epi32*)__P)->__v = __A;
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_storeu_epi32(void *__P, __mmask16 __U, __m512i __A)
{
  __builtin_ia32_storedqusi512_mask ((int *)__P, (__v16si) __A,
                                     (__mmask16) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_storeu_pd(void *__P, __mmask8 __U, __m512d __A)
{
  __builtin_ia32_storeupd512_mask ((double *)__P, (__v8df) __A, (__mmask8) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_storeu_pd(void *__P, __m512d __A)
{
  struct __storeu_pd {
    __m512d_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_pd*)__P)->__v = __A;
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_storeu_ps(void *__P, __mmask16 __U, __m512 __A)
{
  __builtin_ia32_storeups512_mask ((float *)__P, (__v16sf) __A,
                                   (__mmask16) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_storeu_ps(void *__P, __m512 __A)
{
  struct __storeu_ps {
    __m512_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_ps*)__P)->__v = __A;
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_store_pd(void *__P, __mmask8 __U, __m512d __A)
{
  __builtin_ia32_storeapd512_mask ((__v8df *)__P, (__v8df) __A, (__mmask8) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_store_pd(void *__P, __m512d __A)
{
  *(__m512d*)__P = __A;
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_store_ps(void *__P, __mmask16 __U, __m512 __A)
{
  __builtin_ia32_storeaps512_mask ((__v16sf *)__P, (__v16sf) __A,
                                   (__mmask16) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_store_ps(void *__P, __m512 __A)
{
  *(__m512*)__P = __A;
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_store_si512 (void *__P, __m512i __A)
{
  *(__m512i *) __P = __A;
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_store_epi32 (void *__P, __m512i __A)
{
  *(__m512i *) __P = __A;
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_store_epi64 (void *__P, __m512i __A)
{
  *(__m512i *) __P = __A;
}



static __inline __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_knot(__mmask16 __M)
{
  return __builtin_ia32_knothi(__M);
}
# 4731 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtepi8_epi32(__m128i __A)
{


  return (__m512i)__builtin_convertvector((__v16qs)__A, __v16si);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepi8_epi32(__m512i __W, __mmask16 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_cvtepi8_epi32(__A),
                                             (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtepi8_epi32(__mmask16 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_cvtepi8_epi32(__A),
                                             (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtepi8_epi64(__m128i __A)
{


  return (__m512i)__builtin_convertvector(__builtin_shufflevector((__v16qs)__A, (__v16qs)__A, 0, 1, 2, 3, 4, 5, 6, 7), __v8di);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepi8_epi64(__m512i __W, __mmask8 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepi8_epi64(__A),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtepi8_epi64(__mmask8 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepi8_epi64(__A),
                                             (__v8di)_mm512_setzero_si512 ());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtepi32_epi64(__m256i __X)
{
  return (__m512i)__builtin_convertvector((__v8si)__X, __v8di);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepi32_epi64(__m512i __W, __mmask8 __U, __m256i __X)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepi32_epi64(__X),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtepi32_epi64(__mmask8 __U, __m256i __X)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepi32_epi64(__X),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtepi16_epi32(__m256i __A)
{
  return (__m512i)__builtin_convertvector((__v16hi)__A, __v16si);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepi16_epi32(__m512i __W, __mmask16 __U, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                            (__v16si)_mm512_cvtepi16_epi32(__A),
                                            (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtepi16_epi32(__mmask16 __U, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                            (__v16si)_mm512_cvtepi16_epi32(__A),
                                            (__v16si)_mm512_setzero_si512 ());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtepi16_epi64(__m128i __A)
{
  return (__m512i)__builtin_convertvector((__v8hi)__A, __v8di);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepi16_epi64(__m512i __W, __mmask8 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepi16_epi64(__A),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtepi16_epi64(__mmask8 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepi16_epi64(__A),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtepu8_epi32(__m128i __A)
{
  return (__m512i)__builtin_convertvector((__v16qu)__A, __v16si);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepu8_epi32(__m512i __W, __mmask16 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_cvtepu8_epi32(__A),
                                             (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtepu8_epi32(__mmask16 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_cvtepu8_epi32(__A),
                                             (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtepu8_epi64(__m128i __A)
{
  return (__m512i)__builtin_convertvector(__builtin_shufflevector((__v16qu)__A, (__v16qu)__A, 0, 1, 2, 3, 4, 5, 6, 7), __v8di);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepu8_epi64(__m512i __W, __mmask8 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepu8_epi64(__A),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtepu8_epi64(__mmask8 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepu8_epi64(__A),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtepu32_epi64(__m256i __X)
{
  return (__m512i)__builtin_convertvector((__v8su)__X, __v8di);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepu32_epi64(__m512i __W, __mmask8 __U, __m256i __X)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepu32_epi64(__X),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtepu32_epi64(__mmask8 __U, __m256i __X)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepu32_epi64(__X),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtepu16_epi32(__m256i __A)
{
  return (__m512i)__builtin_convertvector((__v16hu)__A, __v16si);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepu16_epi32(__m512i __W, __mmask16 __U, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                            (__v16si)_mm512_cvtepu16_epi32(__A),
                                            (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtepu16_epi32(__mmask16 __U, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                            (__v16si)_mm512_cvtepu16_epi32(__A),
                                            (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtepu16_epi64(__m128i __A)
{
  return (__m512i)__builtin_convertvector((__v8hu)__A, __v8di);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepu16_epi64(__m512i __W, __mmask8 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepu16_epi64(__A),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtepu16_epi64(__mmask8 __U, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_cvtepu16_epi64(__A),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_rorv_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_prorvd512((__v16si)__A, (__v16si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_rorv_epi32 (__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                           (__v16si)_mm512_rorv_epi32(__A, __B),
                                           (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_rorv_epi32 (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                           (__v16si)_mm512_rorv_epi32(__A, __B),
                                           (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_rorv_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_prorvq512((__v8di)__A, (__v8di)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_rorv_epi64 (__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512(__U,
                                            (__v8di)_mm512_rorv_epi64(__A, __B),
                                            (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_rorv_epi64 (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512(__U,
                                            (__v8di)_mm512_rorv_epi64(__A, __B),
                                            (__v8di)_mm512_setzero_si512());
}
# 5067 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_rolv_epi32 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_prolvd512((__v16si)__A, (__v16si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_rolv_epi32 (__m512i __W, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                           (__v16si)_mm512_rolv_epi32(__A, __B),
                                           (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_rolv_epi32 (__mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                           (__v16si)_mm512_rolv_epi32(__A, __B),
                                           (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_rolv_epi64 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_prolvq512((__v8di)__A, (__v8di)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_rolv_epi64 (__m512i __W, __mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512(__U,
                                            (__v8di)_mm512_rolv_epi64(__A, __B),
                                            (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_rolv_epi64 (__mmask8 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectq_512(__U,
                                            (__v8di)_mm512_rolv_epi64(__A, __B),
                                            (__v8di)_mm512_setzero_si512());
}
# 5137 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_slli_epi32(__m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_pslldi512((__v16si)__A, (int)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_slli_epi32(__m512i __W, __mmask16 __U, __m512i __A,
                       unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                         (__v16si)_mm512_slli_epi32(__A, __B),
                                         (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_slli_epi32(__mmask16 __U, __m512i __A, unsigned int __B) {
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                         (__v16si)_mm512_slli_epi32(__A, __B),
                                         (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_slli_epi64(__m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_psllqi512((__v8di)__A, (int)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_slli_epi64(__m512i __W, __mmask8 __U, __m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                          (__v8di)_mm512_slli_epi64(__A, __B),
                                          (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_slli_epi64(__mmask8 __U, __m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                          (__v8di)_mm512_slli_epi64(__A, __B),
                                          (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_srli_epi32(__m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_psrldi512((__v16si)__A, (int)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_srli_epi32(__m512i __W, __mmask16 __U, __m512i __A,
                       unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                         (__v16si)_mm512_srli_epi32(__A, __B),
                                         (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_srli_epi32(__mmask16 __U, __m512i __A, unsigned int __B) {
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                         (__v16si)_mm512_srli_epi32(__A, __B),
                                         (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_srli_epi64(__m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_psrlqi512((__v8di)__A, (int)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_srli_epi64(__m512i __W, __mmask8 __U, __m512i __A,
                       unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                          (__v8di)_mm512_srli_epi64(__A, __B),
                                          (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_srli_epi64(__mmask8 __U, __m512i __A,
                        unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                          (__v8di)_mm512_srli_epi64(__A, __B),
                                          (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_load_epi32 (__m512i __W, __mmask16 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_movdqa32load512_mask ((const __v16si *) __P,
              (__v16si) __W,
              (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_load_epi32 (__mmask16 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_movdqa32load512_mask ((const __v16si *) __P,
              (__v16si)
              _mm512_setzero_si512 (),
              (__mmask16) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_store_epi32 (void *__P, __mmask16 __U, __m512i __A)
{
  __builtin_ia32_movdqa32store512_mask ((__v16si *) __P, (__v16si) __A,
          (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_mov_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_selectd_512 ((__mmask16) __U,
                 (__v16si) __A,
                 (__v16si) __W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_mov_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_selectd_512 ((__mmask16) __U,
                 (__v16si) __A,
                 (__v16si) _mm512_setzero_si512 ());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_mov_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_selectq_512 ((__mmask8) __U,
                 (__v8di) __A,
                 (__v8di) __W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_mov_epi64 (__mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_selectq_512 ((__mmask8) __U,
                 (__v8di) __A,
                 (__v8di) _mm512_setzero_si512 ());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_load_epi64 (__m512i __W, __mmask8 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_movdqa64load512_mask ((const __v8di *) __P,
              (__v8di) __W,
              (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_load_epi64 (__mmask8 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_movdqa64load512_mask ((const __v8di *) __P,
              (__v8di)
              _mm512_setzero_si512 (),
              (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_store_epi64 (void *__P, __mmask8 __U, __m512i __A)
{
  __builtin_ia32_movdqa64store512_mask ((__v8di *) __P, (__v8di) __A,
          (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_movedup_pd (__m512d __A)
{
  return (__m512d)__builtin_shufflevector((__v8df)__A, (__v8df)__A,
                                          0, 0, 2, 2, 4, 4, 6, 6);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_movedup_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_movedup_pd(__A),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_movedup_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_movedup_pd(__A),
                                              (__v8df)_mm512_setzero_pd());
}
# 5495 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_getexp_sd (__m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_getexpsd128_round_mask ((__v2df) __A,
                 (__v2df) __B, (__v2df) _mm_setzero_pd(), (__mmask8) -1, 0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_getexp_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_getexpsd128_round_mask ( (__v2df) __A,
          (__v2df) __B,
          (__v2df) __W,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_getexp_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_getexpsd128_round_mask ( (__v2df) __A,
          (__v2df) __B,
          (__v2df) _mm_setzero_pd (),
          (__mmask8) __U,
          0x04);
}
# 5540 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_getexp_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_getexpss128_round_mask ((__v4sf) __A,
                (__v4sf) __B, (__v4sf) _mm_setzero_ps(), (__mmask8) -1, 0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_getexp_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_getexpss128_round_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) __W,
          (__mmask8) __U,
          0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_getexp_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_getexpss128_round_mask ((__v4sf) __A,
          (__v4sf) __B,
          (__v4sf) _mm_setzero_ps (),
          (__mmask8) __U,
          0x04);
}
# 5669 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_kmov (__mmask16 __A)
{
  return __A;
}
# 5688 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_sll_epi32(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_pslld512((__v16si) __A, (__v4si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_sll_epi32(__m512i __W, __mmask16 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                          (__v16si)_mm512_sll_epi32(__A, __B),
                                          (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_sll_epi32(__mmask16 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                          (__v16si)_mm512_sll_epi32(__A, __B),
                                          (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_sll_epi64(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_psllq512((__v8di)__A, (__v2di)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_sll_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_sll_epi64(__A, __B),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_sll_epi64(__mmask8 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                           (__v8di)_mm512_sll_epi64(__A, __B),
                                           (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_sllv_epi32(__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_psllv16si((__v16si)__X, (__v16si)__Y);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_sllv_epi32(__m512i __W, __mmask16 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                           (__v16si)_mm512_sllv_epi32(__X, __Y),
                                           (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_sllv_epi32(__mmask16 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                           (__v16si)_mm512_sllv_epi32(__X, __Y),
                                           (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_sllv_epi64(__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_psllv8di((__v8di)__X, (__v8di)__Y);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_sllv_epi64(__m512i __W, __mmask8 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                            (__v8di)_mm512_sllv_epi64(__X, __Y),
                                            (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_sllv_epi64(__mmask8 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                            (__v8di)_mm512_sllv_epi64(__X, __Y),
                                            (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_sra_epi32(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_psrad512((__v16si) __A, (__v4si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_sra_epi32(__m512i __W, __mmask16 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                          (__v16si)_mm512_sra_epi32(__A, __B),
                                          (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_sra_epi32(__mmask16 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                          (__v16si)_mm512_sra_epi32(__A, __B),
                                          (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_sra_epi64(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_psraq512((__v8di)__A, (__v2di)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_sra_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                           (__v8di)_mm512_sra_epi64(__A, __B),
                                           (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_sra_epi64(__mmask8 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                           (__v8di)_mm512_sra_epi64(__A, __B),
                                           (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_srav_epi32(__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_psrav16si((__v16si)__X, (__v16si)__Y);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_srav_epi32(__m512i __W, __mmask16 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                           (__v16si)_mm512_srav_epi32(__X, __Y),
                                           (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_srav_epi32(__mmask16 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                           (__v16si)_mm512_srav_epi32(__X, __Y),
                                           (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_srav_epi64(__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_psrav8di((__v8di)__X, (__v8di)__Y);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_srav_epi64(__m512i __W, __mmask8 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                            (__v8di)_mm512_srav_epi64(__X, __Y),
                                            (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_srav_epi64(__mmask8 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                            (__v8di)_mm512_srav_epi64(__X, __Y),
                                            (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_srl_epi32(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_psrld512((__v16si) __A, (__v4si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_srl_epi32(__m512i __W, __mmask16 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                          (__v16si)_mm512_srl_epi32(__A, __B),
                                          (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_srl_epi32(__mmask16 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                          (__v16si)_mm512_srl_epi32(__A, __B),
                                          (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_srl_epi64(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_psrlq512((__v8di)__A, (__v2di)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_srl_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                           (__v8di)_mm512_srl_epi64(__A, __B),
                                           (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_srl_epi64(__mmask8 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                           (__v8di)_mm512_srl_epi64(__A, __B),
                                           (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_srlv_epi32(__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_psrlv16si((__v16si)__X, (__v16si)__Y);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_srlv_epi32(__m512i __W, __mmask16 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                           (__v16si)_mm512_srlv_epi32(__X, __Y),
                                           (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_srlv_epi32(__mmask16 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                           (__v16si)_mm512_srlv_epi32(__X, __Y),
                                           (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_srlv_epi64 (__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_psrlv8di((__v8di)__X, (__v8di)__Y);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_srlv_epi64(__m512i __W, __mmask8 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                            (__v8di)_mm512_srlv_epi64(__X, __Y),
                                            (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_srlv_epi64(__mmask8 __U, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                            (__v8di)_mm512_srlv_epi64(__X, __Y),
                                            (__v8di)_mm512_setzero_si512());
}




typedef enum {
  _MM_TERNLOG_A = 0xF0,
  _MM_TERNLOG_B = 0xCC,
  _MM_TERNLOG_C = 0xAA
} _MM_TERNLOG_ENUM;
# 6005 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ unsigned __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_cvtsd_u32 (__m128d __A)
{
  return (unsigned) __builtin_ia32_vcvtsd2usi32 ((__v2df) __A,
             0x04);
}






static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_cvtsd_u64 (__m128d __A)
{
  return (unsigned long long) __builtin_ia32_vcvtsd2usi64 ((__v2df)
                 __A,
                 0x04);
}
# 6043 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ unsigned __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_cvtss_u32 (__m128 __A)
{
  return (unsigned) __builtin_ia32_vcvtss2usi32 ((__v4sf) __A,
             0x04);
}






static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_cvtss_u64 (__m128 __A)
{
  return (unsigned long long) __builtin_ia32_vcvtss2usi64 ((__v4sf)
                 __A,
                 0x04);
}
# 6070 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_cvttsd_i32 (__m128d __A)
{
  return (int) __builtin_ia32_vcvttsd2si32 ((__v2df) __A,
              0x04);
}
# 6084 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_cvttsd_i64 (__m128d __A)
{
  return (long long) __builtin_ia32_vcvttsd2si64 ((__v2df) __A,
              0x04);
}





static __inline__ unsigned __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_cvttsd_u32 (__m128d __A)
{
  return (unsigned) __builtin_ia32_vcvttsd2usi32 ((__v2df) __A,
              0x04);
}






static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_cvttsd_u64 (__m128d __A)
{
  return (unsigned long long) __builtin_ia32_vcvttsd2usi64 ((__v2df)
                  __A,
                  0x04);
}
# 6122 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_cvttss_i32 (__m128 __A)
{
  return (int) __builtin_ia32_vcvttss2si32 ((__v4sf) __A,
              0x04);
}
# 6136 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_cvttss_i64 (__m128 __A)
{
  return (long long) __builtin_ia32_vcvttss2si64 ((__v4sf) __A,
              0x04);
}





static __inline__ unsigned __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_cvttss_u32 (__m128 __A)
{
  return (unsigned) __builtin_ia32_vcvttss2usi32 ((__v4sf) __A,
              0x04);
}






static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_cvttss_u64 (__m128 __A)
{
  return (unsigned long long) __builtin_ia32_vcvttss2usi64 ((__v4sf)
                  __A,
                  0x04);
}
# 6194 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_permutevar_pd(__m512d __A, __m512i __C)
{
  return (__m512d)__builtin_ia32_vpermilvarpd512((__v8df)__A, (__v8di)__C);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_permutevar_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512i __C)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                         (__v8df)_mm512_permutevar_pd(__A, __C),
                                         (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_permutevar_pd(__mmask8 __U, __m512d __A, __m512i __C)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                         (__v8df)_mm512_permutevar_pd(__A, __C),
                                         (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_permutevar_ps(__m512 __A, __m512i __C)
{
  return (__m512)__builtin_ia32_vpermilvarps512((__v16sf)__A, (__v16si)__C);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_permutevar_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512i __C)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                        (__v16sf)_mm512_permutevar_ps(__A, __C),
                                        (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_permutevar_ps(__mmask16 __U, __m512 __A, __m512i __C)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                        (__v16sf)_mm512_permutevar_ps(__A, __C),
                                        (__v16sf)_mm512_setzero_ps());
}

static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_permutex2var_pd(__m512d __A, __m512i __I, __m512d __B)
{
  return (__m512d)__builtin_ia32_vpermi2varpd512((__v8df)__A, (__v8di)__I,
                                                 (__v8df)__B);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_permutex2var_pd(__m512d __A, __mmask8 __U, __m512i __I, __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512(__U,
                                  (__v8df)_mm512_permutex2var_pd(__A, __I, __B),
                                  (__v8df)__A);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask2_permutex2var_pd(__m512d __A, __m512i __I, __mmask8 __U,
                             __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512(__U,
                                  (__v8df)_mm512_permutex2var_pd(__A, __I, __B),
                                  (__v8df)(__m512d)__I);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_permutex2var_pd(__mmask8 __U, __m512d __A, __m512i __I,
                             __m512d __B)
{
  return (__m512d)__builtin_ia32_selectpd_512(__U,
                                  (__v8df)_mm512_permutex2var_pd(__A, __I, __B),
                                  (__v8df)_mm512_setzero_pd());
}

static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_permutex2var_ps(__m512 __A, __m512i __I, __m512 __B)
{
  return (__m512)__builtin_ia32_vpermi2varps512((__v16sf)__A, (__v16si)__I,
                                                (__v16sf) __B);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_permutex2var_ps(__m512 __A, __mmask16 __U, __m512i __I, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512(__U,
                                 (__v16sf)_mm512_permutex2var_ps(__A, __I, __B),
                                 (__v16sf)__A);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask2_permutex2var_ps(__m512 __A, __m512i __I, __mmask16 __U, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512(__U,
                                 (__v16sf)_mm512_permutex2var_ps(__A, __I, __B),
                                 (__v16sf)(__m512)__I);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_permutex2var_ps(__mmask16 __U, __m512 __A, __m512i __I, __m512 __B)
{
  return (__m512)__builtin_ia32_selectps_512(__U,
                                 (__v16sf)_mm512_permutex2var_ps(__A, __I, __B),
                                 (__v16sf)_mm512_setzero_ps());
}
# 6318 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvttpd_epu32 (__m512d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2udq512_mask ((__v8df) __A,
                  (__v8si)
                  _mm256_undefined_si256 (),
                  (__mmask8) -1,
                  0x04);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvttpd_epu32 (__m256i __W, __mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2udq512_mask ((__v8df) __A,
                  (__v8si) __W,
                  (__mmask8) __U,
                  0x04);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvttpd_epu32 (__mmask8 __U, __m512d __A)
{
  return (__m256i) __builtin_ia32_cvttpd2udq512_mask ((__v8df) __A,
                  (__v8si)
                  _mm256_setzero_si256 (),
                  (__mmask8) __U,
                  0x04);
}
# 6449 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_scalef_pd (__m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_scalefpd512_mask ((__v8df) __A,
                (__v8df) __B,
                (__v8df)
                _mm512_undefined_pd (),
                (__mmask8) -1,
                0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_scalef_pd (__m512d __W, __mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_scalefpd512_mask ((__v8df) __A,
                (__v8df) __B,
                (__v8df) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_scalef_pd (__mmask8 __U, __m512d __A, __m512d __B)
{
  return (__m512d) __builtin_ia32_scalefpd512_mask ((__v8df) __A,
                (__v8df) __B,
                (__v8df)
                _mm512_setzero_pd (),
                (__mmask8) __U,
                0x04);
}
# 6499 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_scalef_ps (__m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_scalefps512_mask ((__v16sf) __A,
               (__v16sf) __B,
               (__v16sf)
               _mm512_undefined_ps (),
               (__mmask16) -1,
               0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_scalef_ps (__m512 __W, __mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_scalefps512_mask ((__v16sf) __A,
               (__v16sf) __B,
               (__v16sf) __W,
               (__mmask16) __U,
               0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_scalef_ps (__mmask16 __U, __m512 __A, __m512 __B)
{
  return (__m512) __builtin_ia32_scalefps512_mask ((__v16sf) __A,
               (__v16sf) __B,
               (__v16sf)
               _mm512_setzero_ps (),
               (__mmask16) __U,
               0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_scalef_sd (__m128d __A, __m128d __B)
{
  return (__m128d) __builtin_ia32_scalefsd_round_mask ((__v2df) __A,
              (__v2df)( __B), (__v2df) _mm_setzero_pd(),
              (__mmask8) -1,
              0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_scalef_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_scalefsd_round_mask ( (__v2df) __A,
                 (__v2df) __B,
                (__v2df) __W,
                (__mmask8) __U,
                0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_scalef_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_scalefsd_round_mask ( (__v2df) __A,
                 (__v2df) __B,
                (__v2df) _mm_setzero_pd (),
                (__mmask8) __U,
                0x04);
}
# 6584 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_scalef_ss (__m128 __A, __m128 __B)
{
  return (__m128) __builtin_ia32_scalefss_round_mask ((__v4sf) __A,
             (__v4sf)( __B), (__v4sf) _mm_setzero_ps(),
             (__mmask8) -1,
             0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_scalef_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_scalefss_round_mask ( (__v4sf) __A,
                (__v4sf) __B,
                (__v4sf) __W,
                (__mmask8) __U,
                0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_scalef_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_scalefss_round_mask ( (__v4sf) __A,
                 (__v4sf) __B,
                (__v4sf) _mm_setzero_ps (),
                (__mmask8) __U,
                0x04);
}
# 6626 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_srai_epi32(__m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_psradi512((__v16si)__A, (int)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_srai_epi32(__m512i __W, __mmask16 __U, __m512i __A,
                       unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                         (__v16si)_mm512_srai_epi32(__A, __B),
                                         (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_srai_epi32(__mmask16 __U, __m512i __A,
                        unsigned int __B) {
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                         (__v16si)_mm512_srai_epi32(__A, __B),
                                         (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_srai_epi64(__m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_psraqi512((__v8di)__A, (int)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_srai_epi64(__m512i __W, __mmask8 __U, __m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                          (__v8di)_mm512_srai_epi64(__A, __B),
                                          (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_srai_epi64(__mmask8 __U, __m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                          (__v8di)_mm512_srai_epi64(__A, __B),
                                          (__v8di)_mm512_setzero_si512());
}
# 6761 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_sqrt_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_sqrtsd_round_mask ( (__v2df) __A,
                 (__v2df) __B,
                (__v2df) __W,
                (__mmask8) __U,
                0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_sqrt_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
 return (__m128d) __builtin_ia32_sqrtsd_round_mask ( (__v2df) __A,
                 (__v2df) __B,
                (__v2df) _mm_setzero_pd (),
                (__mmask8) __U,
                0x04);
}
# 6799 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_sqrt_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_sqrtss_round_mask ( (__v4sf) __A,
                 (__v4sf) __B,
                (__v4sf) __W,
                (__mmask8) __U,
                0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_sqrt_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
 return (__m128) __builtin_ia32_sqrtss_round_mask ( (__v4sf) __A,
                 (__v4sf) __B,
                (__v4sf) _mm_setzero_ps (),
                (__mmask8) __U,
                0x04);
}







static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_broadcast_f32x4(__m128 __A)
{
  return (__m512)__builtin_shufflevector((__v4sf)__A, (__v4sf)__A,
                                         0, 1, 2, 3, 0, 1, 2, 3,
                                         0, 1, 2, 3, 0, 1, 2, 3);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_broadcast_f32x4(__m512 __O, __mmask16 __M, __m128 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__M,
                                           (__v16sf)_mm512_broadcast_f32x4(__A),
                                           (__v16sf)__O);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_broadcast_f32x4(__mmask16 __M, __m128 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__M,
                                           (__v16sf)_mm512_broadcast_f32x4(__A),
                                           (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_broadcast_f64x4(__m256d __A)
{
  return (__m512d)__builtin_shufflevector((__v4df)__A, (__v4df)__A,
                                          0, 1, 2, 3, 0, 1, 2, 3);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_broadcast_f64x4(__m512d __O, __mmask8 __M, __m256d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__M,
                                            (__v8df)_mm512_broadcast_f64x4(__A),
                                            (__v8df)__O);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_broadcast_f64x4(__mmask8 __M, __m256d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__M,
                                            (__v8df)_mm512_broadcast_f64x4(__A),
                                            (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_broadcast_i32x4(__m128i __A)
{
  return (__m512i)__builtin_shufflevector((__v4si)__A, (__v4si)__A,
                                          0, 1, 2, 3, 0, 1, 2, 3,
                                          0, 1, 2, 3, 0, 1, 2, 3);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_broadcast_i32x4(__m512i __O, __mmask16 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                           (__v16si)_mm512_broadcast_i32x4(__A),
                                           (__v16si)__O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_broadcast_i32x4(__mmask16 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                           (__v16si)_mm512_broadcast_i32x4(__A),
                                           (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_broadcast_i64x4(__m256i __A)
{
  return (__m512i)__builtin_shufflevector((__v4di)__A, (__v4di)__A,
                                          0, 1, 2, 3, 0, 1, 2, 3);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_broadcast_i64x4(__m512i __O, __mmask8 __M, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                            (__v8di)_mm512_broadcast_i64x4(__A),
                                            (__v8di)__O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_broadcast_i64x4(__mmask8 __M, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                            (__v8di)_mm512_broadcast_i64x4(__A),
                                            (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_broadcastsd_pd (__m512d __O, __mmask8 __M, __m128d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512(__M,
                                              (__v8df) _mm512_broadcastsd_pd(__A),
                                              (__v8df) __O);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_broadcastsd_pd (__mmask8 __M, __m128d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512(__M,
                                              (__v8df) _mm512_broadcastsd_pd(__A),
                                              (__v8df) _mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_broadcastss_ps (__m512 __O, __mmask16 __M, __m128 __A)
{
  return (__m512)__builtin_ia32_selectps_512(__M,
                                             (__v16sf) _mm512_broadcastss_ps(__A),
                                             (__v16sf) __O);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_broadcastss_ps (__mmask16 __M, __m128 __A)
{
  return (__m512)__builtin_ia32_selectps_512(__M,
                                             (__v16sf) _mm512_broadcastss_ps(__A),
                                             (__v16sf) _mm512_setzero_ps());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtsepi32_epi8 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb512_mask ((__v16si) __A,
               (__v16qi) _mm_undefined_si128 (),
               (__mmask16) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtsepi32_epi8 (__m128i __O, __mmask16 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb512_mask ((__v16si) __A,
               (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtsepi32_epi8 (__mmask16 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb512_mask ((__v16si) __A,
               (__v16qi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtsepi32_storeu_epi8 (void * __P, __mmask16 __M, __m512i __A)
{
  __builtin_ia32_pmovsdb512mem_mask ((__v16qi *) __P, (__v16si) __A, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtsepi32_epi16 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovsdw512_mask ((__v16si) __A,
               (__v16hi) _mm256_undefined_si256 (),
               (__mmask16) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtsepi32_epi16 (__m256i __O, __mmask16 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovsdw512_mask ((__v16si) __A,
               (__v16hi) __O, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtsepi32_epi16 (__mmask16 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovsdw512_mask ((__v16si) __A,
               (__v16hi) _mm256_setzero_si256 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtsepi32_storeu_epi16 (void *__P, __mmask16 __M, __m512i __A)
{
  __builtin_ia32_pmovsdw512mem_mask ((__v16hi*) __P, (__v16si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtsepi64_epi8 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb512_mask ((__v8di) __A,
               (__v16qi) _mm_undefined_si128 (),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtsepi64_epi8 (__m128i __O, __mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb512_mask ((__v8di) __A,
               (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtsepi64_epi8 (__mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb512_mask ((__v8di) __A,
               (__v16qi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtsepi64_storeu_epi8 (void * __P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovsqb512mem_mask ((__v16qi *) __P, (__v8di) __A, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtsepi64_epi32 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovsqd512_mask ((__v8di) __A,
               (__v8si) _mm256_undefined_si256 (),
               (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtsepi64_epi32 (__m256i __O, __mmask8 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovsqd512_mask ((__v8di) __A,
               (__v8si) __O, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtsepi64_epi32 (__mmask8 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovsqd512_mask ((__v8di) __A,
               (__v8si) _mm256_setzero_si256 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtsepi64_storeu_epi32 (void *__P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovsqd512mem_mask ((__v8si *) __P, (__v8di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtsepi64_epi16 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw512_mask ((__v8di) __A,
               (__v8hi) _mm_undefined_si128 (),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtsepi64_epi16 (__m128i __O, __mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw512_mask ((__v8di) __A,
               (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtsepi64_epi16 (__mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw512_mask ((__v8di) __A,
               (__v8hi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtsepi64_storeu_epi16 (void * __P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovsqw512mem_mask ((__v8hi *) __P, (__v8di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtusepi32_epi8 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb512_mask ((__v16si) __A,
                (__v16qi) _mm_undefined_si128 (),
                (__mmask16) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtusepi32_epi8 (__m128i __O, __mmask16 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb512_mask ((__v16si) __A,
                (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtusepi32_epi8 (__mmask16 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb512_mask ((__v16si) __A,
                (__v16qi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtusepi32_storeu_epi8 (void * __P, __mmask16 __M, __m512i __A)
{
  __builtin_ia32_pmovusdb512mem_mask ((__v16qi *) __P, (__v16si) __A, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtusepi32_epi16 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovusdw512_mask ((__v16si) __A,
                (__v16hi) _mm256_undefined_si256 (),
                (__mmask16) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtusepi32_epi16 (__m256i __O, __mmask16 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovusdw512_mask ((__v16si) __A,
                (__v16hi) __O,
                __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtusepi32_epi16 (__mmask16 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovusdw512_mask ((__v16si) __A,
                (__v16hi) _mm256_setzero_si256 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtusepi32_storeu_epi16 (void *__P, __mmask16 __M, __m512i __A)
{
  __builtin_ia32_pmovusdw512mem_mask ((__v16hi*) __P, (__v16si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtusepi64_epi8 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb512_mask ((__v8di) __A,
                (__v16qi) _mm_undefined_si128 (),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtusepi64_epi8 (__m128i __O, __mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb512_mask ((__v8di) __A,
                (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtusepi64_epi8 (__mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb512_mask ((__v8di) __A,
                (__v16qi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtusepi64_storeu_epi8 (void * __P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovusqb512mem_mask ((__v16qi *) __P, (__v8di) __A, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtusepi64_epi32 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovusqd512_mask ((__v8di) __A,
                (__v8si) _mm256_undefined_si256 (),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtusepi64_epi32 (__m256i __O, __mmask8 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovusqd512_mask ((__v8di) __A,
                (__v8si) __O, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtusepi64_epi32 (__mmask8 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovusqd512_mask ((__v8di) __A,
                (__v8si) _mm256_setzero_si256 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtusepi64_storeu_epi32 (void* __P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovusqd512mem_mask ((__v8si*) __P, (__v8di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtusepi64_epi16 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw512_mask ((__v8di) __A,
                (__v8hi) _mm_undefined_si128 (),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtusepi64_epi16 (__m128i __O, __mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw512_mask ((__v8di) __A,
                (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtusepi64_epi16 (__mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw512_mask ((__v8di) __A,
                (__v8hi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtusepi64_storeu_epi16 (void *__P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovusqw512mem_mask ((__v8hi*) __P, (__v8di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtepi32_epi8 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovdb512_mask ((__v16si) __A,
              (__v16qi) _mm_undefined_si128 (),
              (__mmask16) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepi32_epi8 (__m128i __O, __mmask16 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovdb512_mask ((__v16si) __A,
              (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtepi32_epi8 (__mmask16 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovdb512_mask ((__v16si) __A,
              (__v16qi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepi32_storeu_epi8 (void * __P, __mmask16 __M, __m512i __A)
{
  __builtin_ia32_pmovdb512mem_mask ((__v16qi *) __P, (__v16si) __A, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtepi32_epi16 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovdw512_mask ((__v16si) __A,
              (__v16hi) _mm256_undefined_si256 (),
              (__mmask16) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepi32_epi16 (__m256i __O, __mmask16 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovdw512_mask ((__v16si) __A,
              (__v16hi) __O, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtepi32_epi16 (__mmask16 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovdw512_mask ((__v16si) __A,
              (__v16hi) _mm256_setzero_si256 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepi32_storeu_epi16 (void * __P, __mmask16 __M, __m512i __A)
{
  __builtin_ia32_pmovdw512mem_mask ((__v16hi *) __P, (__v16si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtepi64_epi8 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovqb512_mask ((__v8di) __A,
              (__v16qi) _mm_undefined_si128 (),
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepi64_epi8 (__m128i __O, __mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovqb512_mask ((__v8di) __A,
              (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtepi64_epi8 (__mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovqb512_mask ((__v8di) __A,
              (__v16qi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepi64_storeu_epi8 (void * __P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovqb512mem_mask ((__v16qi *) __P, (__v8di) __A, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtepi64_epi32 (__m512i __A)
{
  return (__m256i) __builtin_ia32_pmovqd512_mask ((__v8di) __A,
              (__v8si) _mm256_undefined_si256 (),
              (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepi64_epi32 (__m256i __O, __mmask8 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovqd512_mask ((__v8di) __A,
              (__v8si) __O, __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtepi64_epi32 (__mmask8 __M, __m512i __A)
{
  return (__m256i) __builtin_ia32_pmovqd512_mask ((__v8di) __A,
              (__v8si) _mm256_setzero_si256 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepi64_storeu_epi32 (void* __P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovqd512mem_mask ((__v8si *) __P, (__v8di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtepi64_epi16 (__m512i __A)
{
  return (__m128i) __builtin_ia32_pmovqw512_mask ((__v8di) __A,
              (__v8hi) _mm_undefined_si128 (),
              (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepi64_epi16 (__m128i __O, __mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovqw512_mask ((__v8di) __A,
              (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtepi64_epi16 (__mmask8 __M, __m512i __A)
{
  return (__m128i) __builtin_ia32_pmovqw512_mask ((__v8di) __A,
              (__v8hi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtepi64_storeu_epi16 (void *__P, __mmask8 __M, __m512i __A)
{
  __builtin_ia32_pmovqw512mem_mask ((__v8hi *) __P, (__v8di) __A, __M);
}
# 7574 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_getexp_pd (__m512d __A)
{
  return (__m512d) __builtin_ia32_getexppd512_mask ((__v8df) __A,
                (__v8df) _mm512_undefined_pd (),
                (__mmask8) -1,
                0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_getexp_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_getexppd512_mask ((__v8df) __A,
                (__v8df) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_getexp_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_getexppd512_mask ((__v8df) __A,
                (__v8df) _mm512_setzero_pd (),
                (__mmask8) __U,
                0x04);
}
# 7616 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_getexp_ps (__m512 __A)
{
  return (__m512) __builtin_ia32_getexpps512_mask ((__v16sf) __A,
               (__v16sf) _mm512_undefined_ps (),
               (__mmask16) -1,
               0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_getexp_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_getexpps512_mask ((__v16sf) __A,
               (__v16sf) __W,
               (__mmask16) __U,
               0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_getexp_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_getexpps512_mask ((__v16sf) __A,
               (__v16sf) _mm512_setzero_ps (),
               (__mmask16) __U,
               0x04);
}
# 7819 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_fmadd_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return __builtin_ia32_vfmaddss3_mask((__v4sf)__W,
                                       (__v4sf)__A,
                                       (__v4sf)__B,
                                       (__mmask8)__U,
                                       0x04);
}
# 7841 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_fmadd_ss (__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return __builtin_ia32_vfmaddss3_maskz((__v4sf)__A,
                                        (__v4sf)__B,
                                        (__v4sf)__C,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask3_fmadd_ss (__m128 __W, __m128 __X, __m128 __Y, __mmask8 __U)
{
  return __builtin_ia32_vfmaddss3_mask3((__v4sf)__W,
                                        (__v4sf)__X,
                                        (__v4sf)__Y,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_fmsub_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return __builtin_ia32_vfmaddss3_mask((__v4sf)__W,
                                       (__v4sf)__A,
                                       -(__v4sf)__B,
                                       (__mmask8)__U,
                                       0x04);
}
# 7895 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_fmsub_ss (__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return __builtin_ia32_vfmaddss3_maskz((__v4sf)__A,
                                        (__v4sf)__B,
                                        -(__v4sf)__C,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask3_fmsub_ss (__m128 __W, __m128 __X, __m128 __Y, __mmask8 __U)
{
  return __builtin_ia32_vfmsubss3_mask3((__v4sf)__W,
                                        (__v4sf)__X,
                                        (__v4sf)__Y,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_fnmadd_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return __builtin_ia32_vfmaddss3_mask((__v4sf)__W,
                                       -(__v4sf)__A,
                                       (__v4sf)__B,
                                       (__mmask8)__U,
                                       0x04);
}
# 7949 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_fnmadd_ss (__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return __builtin_ia32_vfmaddss3_maskz((__v4sf)__A,
                                        -(__v4sf)__B,
                                        (__v4sf)__C,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask3_fnmadd_ss (__m128 __W, __m128 __X, __m128 __Y, __mmask8 __U)
{
  return __builtin_ia32_vfmaddss3_mask3((__v4sf)__W,
                                        -(__v4sf)__X,
                                        (__v4sf)__Y,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_fnmsub_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return __builtin_ia32_vfmaddss3_mask((__v4sf)__W,
                                       -(__v4sf)__A,
                                       -(__v4sf)__B,
                                       (__mmask8)__U,
                                       0x04);
}
# 8003 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_fnmsub_ss (__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return __builtin_ia32_vfmaddss3_maskz((__v4sf)__A,
                                        -(__v4sf)__B,
                                        -(__v4sf)__C,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask3_fnmsub_ss (__m128 __W, __m128 __X, __m128 __Y, __mmask8 __U)
{
  return __builtin_ia32_vfmsubss3_mask3((__v4sf)__W,
                                        -(__v4sf)__X,
                                        (__v4sf)__Y,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_fmadd_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return __builtin_ia32_vfmaddsd3_mask((__v2df)__W,
                                       (__v2df)__A,
                                       (__v2df)__B,
                                       (__mmask8)__U,
                                       0x04);
}
# 8057 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_fmadd_sd (__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return __builtin_ia32_vfmaddsd3_maskz((__v2df)__A,
                                        (__v2df)__B,
                                        (__v2df)__C,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask3_fmadd_sd (__m128d __W, __m128d __X, __m128d __Y, __mmask8 __U)
{
  return __builtin_ia32_vfmaddsd3_mask3((__v2df)__W,
                                        (__v2df)__X,
                                        (__v2df)__Y,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_fmsub_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return __builtin_ia32_vfmaddsd3_mask((__v2df)__W,
                                       (__v2df)__A,
                                       -(__v2df)__B,
                                       (__mmask8)__U,
                                       0x04);
}
# 8111 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_fmsub_sd (__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return __builtin_ia32_vfmaddsd3_maskz((__v2df)__A,
                                        (__v2df)__B,
                                        -(__v2df)__C,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask3_fmsub_sd (__m128d __W, __m128d __X, __m128d __Y, __mmask8 __U)
{
  return __builtin_ia32_vfmsubsd3_mask3((__v2df)__W,
                                        (__v2df)__X,
                                        (__v2df)__Y,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_fnmadd_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return __builtin_ia32_vfmaddsd3_mask((__v2df)__W,
                                       -(__v2df)__A,
                                       (__v2df)__B,
                                       (__mmask8)__U,
                                       0x04);
}
# 8165 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_fnmadd_sd (__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return __builtin_ia32_vfmaddsd3_maskz((__v2df)__A,
                                        -(__v2df)__B,
                                        (__v2df)__C,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask3_fnmadd_sd (__m128d __W, __m128d __X, __m128d __Y, __mmask8 __U)
{
  return __builtin_ia32_vfmaddsd3_mask3((__v2df)__W,
                                        -(__v2df)__X,
                                        (__v2df)__Y,
                                        (__mmask8)__U,
                                        0x04);
}







static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_fnmsub_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return __builtin_ia32_vfmaddsd3_mask((__v2df)__W,
                                       -(__v2df)__A,
                                       -(__v2df)__B,
                                       (__mmask8)__U,
                                       0x04);
}
# 8219 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_fnmsub_sd (__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return __builtin_ia32_vfmaddsd3_maskz((__v2df)__A,
                                        -(__v2df)__B,
                                        -(__v2df)__C,
                                        (__mmask8)__U,
                                        0x04);
}
# 8236 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask3_fnmsub_sd (__m128d __W, __m128d __X, __m128d __Y, __mmask8 __U)
{
  return __builtin_ia32_vfmsubsd3_mask3((__v2df)__W,
                                        -(__v2df)__X,
                                        (__v2df)__Y,
                                        (__mmask8)__U,
                                        0x04);
}
# 8278 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_permutexvar_pd (__m512i __X, __m512d __Y)
{
  return (__m512d)__builtin_ia32_permvardf512((__v8df) __Y, (__v8di) __X);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_permutexvar_pd (__m512d __W, __mmask8 __U, __m512i __X, __m512d __Y)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                        (__v8df)_mm512_permutexvar_pd(__X, __Y),
                                        (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_permutexvar_pd (__mmask8 __U, __m512i __X, __m512d __Y)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                        (__v8df)_mm512_permutexvar_pd(__X, __Y),
                                        (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_permutexvar_epi64 (__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_permvardi512((__v8di)__Y, (__v8di)__X);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_permutexvar_epi64 (__mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                     (__v8di)_mm512_permutexvar_epi64(__X, __Y),
                                     (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_permutexvar_epi64 (__m512i __W, __mmask8 __M, __m512i __X,
             __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                     (__v8di)_mm512_permutexvar_epi64(__X, __Y),
                                     (__v8di)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_permutexvar_ps (__m512i __X, __m512 __Y)
{
  return (__m512)__builtin_ia32_permvarsf512((__v16sf)__Y, (__v16si)__X);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_permutexvar_ps (__m512 __W, __mmask16 __U, __m512i __X, __m512 __Y)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                       (__v16sf)_mm512_permutexvar_ps(__X, __Y),
                                       (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_permutexvar_ps (__mmask16 __U, __m512i __X, __m512 __Y)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                       (__v16sf)_mm512_permutexvar_ps(__X, __Y),
                                       (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_permutexvar_epi32 (__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_permvarsi512((__v16si)__Y, (__v16si)__X);
}



static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_permutexvar_epi32 (__mmask16 __M, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                    (__v16si)_mm512_permutexvar_epi32(__X, __Y),
                                    (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_permutexvar_epi32 (__m512i __W, __mmask16 __M, __m512i __X,
             __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                    (__v16si)_mm512_permutexvar_epi32(__X, __Y),
                                    (__v16si)__W);
}



static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_kand (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_kandhi ((__mmask16) __A, (__mmask16) __B);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_kandn (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_kandnhi ((__mmask16) __A, (__mmask16) __B);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_kor (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_korhi ((__mmask16) __A, (__mmask16) __B);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_kortestc (__mmask16 __A, __mmask16 __B)
{
  return __builtin_ia32_kortestchi ((__mmask16) __A, (__mmask16) __B);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_kortestz (__mmask16 __A, __mmask16 __B)
{
  return __builtin_ia32_kortestzhi ((__mmask16) __A, (__mmask16) __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_kortestc_mask16_u8(__mmask16 __A, __mmask16 __B)
{
  return (unsigned char)__builtin_ia32_kortestchi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_kortestz_mask16_u8(__mmask16 __A, __mmask16 __B)
{
  return (unsigned char)__builtin_ia32_kortestzhi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_kortest_mask16_u8(__mmask16 __A, __mmask16 __B, unsigned char *__C) {
  *__C = (unsigned char)__builtin_ia32_kortestchi(__A, __B);
  return (unsigned char)__builtin_ia32_kortestzhi(__A, __B);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_kunpackb (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_kunpckhi ((__mmask16) __A, (__mmask16) __B);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_kxnor (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_kxnorhi ((__mmask16) __A, (__mmask16) __B);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_mm512_kxor (__mmask16 __A, __mmask16 __B)
{
  return (__mmask16) __builtin_ia32_kxorhi ((__mmask16) __A, (__mmask16) __B);
}
# 8451 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_cvtmask16_u32(__mmask16 __A) {
  return (unsigned int)__builtin_ia32_kmovw((__mmask16)__A);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_cvtu32_mask16(unsigned int __A) {
  return (__mmask16)__builtin_ia32_kmovw((__mmask16)__A);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_load_mask16(__mmask16 *__A) {
  return (__mmask16)__builtin_ia32_kmovw(*(__mmask16 *)__A);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f")))
_store_mask16(__mmask16 *__A, __mmask16 __B) {
  *(__mmask16 *)__A = __builtin_ia32_kmovw((__mmask16)__B);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_stream_si512 (void * __P, __m512i __A)
{
  typedef __v8di __v8di_aligned __attribute__((aligned(64)));
  __builtin_nontemporal_store((__v8di_aligned)__A, (__v8di_aligned*)__P);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_stream_load_si512 (void const *__P)
{
  typedef __v8di __v8di_aligned __attribute__((aligned(64)));
  return (__m512i) __builtin_nontemporal_load((const __v8di_aligned *)__P);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_stream_pd (void *__P, __m512d __A)
{
  typedef __v8df __v8df_aligned __attribute__((aligned(64)));
  __builtin_nontemporal_store((__v8df_aligned)__A, (__v8df_aligned*)__P);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_stream_ps (void *__P, __m512 __A)
{
  typedef __v16sf __v16sf_aligned __attribute__((aligned(64)));
  __builtin_nontemporal_store((__v16sf_aligned)__A, (__v16sf_aligned*)__P);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_compress_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_compressdf512_mask ((__v8df) __A,
                  (__v8df) __W,
                  (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_compress_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_compressdf512_mask ((__v8df) __A,
                  (__v8df)
                  _mm512_setzero_pd (),
                  (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_compress_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_compressdi512_mask ((__v8di) __A,
                  (__v8di) __W,
                  (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_compress_epi64 (__mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_compressdi512_mask ((__v8di) __A,
                  (__v8di)
                  _mm512_setzero_si512 (),
                  (__mmask8) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_compress_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_compresssf512_mask ((__v16sf) __A,
                 (__v16sf) __W,
                 (__mmask16) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_compress_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_compresssf512_mask ((__v16sf) __A,
                 (__v16sf)
                 _mm512_setzero_ps (),
                 (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_compress_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_compresssi512_mask ((__v16si) __A,
                  (__v16si) __W,
                  (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_compress_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_compresssi512_mask ((__v16si) __A,
                  (__v16si)
                  _mm512_setzero_si512 (),
                  (__mmask16) __U);
}
# 8613 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_test_epi32_mask (__m512i __A, __m512i __B)
{
  return ((__mmask16)__builtin_ia32_cmpd512_mask((__v16si)(__m512i)((_mm512_and_epi32(__A, __B))), (__v16si)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_NE), (__mmask16)-1));

}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_test_epi32_mask (__mmask16 __U, __m512i __A, __m512i __B)
{
  return ((__mmask16)__builtin_ia32_cmpd512_mask((__v16si)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v16si)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_NE), (__mmask16)((__U))));

}

static __inline __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_test_epi64_mask (__m512i __A, __m512i __B)
{
  return ((__mmask8)__builtin_ia32_cmpq512_mask((__v8di)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v8di)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_NE), (__mmask8)-1));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_test_epi64_mask (__mmask8 __U, __m512i __A, __m512i __B)
{
  return ((__mmask8)__builtin_ia32_cmpq512_mask((__v8di)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v8di)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_NE), (__mmask8)((__U))));

}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_testn_epi32_mask (__m512i __A, __m512i __B)
{
  return ((__mmask16)__builtin_ia32_cmpd512_mask((__v16si)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v16si)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_EQ), (__mmask16)-1));

}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_testn_epi32_mask (__mmask16 __U, __m512i __A, __m512i __B)
{
  return ((__mmask16)__builtin_ia32_cmpd512_mask((__v16si)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v16si)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_EQ), (__mmask16)((__U))));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_testn_epi64_mask (__m512i __A, __m512i __B)
{
  return ((__mmask8)__builtin_ia32_cmpq512_mask((__v8di)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v8di)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_EQ), (__mmask8)-1));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_testn_epi64_mask (__mmask8 __U, __m512i __A, __m512i __B)
{
  return ((__mmask8)__builtin_ia32_cmpq512_mask((__v8di)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v8di)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_EQ), (__mmask8)((__U))));

}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_movehdup_ps (__m512 __A)
{
  return (__m512)__builtin_shufflevector((__v16sf)__A, (__v16sf)__A,
                         1, 1, 3, 3, 5, 5, 7, 7, 9, 9, 11, 11, 13, 13, 15, 15);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_movehdup_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_movehdup_ps(__A),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_movehdup_ps (__mmask16 __U, __m512 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_movehdup_ps(__A),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_moveldup_ps (__m512 __A)
{
  return (__m512)__builtin_shufflevector((__v16sf)__A, (__v16sf)__A,
                         0, 0, 2, 2, 4, 4, 6, 6, 8, 8, 10, 10, 12, 12, 14, 14);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_moveldup_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_moveldup_ps(__A),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_moveldup_ps (__mmask16 __U, __m512 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_moveldup_ps(__A),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_move_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return __builtin_ia32_selectss_128(__U, _mm_move_ss(__A, __B), __W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_move_ss (__mmask8 __U, __m128 __A, __m128 __B)
{
  return __builtin_ia32_selectss_128(__U, _mm_move_ss(__A, __B),
                                     _mm_setzero_ps());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_move_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return __builtin_ia32_selectsd_128(__U, _mm_move_sd(__A, __B), __W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_move_sd (__mmask8 __U, __m128d __A, __m128d __B)
{
  return __builtin_ia32_selectsd_128(__U, _mm_move_sd(__A, __B),
                                     _mm_setzero_pd());
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_store_ss (float * __W, __mmask8 __U, __m128 __A)
{
  __builtin_ia32_storess128_mask ((__v4sf *)__W, __A, __U & 1);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_store_sd (double * __W, __mmask8 __U, __m128d __A)
{
  __builtin_ia32_storesd128_mask ((__v2df *)__W, __A, __U & 1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_load_ss (__m128 __W, __mmask8 __U, const float* __A)
{
  __m128 src = (__v4sf) __builtin_shufflevector((__v4sf) __W,
                                                (__v4sf)_mm_setzero_ps(),
                                                0, 4, 4, 4);

  return (__m128) __builtin_ia32_loadss128_mask ((const __v4sf *) __A, src, __U & 1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_load_ss (__mmask8 __U, const float* __A)
{
  return (__m128)__builtin_ia32_loadss128_mask ((const __v4sf *) __A,
                                                (__v4sf) _mm_setzero_ps(),
                                                __U & 1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_load_sd (__m128d __W, __mmask8 __U, const double* __A)
{
  __m128d src = (__v2df) __builtin_shufflevector((__v2df) __W,
                                                 (__v2df)_mm_setzero_pd(),
                                                 0, 2);

  return (__m128d) __builtin_ia32_loadsd128_mask ((const __v2df *) __A, src, __U & 1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_load_sd (__mmask8 __U, const double* __A)
{
  return (__m128d) __builtin_ia32_loadsd128_mask ((const __v2df *) __A,
                                                  (__v2df) _mm_setzero_pd(),
                                                  __U & 1);
}
# 8802 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_expand_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_expanddf512_mask ((__v8df) __A,
                (__v8df) __W,
                (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_expand_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_expanddf512_mask ((__v8df) __A,
                (__v8df) _mm512_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_expand_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_expanddi512_mask ((__v8di) __A,
                (__v8di) __W,
                (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_expand_epi64 ( __mmask8 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_expanddi512_mask ((__v8di) __A,
                (__v8di) _mm512_setzero_si512 (),
                (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_expandloadu_pd(__m512d __W, __mmask8 __U, void const *__P)
{
  return (__m512d) __builtin_ia32_expandloaddf512_mask ((const __v8df *)__P,
              (__v8df) __W,
              (__mmask8) __U);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_expandloadu_pd(__mmask8 __U, void const *__P)
{
  return (__m512d) __builtin_ia32_expandloaddf512_mask ((const __v8df *)__P,
              (__v8df) _mm512_setzero_pd(),
              (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_expandloadu_epi64(__m512i __W, __mmask8 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_expandloaddi512_mask ((const __v8di *)__P,
              (__v8di) __W,
              (__mmask8) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_expandloadu_epi64(__mmask8 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_expandloaddi512_mask ((const __v8di *)__P,
              (__v8di) _mm512_setzero_si512(),
              (__mmask8) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_expandloadu_ps(__m512 __W, __mmask16 __U, void const *__P)
{
  return (__m512) __builtin_ia32_expandloadsf512_mask ((const __v16sf *)__P,
                   (__v16sf) __W,
                   (__mmask16) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_expandloadu_ps(__mmask16 __U, void const *__P)
{
  return (__m512) __builtin_ia32_expandloadsf512_mask ((const __v16sf *)__P,
                   (__v16sf) _mm512_setzero_ps(),
                   (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_expandloadu_epi32(__m512i __W, __mmask16 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_expandloadsi512_mask ((const __v16si *)__P,
              (__v16si) __W,
              (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_expandloadu_epi32(__mmask16 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_expandloadsi512_mask ((const __v16si *)__P,
              (__v16si) _mm512_setzero_si512(),
              (__mmask16) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_expand_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_expandsf512_mask ((__v16sf) __A,
               (__v16sf) __W,
               (__mmask16) __U);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_expand_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_expandsf512_mask ((__v16sf) __A,
               (__v16sf) _mm512_setzero_ps(),
               (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_expand_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_expandsi512_mask ((__v16si) __A,
                (__v16si) __W,
                (__mmask16) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_expand_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_expandsi512_mask ((__v16si) __A,
                (__v16si) _mm512_setzero_si512(),
                (__mmask16) __U);
}
# 8945 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtps_pd (__m256 __A)
{
  return (__m512d) __builtin_convertvector((__v8sf)__A, __v8df);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtps_pd (__m512d __W, __mmask8 __U, __m256 __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_cvtps_pd(__A),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_cvtps_pd (__mmask8 __U, __m256 __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_cvtps_pd(__A),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtpslo_pd (__m512 __A)
{
  return (__m512d) _mm512_cvtps_pd(_mm512_castps512_ps256(__A));
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_cvtpslo_pd (__m512d __W, __mmask8 __U, __m512 __A)
{
  return (__m512d) _mm512_mask_cvtps_pd(__W, __U, _mm512_castps512_ps256(__A));
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_mov_pd (__m512d __W, __mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_selectpd_512 ((__mmask8) __U,
              (__v8df) __A,
              (__v8df) __W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_mov_pd (__mmask8 __U, __m512d __A)
{
  return (__m512d) __builtin_ia32_selectpd_512 ((__mmask8) __U,
              (__v8df) __A,
              (__v8df) _mm512_setzero_pd ());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_mov_ps (__m512 __W, __mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_selectps_512 ((__mmask16) __U,
             (__v16sf) __A,
             (__v16sf) __W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_maskz_mov_ps (__mmask16 __U, __m512 __A)
{
  return (__m512) __builtin_ia32_selectps_512 ((__mmask16) __U,
             (__v16sf) __A,
             (__v16sf) _mm512_setzero_ps ());
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_compressstoreu_pd (void *__P, __mmask8 __U, __m512d __A)
{
  __builtin_ia32_compressstoredf512_mask ((__v8df *) __P, (__v8df) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_compressstoreu_epi64 (void *__P, __mmask8 __U, __m512i __A)
{
  __builtin_ia32_compressstoredi512_mask ((__v8di *) __P, (__v8di) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_compressstoreu_ps (void *__P, __mmask16 __U, __m512 __A)
{
  __builtin_ia32_compressstoresf512_mask ((__v16sf *) __P, (__v16sf) __A,
            (__mmask16) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_compressstoreu_epi32 (void *__P, __mmask16 __U, __m512i __A)
{
  __builtin_ia32_compressstoresi512_mask ((__v16si *) __P, (__v16si) __A,
            (__mmask16) __U);
}
# 9057 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_cvtsd_ss (__m128 __W, __mmask8 __U, __m128 __A, __m128d __B)
{
  return __builtin_ia32_cvtsd2ss_round_mask ((__v4sf)__A,
                                             (__v2df)__B,
                                             (__v4sf)__W,
                                             (__mmask8)__U, 0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_cvtsd_ss (__mmask8 __U, __m128 __A, __m128d __B)
{
  return __builtin_ia32_cvtsd2ss_round_mask ((__v4sf)__A,
                                             (__v2df)__B,
                                             (__v4sf)_mm_setzero_ps(),
                                             (__mmask8)__U, 0x04);
}
# 9130 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_mask_cvtss_sd (__m128d __W, __mmask8 __U, __m128d __A, __m128 __B)
{
  return __builtin_ia32_cvtss2sd_round_mask((__v2df)__A,
                                            (__v4sf)__B,
                                            (__v2df)__W,
                                            (__mmask8)__U, 0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_maskz_cvtss_sd (__mmask8 __U, __m128d __A, __m128 __B)
{
  return __builtin_ia32_cvtss2sd_round_mask((__v2df)__A,
                                            (__v4sf)__B,
                                            (__v2df)_mm_setzero_pd(),
                                            (__mmask8)__U, 0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_cvtu32_sd (__m128d __A, unsigned __B)
{
  __A[0] = __B;
  return __A;
}






static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_cvtu64_sd (__m128d __A, unsigned long long __B)
{
  __A[0] = __B;
  return __A;
}






static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_cvtu32_ss (__m128 __A, unsigned __B)
{
  __A[0] = __B;
  return __A;
}






static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(128)))
_mm_cvtu64_ss (__m128 __A, unsigned long long __B)
{
  __A[0] = __B;
  return __A;
}


static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_set1_epi32 (__m512i __O, __mmask16 __M, int __A)
{
  return (__m512i) __builtin_ia32_selectd_512(__M,
                                              (__v16si) _mm512_set1_epi32(__A),
                                              (__v16si) __O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_set1_epi64 (__m512i __O, __mmask8 __M, long long __A)
{
  return (__m512i) __builtin_ia32_selectq_512(__M,
                                              (__v8di) _mm512_set1_epi64(__A),
                                              (__v8di) __O);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_set_epi8 (char __e63, char __e62, char __e61, char __e60, char __e59,
    char __e58, char __e57, char __e56, char __e55, char __e54, char __e53,
    char __e52, char __e51, char __e50, char __e49, char __e48, char __e47,
    char __e46, char __e45, char __e44, char __e43, char __e42, char __e41,
    char __e40, char __e39, char __e38, char __e37, char __e36, char __e35,
    char __e34, char __e33, char __e32, char __e31, char __e30, char __e29,
    char __e28, char __e27, char __e26, char __e25, char __e24, char __e23,
    char __e22, char __e21, char __e20, char __e19, char __e18, char __e17,
    char __e16, char __e15, char __e14, char __e13, char __e12, char __e11,
    char __e10, char __e9, char __e8, char __e7, char __e6, char __e5,
    char __e4, char __e3, char __e2, char __e1, char __e0) {

  return __extension__ (__m512i)(__v64qi)
    {__e0, __e1, __e2, __e3, __e4, __e5, __e6, __e7,
     __e8, __e9, __e10, __e11, __e12, __e13, __e14, __e15,
     __e16, __e17, __e18, __e19, __e20, __e21, __e22, __e23,
     __e24, __e25, __e26, __e27, __e28, __e29, __e30, __e31,
     __e32, __e33, __e34, __e35, __e36, __e37, __e38, __e39,
     __e40, __e41, __e42, __e43, __e44, __e45, __e46, __e47,
     __e48, __e49, __e50, __e51, __e52, __e53, __e54, __e55,
     __e56, __e57, __e58, __e59, __e60, __e61, __e62, __e63};
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_set_epi16(short __e31, short __e30, short __e29, short __e28,
    short __e27, short __e26, short __e25, short __e24, short __e23,
    short __e22, short __e21, short __e20, short __e19, short __e18,
    short __e17, short __e16, short __e15, short __e14, short __e13,
    short __e12, short __e11, short __e10, short __e9, short __e8,
    short __e7, short __e6, short __e5, short __e4, short __e3,
    short __e2, short __e1, short __e0) {
  return __extension__ (__m512i)(__v32hi)
    {__e0, __e1, __e2, __e3, __e4, __e5, __e6, __e7,
     __e8, __e9, __e10, __e11, __e12, __e13, __e14, __e15,
     __e16, __e17, __e18, __e19, __e20, __e21, __e22, __e23,
     __e24, __e25, __e26, __e27, __e28, __e29, __e30, __e31 };
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_set_epi32 (int __A, int __B, int __C, int __D,
     int __E, int __F, int __G, int __H,
     int __I, int __J, int __K, int __L,
     int __M, int __N, int __O, int __P)
{
  return __extension__ (__m512i)(__v16si)
  { __P, __O, __N, __M, __L, __K, __J, __I,
    __H, __G, __F, __E, __D, __C, __B, __A };
}






static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_set_epi64 (long long __A, long long __B, long long __C,
     long long __D, long long __E, long long __F,
     long long __G, long long __H)
{
  return __extension__ (__m512i) (__v8di)
  { __H, __G, __F, __E, __D, __C, __B, __A };
}




static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_set_pd (double __A, double __B, double __C, double __D,
        double __E, double __F, double __G, double __H)
{
  return __extension__ (__m512d)
  { __H, __G, __F, __E, __D, __C, __B, __A };
}




static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_set_ps (float __A, float __B, float __C, float __D,
        float __E, float __F, float __G, float __H,
        float __I, float __J, float __K, float __L,
        float __M, float __N, float __O, float __P)
{
  return __extension__ (__m512)
  { __P, __O, __N, __M, __L, __K, __J, __I,
    __H, __G, __F, __E, __D, __C, __B, __A };
}





static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_abs_ps(__m512 __A)
{
  return (__m512)_mm512_and_epi32(_mm512_set1_epi32(0x7FFFFFFF),(__m512i)__A) ;
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_abs_ps(__m512 __W, __mmask16 __K, __m512 __A)
{
  return (__m512)_mm512_mask_and_epi32((__m512i)__W, __K, _mm512_set1_epi32(0x7FFFFFFF),(__m512i)__A) ;
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_abs_pd(__m512d __A)
{
  return (__m512d)_mm512_and_epi64(_mm512_set1_epi64(0x7FFFFFFFFFFFFFFF),(__v8di)__A) ;
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_abs_pd(__m512d __W, __mmask8 __K, __m512d __A)
{
  return (__m512d)_mm512_mask_and_epi64((__v8di)__W, __K, _mm512_set1_epi64(0x7FFFFFFFFFFFFFFF),(__v8di)__A);
}
# 9341 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512))) _mm512_reduce_add_epi64(__m512i __W) {
  return __builtin_reduce_add((__v8di)__W);
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512))) _mm512_reduce_mul_epi64(__m512i __W) {
  return __builtin_reduce_mul((__v8di)__W);
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512))) _mm512_reduce_and_epi64(__m512i __W) {
  return __builtin_reduce_and((__v8di)__W);
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512))) _mm512_reduce_or_epi64(__m512i __W) {
  return __builtin_reduce_or((__v8di)__W);
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_reduce_add_epi64(__mmask8 __M, __m512i __W) {
  __W = _mm512_maskz_mov_epi64(__M, __W);
  return __builtin_reduce_add((__v8di)__W);
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_reduce_mul_epi64(__mmask8 __M, __m512i __W) {
  __W = _mm512_mask_mov_epi64(_mm512_set1_epi64(1), __M, __W);
  return __builtin_reduce_mul((__v8di)__W);
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_reduce_and_epi64(__mmask8 __M, __m512i __W) {
  __W = _mm512_mask_mov_epi64(_mm512_set1_epi64(-1LL), __M, __W);
  return __builtin_reduce_and((__v8di)__W);
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_reduce_or_epi64(__mmask8 __M, __m512i __W) {
  __W = _mm512_maskz_mov_epi64(__M, __W);
  return __builtin_reduce_or((__v8di)__W);
}




static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512))) _mm512_reduce_add_pd(__m512d __W) {
  return __builtin_ia32_reduce_fadd_pd512(-0.0, __W);
}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512))) _mm512_reduce_mul_pd(__m512d __W) {
  return __builtin_ia32_reduce_fmul_pd512(1.0, __W);
}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_reduce_add_pd(__mmask8 __M, __m512d __W) {
  __W = _mm512_maskz_mov_pd(__M, __W);
  return __builtin_ia32_reduce_fadd_pd512(-0.0, __W);
}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_reduce_mul_pd(__mmask8 __M, __m512d __W) {
  __W = _mm512_mask_mov_pd(_mm512_set1_pd(1.0), __M, __W);
  return __builtin_ia32_reduce_fmul_pd512(1.0, __W);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_reduce_add_epi32(__m512i __W) {
  return __builtin_reduce_add((__v16si)__W);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_reduce_mul_epi32(__m512i __W) {
  return __builtin_reduce_mul((__v16si)__W);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_reduce_and_epi32(__m512i __W) {
  return __builtin_reduce_and((__v16si)__W);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_reduce_or_epi32(__m512i __W) {
  return __builtin_reduce_or((__v16si)__W);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_reduce_add_epi32( __mmask16 __M, __m512i __W) {
  __W = _mm512_maskz_mov_epi32(__M, __W);
  return __builtin_reduce_add((__v16si)__W);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_reduce_mul_epi32( __mmask16 __M, __m512i __W) {
  __W = _mm512_mask_mov_epi32(_mm512_set1_epi32(1), __M, __W);
  return __builtin_reduce_mul((__v16si)__W);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_reduce_and_epi32( __mmask16 __M, __m512i __W) {
  __W = _mm512_mask_mov_epi32(_mm512_set1_epi32(-1), __M, __W);
  return __builtin_reduce_and((__v16si)__W);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_reduce_or_epi32(__mmask16 __M, __m512i __W) {
  __W = _mm512_maskz_mov_epi32(__M, __W);
  return __builtin_reduce_or((__v16si)__W);
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_reduce_add_ps(__m512 __W) {
  return __builtin_ia32_reduce_fadd_ps512(-0.0f, __W);
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_reduce_mul_ps(__m512 __W) {
  return __builtin_ia32_reduce_fmul_ps512(1.0f, __W);
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_reduce_add_ps(__mmask16 __M, __m512 __W) {
  __W = _mm512_maskz_mov_ps(__M, __W);
  return __builtin_ia32_reduce_fadd_ps512(-0.0f, __W);
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_reduce_mul_ps(__mmask16 __M, __m512 __W) {
  __W = _mm512_mask_mov_ps(_mm512_set1_ps(1.0f), __M, __W);
  return __builtin_ia32_reduce_fmul_ps512(1.0f, __W);
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_reduce_max_epi64(__m512i __V) {
  return __builtin_reduce_max((__v8di)__V);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_reduce_max_epu64(__m512i __V) {
  return __builtin_reduce_max((__v8du)__V);
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_reduce_min_epi64(__m512i __V) {
  return __builtin_reduce_min((__v8di)__V);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_reduce_min_epu64(__m512i __V) {
  return __builtin_reduce_min((__v8du)__V);
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_reduce_max_epi64(__mmask8 __M, __m512i __V) {
  __V = _mm512_mask_mov_epi64(_mm512_set1_epi64(-9223372036854775807LL - 1LL), __M, __V);
  return __builtin_reduce_max((__v8di)__V);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_reduce_max_epu64(__mmask8 __M, __m512i __V) {
  __V = _mm512_maskz_mov_epi64(__M, __V);
  return __builtin_reduce_max((__v8du)__V);
}

static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_reduce_min_epi64(__mmask8 __M, __m512i __V) {
  __V = _mm512_mask_mov_epi64(_mm512_set1_epi64(9223372036854775807LL), __M, __V);
  return __builtin_reduce_min((__v8di)__V);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_reduce_min_epu64(__mmask8 __M, __m512i __V) {
  __V = _mm512_mask_mov_epi64(_mm512_set1_epi64(-1LL), __M, __V);
  return __builtin_reduce_min((__v8du)__V);
}
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_reduce_max_epi32(__m512i __V) {
  return __builtin_reduce_max((__v16si)__V);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_reduce_max_epu32(__m512i __V) {
  return __builtin_reduce_max((__v16su)__V);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_reduce_min_epi32(__m512i __V) {
  return __builtin_reduce_min((__v16si)__V);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_reduce_min_epu32(__m512i __V) {
  return __builtin_reduce_min((__v16su)__V);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_reduce_max_epi32(__mmask16 __M, __m512i __V) {
  __V = _mm512_mask_mov_epi32(_mm512_set1_epi32(-2147483647 - 1), __M, __V);
  return __builtin_reduce_max((__v16si)__V);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_reduce_max_epu32(__mmask16 __M, __m512i __V) {
  __V = _mm512_maskz_mov_epi32(__M, __V);
  return __builtin_reduce_max((__v16su)__V);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_reduce_min_epi32(__mmask16 __M, __m512i __V) {
  __V = _mm512_mask_mov_epi32(_mm512_set1_epi32(2147483647), __M, __V);
  return __builtin_reduce_min((__v16si)__V);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_reduce_min_epu32(__mmask16 __M, __m512i __V) {
  __V = _mm512_mask_mov_epi32(_mm512_set1_epi32(-1), __M, __V);
  return __builtin_reduce_min((__v16su)__V);
}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_reduce_max_pd(__m512d __V) {
  return __builtin_ia32_reduce_fmax_pd512(__V);
}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_reduce_min_pd(__m512d __V) {
  return __builtin_ia32_reduce_fmin_pd512(__V);
}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_reduce_max_pd(__mmask8 __M, __m512d __V) {
  __V = _mm512_mask_mov_pd(_mm512_set1_pd(-__builtin_inf()), __M, __V);
  return __builtin_ia32_reduce_fmax_pd512(__V);
}

static __inline__ double __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_reduce_min_pd(__mmask8 __M, __m512d __V) {
  __V = _mm512_mask_mov_pd(_mm512_set1_pd(__builtin_inf()), __M, __V);
  return __builtin_ia32_reduce_fmin_pd512(__V);
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_reduce_max_ps(__m512 __V) {
  return __builtin_ia32_reduce_fmax_ps512(__V);
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_reduce_min_ps(__m512 __V) {
  return __builtin_ia32_reduce_fmin_ps512(__V);
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_reduce_max_ps(__mmask16 __M, __m512 __V) {
  __V = _mm512_mask_mov_ps(_mm512_set1_ps(-__builtin_inff()), __M, __V);
  return __builtin_ia32_reduce_fmax_ps512(__V);
}

static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_reduce_min_ps(__mmask16 __M, __m512 __V) {
  __V = _mm512_mask_mov_ps(_mm512_set1_ps(__builtin_inff()), __M, __V);
  return __builtin_ia32_reduce_fmin_ps512(__V);
}
# 9612 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_cvtsi512_si32(__m512i __A) {
  __v16si __b = (__v16si)__A;
  return __b[0];
}
# 144 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlintrin.h" 1 3
# 33 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlintrin.h" 3
typedef short __v2hi __attribute__((__vector_size__(4)));
typedef char __v4qi __attribute__((__vector_size__(4)));
typedef char __v2qi __attribute__((__vector_size__(2)));
# 239 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_add_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_add_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_add_epi32(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_add_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_add_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_add_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_add_epi64(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_add_epi64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_sub_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_sub_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_sub_epi32(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_sub_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_sub_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_sub_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_sub_epi64(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_sub_epi64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_add_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_add_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_add_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_add_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_add_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_add_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_add_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_add_epi64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_sub_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_sub_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_sub_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_sub_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_sub_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_sub_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_sub_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_sub_epi64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_mul_epi32(__m256i __W, __mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_mul_epi32(__X, __Y),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_mul_epi32(__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_mul_epi32(__X, __Y),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_mul_epi32(__m128i __W, __mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_mul_epi32(__X, __Y),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_mul_epi32(__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_mul_epi32(__X, __Y),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_mul_epu32(__m256i __W, __mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_mul_epu32(__X, __Y),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_mul_epu32(__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_mul_epu32(__X, __Y),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_mul_epu32(__m128i __W, __mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_mul_epu32(__X, __Y),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_mul_epu32(__mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_mul_epu32(__X, __Y),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_mullo_epi32(__mmask8 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_mullo_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_mullo_epi32(__m256i __W, __mmask8 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_mullo_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_mullo_epi32(__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_mullo_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_mullo_epi32(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_mullo_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_and_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)((__v8su)__a & (__v8su)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_and_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_and_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_and_epi32(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)_mm256_mask_and_epi32(_mm256_setzero_si256(), __U, __A, __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_and_epi32(__m128i __a, __m128i __b)
{
  return (__m128i)((__v4su)__a & (__v4su)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_and_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_and_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_and_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)_mm_mask_and_epi32(_mm_setzero_si128(), __U, __A, __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_andnot_epi32(__m256i __A, __m256i __B)
{
  return (__m256i)(~(__v8su)__A & (__v8su)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_andnot_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                          (__v8si)_mm256_andnot_epi32(__A, __B),
                                          (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_andnot_epi32(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)_mm256_mask_andnot_epi32(_mm256_setzero_si256(),
                                           __U, __A, __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_andnot_epi32(__m128i __A, __m128i __B)
{
  return (__m128i)(~(__v4su)__A & (__v4su)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_andnot_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_andnot_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_andnot_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)_mm_mask_andnot_epi32(_mm_setzero_si128(), __U, __A, __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_or_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)((__v8su)__a | (__v8su)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_or_epi32 (__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_or_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_or_epi32(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)_mm256_mask_or_epi32(_mm256_setzero_si256(), __U, __A, __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_or_epi32(__m128i __a, __m128i __b)
{
  return (__m128i)((__v4su)__a | (__v4su)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_or_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_or_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_or_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)_mm_mask_or_epi32(_mm_setzero_si128(), __U, __A, __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_xor_epi32(__m256i __a, __m256i __b)
{
  return (__m256i)((__v8su)__a ^ (__v8su)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_xor_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_xor_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_xor_epi32(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)_mm256_mask_xor_epi32(_mm256_setzero_si256(), __U, __A, __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_xor_epi32(__m128i __a, __m128i __b)
{
  return (__m128i)((__v4su)__a ^ (__v4su)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_xor_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_xor_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_xor_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)_mm_mask_xor_epi32(_mm_setzero_si128(), __U, __A, __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_and_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4du)__a & (__v4du)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_and_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_and_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_and_epi64(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)_mm256_mask_and_epi64(_mm256_setzero_si256(), __U, __A, __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_and_epi64(__m128i __a, __m128i __b)
{
  return (__m128i)((__v2du)__a & (__v2du)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_and_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_and_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_and_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)_mm_mask_and_epi64(_mm_setzero_si128(), __U, __A, __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_andnot_epi64(__m256i __A, __m256i __B)
{
  return (__m256i)(~(__v4du)__A & (__v4du)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_andnot_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                          (__v4di)_mm256_andnot_epi64(__A, __B),
                                          (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_andnot_epi64(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)_mm256_mask_andnot_epi64(_mm256_setzero_si256(),
                                           __U, __A, __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_andnot_epi64(__m128i __A, __m128i __B)
{
  return (__m128i)(~(__v2du)__A & (__v2du)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_andnot_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_andnot_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_andnot_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)_mm_mask_andnot_epi64(_mm_setzero_si128(), __U, __A, __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_or_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4du)__a | (__v4du)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_or_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_or_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_or_epi64(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)_mm256_mask_or_epi64(_mm256_setzero_si256(), __U, __A, __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_or_epi64(__m128i __a, __m128i __b)
{
  return (__m128i)((__v2du)__a | (__v2du)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_or_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_or_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_or_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)_mm_mask_or_epi64(_mm_setzero_si128(), __U, __A, __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_xor_epi64(__m256i __a, __m256i __b)
{
  return (__m256i)((__v4du)__a ^ (__v4du)__b);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_xor_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_xor_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_xor_epi64(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)_mm256_mask_xor_epi64(_mm256_setzero_si256(), __U, __A, __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_xor_epi64(__m128i __a, __m128i __b)
{
  return (__m128i)((__v2du)__a ^ (__v2du)__b);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_xor_epi64(__m128i __W, __mmask8 __U, __m128i __A,
        __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_xor_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_xor_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)_mm_mask_xor_epi64(_mm_setzero_si128(), __U, __A, __B);
}
# 906 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_fmadd_pd(__m128d __A, __mmask8 __U, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddpd ((__v2df) __A,
                                             (__v2df) __B,
                                             (__v2df) __C),
                    (__v2df) __A);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask3_fmadd_pd(__m128d __A, __m128d __B, __m128d __C, __mmask8 __U)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddpd ((__v2df) __A,
                                             (__v2df) __B,
                                             (__v2df) __C),
                    (__v2df) __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_fmadd_pd(__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddpd ((__v2df) __A,
                                             (__v2df) __B,
                                             (__v2df) __C),
                    (__v2df)_mm_setzero_pd());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_fmsub_pd(__m128d __A, __mmask8 __U, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddpd ((__v2df) __A,
                                             (__v2df) __B,
                                             -(__v2df) __C),
                    (__v2df) __A);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_fmsub_pd(__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddpd ((__v2df) __A,
                                             (__v2df) __B,
                                             -(__v2df) __C),
                    (__v2df)_mm_setzero_pd());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask3_fnmadd_pd(__m128d __A, __m128d __B, __m128d __C, __mmask8 __U)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddpd (-(__v2df) __A,
                                             (__v2df) __B,
                                             (__v2df) __C),
                    (__v2df) __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_fnmadd_pd(__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddpd (-(__v2df) __A,
                                             (__v2df) __B,
                                             (__v2df) __C),
                    (__v2df)_mm_setzero_pd());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_fnmsub_pd(__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddpd (-(__v2df) __A,
                                             (__v2df) __B,
                                             -(__v2df) __C),
                    (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_fmadd_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddpd256 ((__v4df) __A,
                                                (__v4df) __B,
                                                (__v4df) __C),
                    (__v4df) __A);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask3_fmadd_pd(__m256d __A, __m256d __B, __m256d __C, __mmask8 __U)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddpd256 ((__v4df) __A,
                                                (__v4df) __B,
                                                (__v4df) __C),
                    (__v4df) __C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_fmadd_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddpd256 ((__v4df) __A,
                                                (__v4df) __B,
                                                (__v4df) __C),
                    (__v4df)_mm256_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_fmsub_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddpd256 ((__v4df) __A,
                                                (__v4df) __B,
                                                -(__v4df) __C),
                    (__v4df) __A);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_fmsub_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddpd256 ((__v4df) __A,
                                                (__v4df) __B,
                                                -(__v4df) __C),
                    (__v4df)_mm256_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask3_fnmadd_pd(__m256d __A, __m256d __B, __m256d __C, __mmask8 __U)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddpd256 (-(__v4df) __A,
                                                (__v4df) __B,
                                                (__v4df) __C),
                    (__v4df) __C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_fnmadd_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddpd256 (-(__v4df) __A,
                                                (__v4df) __B,
                                                (__v4df) __C),
                    (__v4df)_mm256_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_fnmsub_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddpd256 (-(__v4df) __A,
                                                (__v4df) __B,
                                                -(__v4df) __C),
                    (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_fmadd_ps(__m128 __A, __mmask8 __U, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddps ((__v4sf) __A,
                                             (__v4sf) __B,
                                             (__v4sf) __C),
                    (__v4sf) __A);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask3_fmadd_ps(__m128 __A, __m128 __B, __m128 __C, __mmask8 __U)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddps ((__v4sf) __A,
                                             (__v4sf) __B,
                                             (__v4sf) __C),
                    (__v4sf) __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_fmadd_ps(__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddps ((__v4sf) __A,
                                             (__v4sf) __B,
                                             (__v4sf) __C),
                    (__v4sf)_mm_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_fmsub_ps(__m128 __A, __mmask8 __U, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddps ((__v4sf) __A,
                                             (__v4sf) __B,
                                             -(__v4sf) __C),
                    (__v4sf) __A);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_fmsub_ps(__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddps ((__v4sf) __A,
                                             (__v4sf) __B,
                                             -(__v4sf) __C),
                    (__v4sf)_mm_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask3_fnmadd_ps(__m128 __A, __m128 __B, __m128 __C, __mmask8 __U)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddps (-(__v4sf) __A,
                                             (__v4sf) __B,
                                             (__v4sf) __C),
                    (__v4sf) __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_fnmadd_ps(__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddps (-(__v4sf) __A,
                                             (__v4sf) __B,
                                             (__v4sf) __C),
                    (__v4sf)_mm_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_fnmsub_ps(__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddps (-(__v4sf) __A,
                                             (__v4sf) __B,
                                             -(__v4sf) __C),
                    (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_fmadd_ps(__m256 __A, __mmask8 __U, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddps256 ((__v8sf) __A,
                                                (__v8sf) __B,
                                                (__v8sf) __C),
                    (__v8sf) __A);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask3_fmadd_ps(__m256 __A, __m256 __B, __m256 __C, __mmask8 __U)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddps256 ((__v8sf) __A,
                                                (__v8sf) __B,
                                                (__v8sf) __C),
                    (__v8sf) __C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_fmadd_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddps256 ((__v8sf) __A,
                                                (__v8sf) __B,
                                                (__v8sf) __C),
                    (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_fmsub_ps(__m256 __A, __mmask8 __U, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddps256 ((__v8sf) __A,
                                                (__v8sf) __B,
                                                -(__v8sf) __C),
                    (__v8sf) __A);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_fmsub_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddps256 ((__v8sf) __A,
                                                (__v8sf) __B,
                                                -(__v8sf) __C),
                    (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask3_fnmadd_ps(__m256 __A, __m256 __B, __m256 __C, __mmask8 __U)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddps256 (-(__v8sf) __A,
                                                (__v8sf) __B,
                                                (__v8sf) __C),
                    (__v8sf) __C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_fnmadd_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddps256 (-(__v8sf) __A,
                                                (__v8sf) __B,
                                                (__v8sf) __C),
                    (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_fnmsub_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddps256 (-(__v8sf) __A,
                                                (__v8sf) __B,
                                                -(__v8sf) __C),
                    (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_fmaddsub_pd(__m128d __A, __mmask8 __U, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddsubpd ((__v2df) __A,
                                                (__v2df) __B,
                                                (__v2df) __C),
                    (__v2df) __A);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask3_fmaddsub_pd(__m128d __A, __m128d __B, __m128d __C, __mmask8 __U)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddsubpd ((__v2df) __A,
                                                (__v2df) __B,
                                                (__v2df) __C),
                    (__v2df) __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_fmaddsub_pd(__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddsubpd ((__v2df) __A,
                                                (__v2df) __B,
                                                (__v2df) __C),
                    (__v2df)_mm_setzero_pd());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_fmsubadd_pd(__m128d __A, __mmask8 __U, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddsubpd ((__v2df) __A,
                                                (__v2df) __B,
                                                -(__v2df) __C),
                    (__v2df) __A);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_fmsubadd_pd(__mmask8 __U, __m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddsubpd ((__v2df) __A,
                                                (__v2df) __B,
                                                -(__v2df) __C),
                    (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_fmaddsub_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddsubpd256 ((__v4df) __A,
                                                   (__v4df) __B,
                                                   (__v4df) __C),
                    (__v4df) __A);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask3_fmaddsub_pd(__m256d __A, __m256d __B, __m256d __C, __mmask8 __U)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddsubpd256 ((__v4df) __A,
                                                   (__v4df) __B,
                                                   (__v4df) __C),
                    (__v4df) __C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_fmaddsub_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddsubpd256 ((__v4df) __A,
                                                   (__v4df) __B,
                                                   (__v4df) __C),
                    (__v4df)_mm256_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_fmsubadd_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddsubpd256 ((__v4df) __A,
                                                   (__v4df) __B,
                                                   -(__v4df) __C),
                    (__v4df) __A);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_fmsubadd_pd(__mmask8 __U, __m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddsubpd256 ((__v4df) __A,
                                                   (__v4df) __B,
                                                   -(__v4df) __C),
                    (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_fmaddsub_ps(__m128 __A, __mmask8 __U, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddsubps ((__v4sf) __A,
                                                (__v4sf) __B,
                                                (__v4sf) __C),
                    (__v4sf) __A);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask3_fmaddsub_ps(__m128 __A, __m128 __B, __m128 __C, __mmask8 __U)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddsubps ((__v4sf) __A,
                                                (__v4sf) __B,
                                                (__v4sf) __C),
                    (__v4sf) __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_fmaddsub_ps(__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddsubps ((__v4sf) __A,
                                                (__v4sf) __B,
                                                (__v4sf) __C),
                    (__v4sf)_mm_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_fmsubadd_ps(__m128 __A, __mmask8 __U, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddsubps ((__v4sf) __A,
                                                (__v4sf) __B,
                                                -(__v4sf) __C),
                    (__v4sf) __A);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_fmsubadd_ps(__mmask8 __U, __m128 __A, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddsubps ((__v4sf) __A,
                                                (__v4sf) __B,
                                                -(__v4sf) __C),
                    (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_fmaddsub_ps(__m256 __A, __mmask8 __U, __m256 __B,
                         __m256 __C)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddsubps256 ((__v8sf) __A,
                                                   (__v8sf) __B,
                                                   (__v8sf) __C),
                    (__v8sf) __A);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask3_fmaddsub_ps(__m256 __A, __m256 __B, __m256 __C, __mmask8 __U)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddsubps256 ((__v8sf) __A,
                                                   (__v8sf) __B,
                                                   (__v8sf) __C),
                    (__v8sf) __C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_fmaddsub_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddsubps256 ((__v8sf) __A,
                                                   (__v8sf) __B,
                                                   (__v8sf) __C),
                    (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_fmsubadd_ps(__m256 __A, __mmask8 __U, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddsubps256 ((__v8sf) __A,
                                                   (__v8sf) __B,
                                                   -(__v8sf) __C),
                    (__v8sf) __A);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_fmsubadd_ps(__mmask8 __U, __m256 __A, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddsubps256 ((__v8sf) __A,
                                                   (__v8sf) __B,
                                                   -(__v8sf) __C),
                    (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask3_fmsub_pd(__m128d __A, __m128d __B, __m128d __C, __mmask8 __U)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddpd ((__v2df) __A,
                                             (__v2df) __B,
                                             -(__v2df) __C),
                    (__v2df) __C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask3_fmsub_pd(__m256d __A, __m256d __B, __m256d __C, __mmask8 __U)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddpd256 ((__v4df) __A,
                                                (__v4df) __B,
                                                -(__v4df) __C),
                    (__v4df) __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask3_fmsub_ps(__m128 __A, __m128 __B, __m128 __C, __mmask8 __U)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddps ((__v4sf) __A,
                                             (__v4sf) __B,
                                             -(__v4sf) __C),
                    (__v4sf) __C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask3_fmsub_ps(__m256 __A, __m256 __B, __m256 __C, __mmask8 __U)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddps256 ((__v8sf) __A,
                                                (__v8sf) __B,
                                                -(__v8sf) __C),
                    (__v8sf) __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask3_fmsubadd_pd(__m128d __A, __m128d __B, __m128d __C, __mmask8 __U)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddsubpd ((__v2df) __A,
                                                (__v2df) __B,
                                                -(__v2df) __C),
                    (__v2df) __C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask3_fmsubadd_pd(__m256d __A, __m256d __B, __m256d __C, __mmask8 __U)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddsubpd256 ((__v4df) __A,
                                                   (__v4df) __B,
                                                   -(__v4df) __C),
                    (__v4df) __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask3_fmsubadd_ps(__m128 __A, __m128 __B, __m128 __C, __mmask8 __U)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddsubps ((__v4sf) __A,
                                                (__v4sf) __B,
                                                -(__v4sf) __C),
                    (__v4sf) __C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask3_fmsubadd_ps(__m256 __A, __m256 __B, __m256 __C, __mmask8 __U)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddsubps256 ((__v8sf) __A,
                                                   (__v8sf) __B,
                                                   -(__v8sf) __C),
                    (__v8sf) __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_fnmadd_pd(__m128d __A, __mmask8 __U, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddpd ((__v2df) __A,
                                             -(__v2df) __B,
                                             (__v2df) __C),
                    (__v2df) __A);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_fnmadd_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddpd256 ((__v4df) __A,
                                                -(__v4df) __B,
                                                (__v4df) __C),
                    (__v4df) __A);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_fnmadd_ps(__m128 __A, __mmask8 __U, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddps ((__v4sf) __A,
                                             -(__v4sf) __B,
                                             (__v4sf) __C),
                    (__v4sf) __A);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_fnmadd_ps(__m256 __A, __mmask8 __U, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddps256 ((__v8sf) __A,
                                                -(__v8sf) __B,
                                                (__v8sf) __C),
                    (__v8sf) __A);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_fnmsub_pd(__m128d __A, __mmask8 __U, __m128d __B, __m128d __C)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddpd ((__v2df) __A,
                                             -(__v2df) __B,
                                             -(__v2df) __C),
                    (__v2df) __A);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask3_fnmsub_pd(__m128d __A, __m128d __B, __m128d __C, __mmask8 __U)
{
  return (__m128d) __builtin_ia32_selectpd_128((__mmask8) __U,
                    __builtin_ia32_vfmaddpd ((__v2df) __A,
                                             -(__v2df) __B,
                                             -(__v2df) __C),
                    (__v2df) __C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_fnmsub_pd(__m256d __A, __mmask8 __U, __m256d __B, __m256d __C)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddpd256 ((__v4df) __A,
                                                -(__v4df) __B,
                                                -(__v4df) __C),
                    (__v4df) __A);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask3_fnmsub_pd(__m256d __A, __m256d __B, __m256d __C, __mmask8 __U)
{
  return (__m256d) __builtin_ia32_selectpd_256((__mmask8) __U,
                    __builtin_ia32_vfmaddpd256 ((__v4df) __A,
                                                -(__v4df) __B,
                                                -(__v4df) __C),
                    (__v4df) __C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_fnmsub_ps(__m128 __A, __mmask8 __U, __m128 __B, __m128 __C)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddps ((__v4sf) __A,
                                             -(__v4sf) __B,
                                             -(__v4sf) __C),
                    (__v4sf) __A);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask3_fnmsub_ps(__m128 __A, __m128 __B, __m128 __C, __mmask8 __U)
{
  return (__m128) __builtin_ia32_selectps_128((__mmask8) __U,
                    __builtin_ia32_vfmaddps ((__v4sf) __A,
                                             -(__v4sf) __B,
                                             -(__v4sf) __C),
                    (__v4sf) __C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_fnmsub_ps(__m256 __A, __mmask8 __U, __m256 __B, __m256 __C)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddps256 ((__v8sf) __A,
                                                -(__v8sf) __B,
                                                -(__v8sf) __C),
                    (__v8sf) __A);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask3_fnmsub_ps(__m256 __A, __m256 __B, __m256 __C, __mmask8 __U)
{
  return (__m256) __builtin_ia32_selectps_256((__mmask8) __U,
                    __builtin_ia32_vfmaddps256 ((__v8sf) __A,
                                                -(__v8sf) __B,
                                                -(__v8sf) __C),
                    (__v8sf) __C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_add_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_add_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_add_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_add_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_add_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_add_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_add_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_add_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_add_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_add_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_add_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_add_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_add_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_add_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_add_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_add_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_blend_epi32 (__mmask8 __U, __m128i __A, __m128i __W) {
  return (__m128i) __builtin_ia32_selectd_128 ((__mmask8) __U,
                (__v4si) __W,
                (__v4si) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_blend_epi32 (__mmask8 __U, __m256i __A, __m256i __W) {
  return (__m256i) __builtin_ia32_selectd_256 ((__mmask8) __U,
                (__v8si) __W,
                (__v8si) __A);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_blend_pd (__mmask8 __U, __m128d __A, __m128d __W) {
  return (__m128d) __builtin_ia32_selectpd_128 ((__mmask8) __U,
                 (__v2df) __W,
                 (__v2df) __A);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_blend_pd (__mmask8 __U, __m256d __A, __m256d __W) {
  return (__m256d) __builtin_ia32_selectpd_256 ((__mmask8) __U,
                 (__v4df) __W,
                 (__v4df) __A);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_blend_ps (__mmask8 __U, __m128 __A, __m128 __W) {
  return (__m128) __builtin_ia32_selectps_128 ((__mmask8) __U,
                (__v4sf) __W,
                (__v4sf) __A);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_blend_ps (__mmask8 __U, __m256 __A, __m256 __W) {
  return (__m256) __builtin_ia32_selectps_256 ((__mmask8) __U,
                (__v8sf) __W,
                (__v8sf) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_blend_epi64 (__mmask8 __U, __m128i __A, __m128i __W) {
  return (__m128i) __builtin_ia32_selectq_128 ((__mmask8) __U,
                (__v2di) __W,
                (__v2di) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_blend_epi64 (__mmask8 __U, __m256i __A, __m256i __W) {
  return (__m256i) __builtin_ia32_selectq_256 ((__mmask8) __U,
                (__v4di) __W,
                (__v4di) __A);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_compress_pd (__m128d __W, __mmask8 __U, __m128d __A) {
  return (__m128d) __builtin_ia32_compressdf128_mask ((__v2df) __A,
                  (__v2df) __W,
                  (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_compress_pd (__mmask8 __U, __m128d __A) {
  return (__m128d) __builtin_ia32_compressdf128_mask ((__v2df) __A,
                  (__v2df)
                  _mm_setzero_pd (),
                  (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_compress_pd (__m256d __W, __mmask8 __U, __m256d __A) {
  return (__m256d) __builtin_ia32_compressdf256_mask ((__v4df) __A,
                  (__v4df) __W,
                  (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_compress_pd (__mmask8 __U, __m256d __A) {
  return (__m256d) __builtin_ia32_compressdf256_mask ((__v4df) __A,
                  (__v4df)
                  _mm256_setzero_pd (),
                  (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_compress_epi64 (__m128i __W, __mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_compressdi128_mask ((__v2di) __A,
                  (__v2di) __W,
                  (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_compress_epi64 (__mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_compressdi128_mask ((__v2di) __A,
                  (__v2di)
                  _mm_setzero_si128 (),
                  (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_compress_epi64 (__m256i __W, __mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_compressdi256_mask ((__v4di) __A,
                  (__v4di) __W,
                  (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_compress_epi64 (__mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_compressdi256_mask ((__v4di) __A,
                  (__v4di)
                  _mm256_setzero_si256 (),
                  (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_compress_ps (__m128 __W, __mmask8 __U, __m128 __A) {
  return (__m128) __builtin_ia32_compresssf128_mask ((__v4sf) __A,
                 (__v4sf) __W,
                 (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_compress_ps (__mmask8 __U, __m128 __A) {
  return (__m128) __builtin_ia32_compresssf128_mask ((__v4sf) __A,
                 (__v4sf)
                 _mm_setzero_ps (),
                 (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_compress_ps (__m256 __W, __mmask8 __U, __m256 __A) {
  return (__m256) __builtin_ia32_compresssf256_mask ((__v8sf) __A,
                 (__v8sf) __W,
                 (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_compress_ps (__mmask8 __U, __m256 __A) {
  return (__m256) __builtin_ia32_compresssf256_mask ((__v8sf) __A,
                 (__v8sf)
                 _mm256_setzero_ps (),
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_compress_epi32 (__m128i __W, __mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_compresssi128_mask ((__v4si) __A,
                  (__v4si) __W,
                  (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_compress_epi32 (__mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_compresssi128_mask ((__v4si) __A,
                  (__v4si)
                  _mm_setzero_si128 (),
                  (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_compress_epi32 (__m256i __W, __mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_compresssi256_mask ((__v8si) __A,
                  (__v8si) __W,
                  (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_compress_epi32 (__mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_compresssi256_mask ((__v8si) __A,
                  (__v8si)
                  _mm256_setzero_si256 (),
                  (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_compressstoreu_pd (void *__P, __mmask8 __U, __m128d __A) {
  __builtin_ia32_compressstoredf128_mask ((__v2df *) __P,
            (__v2df) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_compressstoreu_pd (void *__P, __mmask8 __U, __m256d __A) {
  __builtin_ia32_compressstoredf256_mask ((__v4df *) __P,
            (__v4df) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_compressstoreu_epi64 (void *__P, __mmask8 __U, __m128i __A) {
  __builtin_ia32_compressstoredi128_mask ((__v2di *) __P,
            (__v2di) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_compressstoreu_epi64 (void *__P, __mmask8 __U, __m256i __A) {
  __builtin_ia32_compressstoredi256_mask ((__v4di *) __P,
            (__v4di) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_compressstoreu_ps (void *__P, __mmask8 __U, __m128 __A) {
  __builtin_ia32_compressstoresf128_mask ((__v4sf *) __P,
            (__v4sf) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_compressstoreu_ps (void *__P, __mmask8 __U, __m256 __A) {
  __builtin_ia32_compressstoresf256_mask ((__v8sf *) __P,
            (__v8sf) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_compressstoreu_epi32 (void *__P, __mmask8 __U, __m128i __A) {
  __builtin_ia32_compressstoresi128_mask ((__v4si *) __P,
            (__v4si) __A,
            (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_compressstoreu_epi32 (void *__P, __mmask8 __U, __m256i __A) {
  __builtin_ia32_compressstoresi256_mask ((__v8si *) __P,
            (__v8si) __A,
            (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtepi32_pd (__m128d __W, __mmask8 __U, __m128i __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8) __U,
                                              (__v2df)_mm_cvtepi32_pd(__A),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtepi32_pd (__mmask8 __U, __m128i __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8) __U,
                                              (__v2df)_mm_cvtepi32_pd(__A),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtepi32_pd (__m256d __W, __mmask8 __U, __m128i __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8) __U,
                                              (__v4df)_mm256_cvtepi32_pd(__A),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtepi32_pd (__mmask8 __U, __m128i __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8) __U,
                                              (__v4df)_mm256_cvtepi32_pd(__A),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtepi32_ps (__m128 __W, __mmask8 __U, __m128i __A) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_cvtepi32_ps(__A),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtepi32_ps (__mmask8 __U, __m128i __A) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_cvtepi32_ps(__A),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtepi32_ps (__m256 __W, __mmask8 __U, __m256i __A) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_cvtepi32_ps(__A),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtepi32_ps (__mmask8 __U, __m256i __A) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_cvtepi32_ps(__A),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtpd_epi32 (__m128i __W, __mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2dq128_mask ((__v2df) __A,
                (__v4si) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtpd_epi32 (__mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2dq128_mask ((__v2df) __A,
                (__v4si)
                _mm_setzero_si128 (),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtpd_epi32 (__m128i __W, __mmask8 __U, __m256d __A) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm256_cvtpd_epi32(__A),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtpd_epi32 (__mmask8 __U, __m256d __A) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm256_cvtpd_epi32(__A),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtpd_ps (__m128 __W, __mmask8 __U, __m128d __A) {
  return (__m128) __builtin_ia32_cvtpd2ps_mask ((__v2df) __A,
            (__v4sf) __W,
            (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtpd_ps (__mmask8 __U, __m128d __A) {
  return (__m128) __builtin_ia32_cvtpd2ps_mask ((__v2df) __A,
            (__v4sf)
            _mm_setzero_ps (),
            (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtpd_ps (__m128 __W, __mmask8 __U, __m256d __A) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm256_cvtpd_ps(__A),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtpd_ps (__mmask8 __U, __m256d __A) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm256_cvtpd_ps(__A),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_cvtpd_epu32 (__m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2udq128_mask ((__v2df) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtpd_epu32 (__m128i __W, __mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2udq128_mask ((__v2df) __A,
                 (__v4si) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtpd_epu32 (__mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2udq128_mask ((__v2df) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_cvtpd_epu32 (__m256d __A) {
  return (__m128i) __builtin_ia32_cvtpd2udq256_mask ((__v4df) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtpd_epu32 (__m128i __W, __mmask8 __U, __m256d __A) {
  return (__m128i) __builtin_ia32_cvtpd2udq256_mask ((__v4df) __A,
                 (__v4si) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtpd_epu32 (__mmask8 __U, __m256d __A) {
  return (__m128i) __builtin_ia32_cvtpd2udq256_mask ((__v4df) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtps_epi32 (__m128i __W, __mmask8 __U, __m128 __A) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_cvtps_epi32(__A),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtps_epi32 (__mmask8 __U, __m128 __A) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_cvtps_epi32(__A),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtps_epi32 (__m256i __W, __mmask8 __U, __m256 __A) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_cvtps_epi32(__A),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtps_epi32 (__mmask8 __U, __m256 __A) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_cvtps_epi32(__A),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtps_pd (__m128d __W, __mmask8 __U, __m128 __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_cvtps_pd(__A),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtps_pd (__mmask8 __U, __m128 __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_cvtps_pd(__A),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtps_pd (__m256d __W, __mmask8 __U, __m128 __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_cvtps_pd(__A),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtps_pd (__mmask8 __U, __m128 __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_cvtps_pd(__A),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_cvtps_epu32 (__m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2udq128_mask ((__v4sf) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtps_epu32 (__m128i __W, __mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2udq128_mask ((__v4sf) __A,
                 (__v4si) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtps_epu32 (__mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2udq128_mask ((__v4sf) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_cvtps_epu32 (__m256 __A) {
  return (__m256i) __builtin_ia32_cvtps2udq256_mask ((__v8sf) __A,
                 (__v8si)
                 _mm256_setzero_si256 (),
                 (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtps_epu32 (__m256i __W, __mmask8 __U, __m256 __A) {
  return (__m256i) __builtin_ia32_cvtps2udq256_mask ((__v8sf) __A,
                 (__v8si) __W,
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtps_epu32 (__mmask8 __U, __m256 __A) {
  return (__m256i) __builtin_ia32_cvtps2udq256_mask ((__v8sf) __A,
                 (__v8si)
                 _mm256_setzero_si256 (),
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvttpd_epi32 (__m128i __W, __mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2dq128_mask ((__v2df) __A,
                 (__v4si) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvttpd_epi32 (__mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2dq128_mask ((__v2df) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvttpd_epi32 (__m128i __W, __mmask8 __U, __m256d __A) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm256_cvttpd_epi32(__A),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvttpd_epi32 (__mmask8 __U, __m256d __A) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm256_cvttpd_epi32(__A),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_cvttpd_epu32 (__m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2udq128_mask ((__v2df) __A,
                  (__v4si)
                  _mm_setzero_si128 (),
                  (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvttpd_epu32 (__m128i __W, __mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2udq128_mask ((__v2df) __A,
                  (__v4si) __W,
                  (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvttpd_epu32 (__mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2udq128_mask ((__v2df) __A,
                  (__v4si)
                  _mm_setzero_si128 (),
                  (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_cvttpd_epu32 (__m256d __A) {
  return (__m128i) __builtin_ia32_cvttpd2udq256_mask ((__v4df) __A,
                  (__v4si)
                  _mm_setzero_si128 (),
                  (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvttpd_epu32 (__m128i __W, __mmask8 __U, __m256d __A) {
  return (__m128i) __builtin_ia32_cvttpd2udq256_mask ((__v4df) __A,
                  (__v4si) __W,
                  (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvttpd_epu32 (__mmask8 __U, __m256d __A) {
  return (__m128i) __builtin_ia32_cvttpd2udq256_mask ((__v4df) __A,
                  (__v4si)
                  _mm_setzero_si128 (),
                  (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvttps_epi32 (__m128i __W, __mmask8 __U, __m128 __A) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_cvttps_epi32(__A),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvttps_epi32 (__mmask8 __U, __m128 __A) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_cvttps_epi32(__A),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvttps_epi32 (__m256i __W, __mmask8 __U, __m256 __A) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_cvttps_epi32(__A),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvttps_epi32 (__mmask8 __U, __m256 __A) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_cvttps_epi32(__A),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_cvttps_epu32 (__m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2udq128_mask ((__v4sf) __A,
                  (__v4si)
                  _mm_setzero_si128 (),
                  (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvttps_epu32 (__m128i __W, __mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2udq128_mask ((__v4sf) __A,
                  (__v4si) __W,
                  (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvttps_epu32 (__mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2udq128_mask ((__v4sf) __A,
                  (__v4si)
                  _mm_setzero_si128 (),
                  (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_cvttps_epu32 (__m256 __A) {
  return (__m256i) __builtin_ia32_cvttps2udq256_mask ((__v8sf) __A,
                  (__v8si)
                  _mm256_setzero_si256 (),
                  (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvttps_epu32 (__m256i __W, __mmask8 __U, __m256 __A) {
  return (__m256i) __builtin_ia32_cvttps2udq256_mask ((__v8sf) __A,
                  (__v8si) __W,
                  (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvttps_epu32 (__mmask8 __U, __m256 __A) {
  return (__m256i) __builtin_ia32_cvttps2udq256_mask ((__v8sf) __A,
                  (__v8si)
                  _mm256_setzero_si256 (),
                  (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_cvtepu32_pd (__m128i __A) {
  return (__m128d) __builtin_convertvector(
      __builtin_shufflevector((__v4su)__A, (__v4su)__A, 0, 1), __v2df);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtepu32_pd (__m128d __W, __mmask8 __U, __m128i __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8) __U,
                                              (__v2df)_mm_cvtepu32_pd(__A),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtepu32_pd (__mmask8 __U, __m128i __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8) __U,
                                              (__v2df)_mm_cvtepu32_pd(__A),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_cvtepu32_pd (__m128i __A) {
  return (__m256d)__builtin_convertvector((__v4su)__A, __v4df);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtepu32_pd (__m256d __W, __mmask8 __U, __m128i __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8) __U,
                                              (__v4df)_mm256_cvtepu32_pd(__A),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtepu32_pd (__mmask8 __U, __m128i __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8) __U,
                                              (__v4df)_mm256_cvtepu32_pd(__A),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_cvtepu32_ps (__m128i __A) {
  return (__m128)__builtin_convertvector((__v4su)__A, __v4sf);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtepu32_ps (__m128 __W, __mmask8 __U, __m128i __A) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_cvtepu32_ps(__A),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtepu32_ps (__mmask8 __U, __m128i __A) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_cvtepu32_ps(__A),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_cvtepu32_ps (__m256i __A) {
  return (__m256)__builtin_convertvector((__v8su)__A, __v8sf);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtepu32_ps (__m256 __W, __mmask8 __U, __m256i __A) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_cvtepu32_ps(__A),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtepu32_ps (__mmask8 __U, __m256i __A) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_cvtepu32_ps(__A),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_div_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_div_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_div_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_div_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_div_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_div_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_div_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_div_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_div_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_div_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_div_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_div_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_div_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_div_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_div_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_div_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_expand_pd (__m128d __W, __mmask8 __U, __m128d __A) {
  return (__m128d) __builtin_ia32_expanddf128_mask ((__v2df) __A,
                (__v2df) __W,
                (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_expand_pd (__mmask8 __U, __m128d __A) {
  return (__m128d) __builtin_ia32_expanddf128_mask ((__v2df) __A,
                 (__v2df)
                 _mm_setzero_pd (),
                 (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_expand_pd (__m256d __W, __mmask8 __U, __m256d __A) {
  return (__m256d) __builtin_ia32_expanddf256_mask ((__v4df) __A,
                (__v4df) __W,
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_expand_pd (__mmask8 __U, __m256d __A) {
  return (__m256d) __builtin_ia32_expanddf256_mask ((__v4df) __A,
                 (__v4df)
                 _mm256_setzero_pd (),
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_expand_epi64 (__m128i __W, __mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_expanddi128_mask ((__v2di) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_expand_epi64 (__mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_expanddi128_mask ((__v2di) __A,
                 (__v2di)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_expand_epi64 (__m256i __W, __mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_expanddi256_mask ((__v4di) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_expand_epi64 (__mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_expanddi256_mask ((__v4di) __A,
                 (__v4di)
                 _mm256_setzero_si256 (),
                 (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_expandloadu_pd (__m128d __W, __mmask8 __U, void const *__P) {
  return (__m128d) __builtin_ia32_expandloaddf128_mask ((const __v2df *) __P,
              (__v2df) __W,
              (__mmask8)
              __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_expandloadu_pd (__mmask8 __U, void const *__P) {
  return (__m128d) __builtin_ia32_expandloaddf128_mask ((const __v2df *) __P,
               (__v2df)
               _mm_setzero_pd (),
               (__mmask8)
               __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_expandloadu_pd (__m256d __W, __mmask8 __U, void const *__P) {
  return (__m256d) __builtin_ia32_expandloaddf256_mask ((const __v4df *) __P,
              (__v4df) __W,
              (__mmask8)
              __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_expandloadu_pd (__mmask8 __U, void const *__P) {
  return (__m256d) __builtin_ia32_expandloaddf256_mask ((const __v4df *) __P,
               (__v4df)
               _mm256_setzero_pd (),
               (__mmask8)
               __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_expandloadu_epi64 (__m128i __W, __mmask8 __U, void const *__P) {
  return (__m128i) __builtin_ia32_expandloaddi128_mask ((const __v2di *) __P,
              (__v2di) __W,
              (__mmask8)
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_expandloadu_epi64 (__mmask8 __U, void const *__P) {
  return (__m128i) __builtin_ia32_expandloaddi128_mask ((const __v2di *) __P,
               (__v2di)
               _mm_setzero_si128 (),
               (__mmask8)
               __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_expandloadu_epi64 (__m256i __W, __mmask8 __U,
             void const *__P) {
  return (__m256i) __builtin_ia32_expandloaddi256_mask ((const __v4di *) __P,
              (__v4di) __W,
              (__mmask8)
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_expandloadu_epi64 (__mmask8 __U, void const *__P) {
  return (__m256i) __builtin_ia32_expandloaddi256_mask ((const __v4di *) __P,
               (__v4di)
               _mm256_setzero_si256 (),
               (__mmask8)
               __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_expandloadu_ps (__m128 __W, __mmask8 __U, void const *__P) {
  return (__m128) __builtin_ia32_expandloadsf128_mask ((const __v4sf *) __P,
                   (__v4sf) __W,
                   (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_expandloadu_ps (__mmask8 __U, void const *__P) {
  return (__m128) __builtin_ia32_expandloadsf128_mask ((const __v4sf *) __P,
              (__v4sf)
              _mm_setzero_ps (),
              (__mmask8)
              __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_expandloadu_ps (__m256 __W, __mmask8 __U, void const *__P) {
  return (__m256) __builtin_ia32_expandloadsf256_mask ((const __v8sf *) __P,
                   (__v8sf) __W,
                   (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_expandloadu_ps (__mmask8 __U, void const *__P) {
  return (__m256) __builtin_ia32_expandloadsf256_mask ((const __v8sf *) __P,
              (__v8sf)
              _mm256_setzero_ps (),
              (__mmask8)
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_expandloadu_epi32 (__m128i __W, __mmask8 __U, void const *__P) {
  return (__m128i) __builtin_ia32_expandloadsi128_mask ((const __v4si *) __P,
              (__v4si) __W,
              (__mmask8)
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_expandloadu_epi32 (__mmask8 __U, void const *__P) {
  return (__m128i) __builtin_ia32_expandloadsi128_mask ((const __v4si *) __P,
               (__v4si)
               _mm_setzero_si128 (),
               (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_expandloadu_epi32 (__m256i __W, __mmask8 __U,
             void const *__P) {
  return (__m256i) __builtin_ia32_expandloadsi256_mask ((const __v8si *) __P,
              (__v8si) __W,
              (__mmask8)
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_expandloadu_epi32 (__mmask8 __U, void const *__P) {
  return (__m256i) __builtin_ia32_expandloadsi256_mask ((const __v8si *) __P,
               (__v8si)
               _mm256_setzero_si256 (),
               (__mmask8)
               __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_expand_ps (__m128 __W, __mmask8 __U, __m128 __A) {
  return (__m128) __builtin_ia32_expandsf128_mask ((__v4sf) __A,
               (__v4sf) __W,
               (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_expand_ps (__mmask8 __U, __m128 __A) {
  return (__m128) __builtin_ia32_expandsf128_mask ((__v4sf) __A,
                (__v4sf)
                _mm_setzero_ps (),
                (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_expand_ps (__m256 __W, __mmask8 __U, __m256 __A) {
  return (__m256) __builtin_ia32_expandsf256_mask ((__v8sf) __A,
               (__v8sf) __W,
               (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_expand_ps (__mmask8 __U, __m256 __A) {
  return (__m256) __builtin_ia32_expandsf256_mask ((__v8sf) __A,
                (__v8sf)
                _mm256_setzero_ps (),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_expand_epi32 (__m128i __W, __mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_expandsi128_mask ((__v4si) __A,
                (__v4si) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_expand_epi32 (__mmask8 __U, __m128i __A) {
  return (__m128i) __builtin_ia32_expandsi128_mask ((__v4si) __A,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_expand_epi32 (__m256i __W, __mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_expandsi256_mask ((__v8si) __A,
                (__v8si) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_expand_epi32 (__mmask8 __U, __m256i __A) {
  return (__m256i) __builtin_ia32_expandsi256_mask ((__v8si) __A,
                 (__v8si)
                 _mm256_setzero_si256 (),
                 (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_getexp_pd (__m128d __A) {
  return (__m128d) __builtin_ia32_getexppd128_mask ((__v2df) __A,
                (__v2df)
                _mm_setzero_pd (),
                (__mmask8) -1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_getexp_pd (__m128d __W, __mmask8 __U, __m128d __A) {
  return (__m128d) __builtin_ia32_getexppd128_mask ((__v2df) __A,
                (__v2df) __W,
                (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_getexp_pd (__mmask8 __U, __m128d __A) {
  return (__m128d) __builtin_ia32_getexppd128_mask ((__v2df) __A,
                (__v2df)
                _mm_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_getexp_pd (__m256d __A) {
  return (__m256d) __builtin_ia32_getexppd256_mask ((__v4df) __A,
                (__v4df)
                _mm256_setzero_pd (),
                (__mmask8) -1);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_getexp_pd (__m256d __W, __mmask8 __U, __m256d __A) {
  return (__m256d) __builtin_ia32_getexppd256_mask ((__v4df) __A,
                (__v4df) __W,
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_getexp_pd (__mmask8 __U, __m256d __A) {
  return (__m256d) __builtin_ia32_getexppd256_mask ((__v4df) __A,
                (__v4df)
                _mm256_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_getexp_ps (__m128 __A) {
  return (__m128) __builtin_ia32_getexpps128_mask ((__v4sf) __A,
               (__v4sf)
               _mm_setzero_ps (),
               (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_getexp_ps (__m128 __W, __mmask8 __U, __m128 __A) {
  return (__m128) __builtin_ia32_getexpps128_mask ((__v4sf) __A,
               (__v4sf) __W,
               (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_getexp_ps (__mmask8 __U, __m128 __A) {
  return (__m128) __builtin_ia32_getexpps128_mask ((__v4sf) __A,
               (__v4sf)
               _mm_setzero_ps (),
               (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_getexp_ps (__m256 __A) {
  return (__m256) __builtin_ia32_getexpps256_mask ((__v8sf) __A,
               (__v8sf)
               _mm256_setzero_ps (),
               (__mmask8) -1);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_getexp_ps (__m256 __W, __mmask8 __U, __m256 __A) {
  return (__m256) __builtin_ia32_getexpps256_mask ((__v8sf) __A,
               (__v8sf) __W,
               (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_getexp_ps (__mmask8 __U, __m256 __A) {
  return (__m256) __builtin_ia32_getexpps256_mask ((__v8sf) __A,
               (__v8sf)
               _mm256_setzero_ps (),
               (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_max_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_max_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_max_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_max_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_max_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_max_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_max_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_max_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_max_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_max_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_max_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_max_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_max_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_max_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_max_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_max_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_min_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_min_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_min_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_min_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_min_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_min_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_min_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_min_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_min_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_min_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_min_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_min_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_min_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_min_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_min_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_min_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_mul_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_mul_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_mul_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_mul_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_mul_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_mul_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_mul_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_mul_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_mul_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_mul_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_mul_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_mul_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_mul_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_mul_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_mul_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_mul_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_abs_epi32(__m128i __W, __mmask8 __U, __m128i __A) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_abs_epi32(__A),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_abs_epi32(__mmask8 __U, __m128i __A) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_abs_epi32(__A),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_abs_epi32(__m256i __W, __mmask8 __U, __m256i __A) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_abs_epi32(__A),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_abs_epi32(__mmask8 __U, __m256i __A) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_abs_epi32(__A),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_abs_epi64 (__m128i __A) {
  return (__m128i)__builtin_elementwise_abs((__v2di)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_abs_epi64 (__m128i __W, __mmask8 __U, __m128i __A) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_abs_epi64(__A),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_abs_epi64 (__mmask8 __U, __m128i __A) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_abs_epi64(__A),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_abs_epi64 (__m256i __A) {
  return (__m256i)__builtin_elementwise_abs((__v4di)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_abs_epi64 (__m256i __W, __mmask8 __U, __m256i __A) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_abs_epi64(__A),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_abs_epi64 (__mmask8 __U, __m256i __A) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_abs_epi64(__A),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_max_epi32(__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_max_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_max_epi32(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_max_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_max_epi32(__mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_max_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_max_epi32(__m256i __W, __mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_max_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_max_epi64 (__m128i __A, __m128i __B) {
  return (__m128i)__builtin_elementwise_max((__v2di)__A, (__v2di)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_max_epi64 (__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_max_epi64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_max_epi64 (__m128i __W, __mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_max_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_max_epi64 (__m256i __A, __m256i __B) {
  return (__m256i)__builtin_elementwise_max((__v4di)__A, (__v4di)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_max_epi64 (__mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_max_epi64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_max_epi64 (__m256i __W, __mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_max_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_max_epu32(__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_max_epu32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_max_epu32(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_max_epu32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_max_epu32(__mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_max_epu32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_max_epu32(__m256i __W, __mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_max_epu32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_max_epu64 (__m128i __A, __m128i __B) {
  return (__m128i)__builtin_elementwise_max((__v2du)__A, (__v2du)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_max_epu64 (__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_max_epu64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_max_epu64 (__m128i __W, __mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_max_epu64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_max_epu64 (__m256i __A, __m256i __B) {
  return (__m256i)__builtin_elementwise_max((__v4du)__A, (__v4du)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_max_epu64 (__mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_max_epu64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_max_epu64 (__m256i __W, __mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_max_epu64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_min_epi32(__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_min_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_min_epi32(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_min_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_min_epi32(__mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_min_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_min_epi32(__m256i __W, __mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_min_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_min_epi64 (__m128i __A, __m128i __B) {
  return (__m128i)__builtin_elementwise_min((__v2di)__A, (__v2di)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_min_epi64 (__m128i __W, __mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_min_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_min_epi64 (__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_min_epi64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_min_epi64 (__m256i __A, __m256i __B) {
  return (__m256i)__builtin_elementwise_min((__v4di)__A, (__v4di)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_min_epi64 (__m256i __W, __mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_min_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_min_epi64 (__mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_min_epi64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_min_epu32(__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_min_epu32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_min_epu32(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_min_epu32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_min_epu32(__mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_min_epu32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_min_epu32(__m256i __W, __mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_min_epu32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_min_epu64 (__m128i __A, __m128i __B) {
  return (__m128i)__builtin_elementwise_min((__v2du)__A, (__v2du)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_min_epu64 (__m128i __W, __mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_min_epu64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_min_epu64 (__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__M,
                                             (__v2di)_mm_min_epu64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_min_epu64 (__m256i __A, __m256i __B) {
  return (__m256i)__builtin_elementwise_min((__v4du)__A, (__v4du)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_min_epu64 (__m256i __W, __mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_min_epu64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_min_epu64 (__mmask8 __M, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                             (__v4di)_mm256_min_epu64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}
# 3378 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_scalef_pd (__m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_scalefpd128_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df)
                _mm_setzero_pd (),
                (__mmask8) -1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_scalef_pd (__m128d __W, __mmask8 __U, __m128d __A,
        __m128d __B) {
  return (__m128d) __builtin_ia32_scalefpd128_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df) __W,
                (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_scalef_pd (__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d) __builtin_ia32_scalefpd128_mask ((__v2df) __A,
                (__v2df) __B,
                (__v2df)
                _mm_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_scalef_pd (__m256d __A, __m256d __B) {
  return (__m256d) __builtin_ia32_scalefpd256_mask ((__v4df) __A,
                (__v4df) __B,
                (__v4df)
                _mm256_setzero_pd (),
                (__mmask8) -1);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_scalef_pd (__m256d __W, __mmask8 __U, __m256d __A,
           __m256d __B) {
  return (__m256d) __builtin_ia32_scalefpd256_mask ((__v4df) __A,
                (__v4df) __B,
                (__v4df) __W,
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_scalef_pd (__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d) __builtin_ia32_scalefpd256_mask ((__v4df) __A,
                (__v4df) __B,
                (__v4df)
                _mm256_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_scalef_ps (__m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_scalefps128_mask ((__v4sf) __A,
               (__v4sf) __B,
               (__v4sf)
               _mm_setzero_ps (),
               (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_scalef_ps (__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_scalefps128_mask ((__v4sf) __A,
               (__v4sf) __B,
               (__v4sf) __W,
               (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_scalef_ps (__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128) __builtin_ia32_scalefps128_mask ((__v4sf) __A,
               (__v4sf) __B,
               (__v4sf)
               _mm_setzero_ps (),
               (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_scalef_ps (__m256 __A, __m256 __B) {
  return (__m256) __builtin_ia32_scalefps256_mask ((__v8sf) __A,
               (__v8sf) __B,
               (__v8sf)
               _mm256_setzero_ps (),
               (__mmask8) -1);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_scalef_ps (__m256 __W, __mmask8 __U, __m256 __A,
           __m256 __B) {
  return (__m256) __builtin_ia32_scalefps256_mask ((__v8sf) __A,
               (__v8sf) __B,
               (__v8sf) __W,
               (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_scalef_ps (__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256) __builtin_ia32_scalefps256_mask ((__v8sf) __A,
               (__v8sf) __B,
               (__v8sf)
               _mm256_setzero_ps (),
               (__mmask8) __U);
}
# 3645 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlintrin.h" 3
  static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_mask_sqrt_pd(__m128d __W, __mmask8 __U, __m128d __A) {
    return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                                (__v2df)_mm_sqrt_pd(__A),
                                                (__v2df)__W);
  }

  static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_maskz_sqrt_pd(__mmask8 __U, __m128d __A) {
    return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                                (__v2df)_mm_sqrt_pd(__A),
                                                (__v2df)_mm_setzero_pd());
  }

  static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_mask_sqrt_pd(__m256d __W, __mmask8 __U, __m256d __A) {
    return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                                (__v4df)_mm256_sqrt_pd(__A),
                                                (__v4df)__W);
  }

  static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_maskz_sqrt_pd(__mmask8 __U, __m256d __A) {
    return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                                (__v4df)_mm256_sqrt_pd(__A),
                                                (__v4df)_mm256_setzero_pd());
  }

  static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_mask_sqrt_ps(__m128 __W, __mmask8 __U, __m128 __A) {
    return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                               (__v4sf)_mm_sqrt_ps(__A),
                                               (__v4sf)__W);
  }

  static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_maskz_sqrt_ps(__mmask8 __U, __m128 __A) {
    return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                               (__v4sf)_mm_sqrt_ps(__A),
                                               (__v4sf)_mm_setzero_ps());
  }

  static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_mask_sqrt_ps(__m256 __W, __mmask8 __U, __m256 __A) {
    return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                               (__v8sf)_mm256_sqrt_ps(__A),
                                               (__v8sf)__W);
  }

  static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_maskz_sqrt_ps(__mmask8 __U, __m256 __A) {
    return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                               (__v8sf)_mm256_sqrt_ps(__A),
                                               (__v8sf)_mm256_setzero_ps());
  }

  static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_mask_sub_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
    return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                                (__v2df)_mm_sub_pd(__A, __B),
                                                (__v2df)__W);
  }

  static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_maskz_sub_pd(__mmask8 __U, __m128d __A, __m128d __B) {
    return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                                (__v2df)_mm_sub_pd(__A, __B),
                                                (__v2df)_mm_setzero_pd());
  }

  static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_mask_sub_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
    return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                                (__v4df)_mm256_sub_pd(__A, __B),
                                                (__v4df)__W);
  }

  static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_maskz_sub_pd(__mmask8 __U, __m256d __A, __m256d __B) {
    return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                                (__v4df)_mm256_sub_pd(__A, __B),
                                                (__v4df)_mm256_setzero_pd());
  }

  static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_mask_sub_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
    return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                               (__v4sf)_mm_sub_ps(__A, __B),
                                               (__v4sf)__W);
  }

  static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_maskz_sub_ps(__mmask8 __U, __m128 __A, __m128 __B) {
    return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                               (__v4sf)_mm_sub_ps(__A, __B),
                                               (__v4sf)_mm_setzero_ps());
  }

  static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_mask_sub_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
    return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                               (__v8sf)_mm256_sub_ps(__A, __B),
                                               (__v8sf)__W);
  }

  static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_maskz_sub_ps(__mmask8 __U, __m256 __A, __m256 __B) {
    return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                               (__v8sf)_mm256_sub_ps(__A, __B),
                                               (__v8sf)_mm256_setzero_ps());
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_permutex2var_epi32(__m128i __A, __m128i __I, __m128i __B) {
    return (__m128i)__builtin_ia32_vpermi2vard128((__v4si) __A, (__v4si)__I,
                                                  (__v4si)__B);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_mask_permutex2var_epi32(__m128i __A, __mmask8 __U, __m128i __I,
                              __m128i __B) {
    return (__m128i)__builtin_ia32_selectd_128(__U,
                                    (__v4si)_mm_permutex2var_epi32(__A, __I, __B),
                                    (__v4si)__A);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_mask2_permutex2var_epi32(__m128i __A, __m128i __I, __mmask8 __U,
                               __m128i __B) {
    return (__m128i)__builtin_ia32_selectd_128(__U,
                                    (__v4si)_mm_permutex2var_epi32(__A, __I, __B),
                                    (__v4si)__I);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_maskz_permutex2var_epi32(__mmask8 __U, __m128i __A, __m128i __I,
                               __m128i __B) {
    return (__m128i)__builtin_ia32_selectd_128(__U,
                                    (__v4si)_mm_permutex2var_epi32(__A, __I, __B),
                                    (__v4si)_mm_setzero_si128());
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_permutex2var_epi32(__m256i __A, __m256i __I, __m256i __B) {
    return (__m256i)__builtin_ia32_vpermi2vard256((__v8si)__A, (__v8si) __I,
                                                  (__v8si) __B);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_mask_permutex2var_epi32(__m256i __A, __mmask8 __U, __m256i __I,
                                 __m256i __B) {
    return (__m256i)__builtin_ia32_selectd_256(__U,
                                 (__v8si)_mm256_permutex2var_epi32(__A, __I, __B),
                                 (__v8si)__A);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_mask2_permutex2var_epi32(__m256i __A, __m256i __I, __mmask8 __U,
                                  __m256i __B) {
    return (__m256i)__builtin_ia32_selectd_256(__U,
                                 (__v8si)_mm256_permutex2var_epi32(__A, __I, __B),
                                 (__v8si)__I);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_maskz_permutex2var_epi32(__mmask8 __U, __m256i __A, __m256i __I,
                                  __m256i __B) {
    return (__m256i)__builtin_ia32_selectd_256(__U,
                                 (__v8si)_mm256_permutex2var_epi32(__A, __I, __B),
                                 (__v8si)_mm256_setzero_si256());
  }

  static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_permutex2var_pd(__m128d __A, __m128i __I, __m128d __B) {
    return (__m128d)__builtin_ia32_vpermi2varpd128((__v2df)__A, (__v2di)__I,
                                                   (__v2df)__B);
  }

  static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_mask_permutex2var_pd(__m128d __A, __mmask8 __U, __m128i __I, __m128d __B) {
    return (__m128d)__builtin_ia32_selectpd_128(__U,
                                       (__v2df)_mm_permutex2var_pd(__A, __I, __B),
                                       (__v2df)__A);
  }

  static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_mask2_permutex2var_pd(__m128d __A, __m128i __I, __mmask8 __U, __m128d __B) {
    return (__m128d)__builtin_ia32_selectpd_128(__U,
                                       (__v2df)_mm_permutex2var_pd(__A, __I, __B),
                                       (__v2df)(__m128d)__I);
  }

  static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_maskz_permutex2var_pd(__mmask8 __U, __m128d __A, __m128i __I, __m128d __B) {
    return (__m128d)__builtin_ia32_selectpd_128(__U,
                                       (__v2df)_mm_permutex2var_pd(__A, __I, __B),
                                       (__v2df)_mm_setzero_pd());
  }

  static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_permutex2var_pd(__m256d __A, __m256i __I, __m256d __B) {
    return (__m256d)__builtin_ia32_vpermi2varpd256((__v4df)__A, (__v4di)__I,
                                                   (__v4df)__B);
  }

  static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_mask_permutex2var_pd(__m256d __A, __mmask8 __U, __m256i __I,
                              __m256d __B) {
    return (__m256d)__builtin_ia32_selectpd_256(__U,
                                    (__v4df)_mm256_permutex2var_pd(__A, __I, __B),
                                    (__v4df)__A);
  }

  static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_mask2_permutex2var_pd(__m256d __A, __m256i __I, __mmask8 __U,
                               __m256d __B) {
    return (__m256d)__builtin_ia32_selectpd_256(__U,
                                    (__v4df)_mm256_permutex2var_pd(__A, __I, __B),
                                    (__v4df)(__m256d)__I);
  }

  static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_maskz_permutex2var_pd(__mmask8 __U, __m256d __A, __m256i __I,
                               __m256d __B) {
    return (__m256d)__builtin_ia32_selectpd_256(__U,
                                    (__v4df)_mm256_permutex2var_pd(__A, __I, __B),
                                    (__v4df)_mm256_setzero_pd());
  }

  static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_permutex2var_ps(__m128 __A, __m128i __I, __m128 __B) {
    return (__m128)__builtin_ia32_vpermi2varps128((__v4sf)__A, (__v4si)__I,
                                                  (__v4sf)__B);
  }

  static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_mask_permutex2var_ps(__m128 __A, __mmask8 __U, __m128i __I, __m128 __B) {
    return (__m128)__builtin_ia32_selectps_128(__U,
                                       (__v4sf)_mm_permutex2var_ps(__A, __I, __B),
                                       (__v4sf)__A);
  }

  static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_mask2_permutex2var_ps(__m128 __A, __m128i __I, __mmask8 __U, __m128 __B) {
    return (__m128)__builtin_ia32_selectps_128(__U,
                                       (__v4sf)_mm_permutex2var_ps(__A, __I, __B),
                                       (__v4sf)(__m128)__I);
  }

  static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_maskz_permutex2var_ps(__mmask8 __U, __m128 __A, __m128i __I, __m128 __B) {
    return (__m128)__builtin_ia32_selectps_128(__U,
                                       (__v4sf)_mm_permutex2var_ps(__A, __I, __B),
                                       (__v4sf)_mm_setzero_ps());
  }

  static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_permutex2var_ps(__m256 __A, __m256i __I, __m256 __B) {
    return (__m256)__builtin_ia32_vpermi2varps256((__v8sf)__A, (__v8si)__I,
                                                  (__v8sf) __B);
  }

  static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_mask_permutex2var_ps(__m256 __A, __mmask8 __U, __m256i __I, __m256 __B) {
    return (__m256)__builtin_ia32_selectps_256(__U,
                                    (__v8sf)_mm256_permutex2var_ps(__A, __I, __B),
                                    (__v8sf)__A);
  }

  static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_mask2_permutex2var_ps(__m256 __A, __m256i __I, __mmask8 __U,
                               __m256 __B) {
    return (__m256)__builtin_ia32_selectps_256(__U,
                                    (__v8sf)_mm256_permutex2var_ps(__A, __I, __B),
                                    (__v8sf)(__m256)__I);
  }

  static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_maskz_permutex2var_ps(__mmask8 __U, __m256 __A, __m256i __I,
                               __m256 __B) {
    return (__m256)__builtin_ia32_selectps_256(__U,
                                    (__v8sf)_mm256_permutex2var_ps(__A, __I, __B),
                                    (__v8sf)_mm256_setzero_ps());
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_permutex2var_epi64(__m128i __A, __m128i __I, __m128i __B) {
    return (__m128i)__builtin_ia32_vpermi2varq128((__v2di)__A, (__v2di)__I,
                                                  (__v2di)__B);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_mask_permutex2var_epi64(__m128i __A, __mmask8 __U, __m128i __I,
                              __m128i __B) {
    return (__m128i)__builtin_ia32_selectq_128(__U,
                                    (__v2di)_mm_permutex2var_epi64(__A, __I, __B),
                                    (__v2di)__A);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_mask2_permutex2var_epi64(__m128i __A, __m128i __I, __mmask8 __U,
                               __m128i __B) {
    return (__m128i)__builtin_ia32_selectq_128(__U,
                                    (__v2di)_mm_permutex2var_epi64(__A, __I, __B),
                                    (__v2di)__I);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_maskz_permutex2var_epi64(__mmask8 __U, __m128i __A, __m128i __I,
                               __m128i __B) {
    return (__m128i)__builtin_ia32_selectq_128(__U,
                                    (__v2di)_mm_permutex2var_epi64(__A, __I, __B),
                                    (__v2di)_mm_setzero_si128());
  }


  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_permutex2var_epi64(__m256i __A, __m256i __I, __m256i __B) {
    return (__m256i)__builtin_ia32_vpermi2varq256((__v4di)__A, (__v4di) __I,
                                                  (__v4di) __B);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_mask_permutex2var_epi64(__m256i __A, __mmask8 __U, __m256i __I,
                                 __m256i __B) {
    return (__m256i)__builtin_ia32_selectq_256(__U,
                                 (__v4di)_mm256_permutex2var_epi64(__A, __I, __B),
                                 (__v4di)__A);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_mask2_permutex2var_epi64(__m256i __A, __m256i __I, __mmask8 __U,
                                  __m256i __B) {
    return (__m256i)__builtin_ia32_selectq_256(__U,
                                 (__v4di)_mm256_permutex2var_epi64(__A, __I, __B),
                                 (__v4di)__I);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_maskz_permutex2var_epi64(__mmask8 __U, __m256i __A, __m256i __I,
                                  __m256i __B) {
    return (__m256i)__builtin_ia32_selectq_256(__U,
                                 (__v4di)_mm256_permutex2var_epi64(__A, __I, __B),
                                 (__v4di)_mm256_setzero_si256());
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_mask_cvtepi8_epi32(__m128i __W, __mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                               (__v4si)_mm_cvtepi8_epi32(__A),
                                               (__v4si)__W);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_maskz_cvtepi8_epi32(__mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                               (__v4si)_mm_cvtepi8_epi32(__A),
                                               (__v4si)_mm_setzero_si128());
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_mask_cvtepi8_epi32 (__m256i __W, __mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                               (__v8si)_mm256_cvtepi8_epi32(__A),
                                               (__v8si)__W);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_maskz_cvtepi8_epi32 (__mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                               (__v8si)_mm256_cvtepi8_epi32(__A),
                                               (__v8si)_mm256_setzero_si256());
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_mask_cvtepi8_epi64(__m128i __W, __mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                               (__v2di)_mm_cvtepi8_epi64(__A),
                                               (__v2di)__W);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_maskz_cvtepi8_epi64(__mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                               (__v2di)_mm_cvtepi8_epi64(__A),
                                               (__v2di)_mm_setzero_si128());
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_mask_cvtepi8_epi64(__m256i __W, __mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                               (__v4di)_mm256_cvtepi8_epi64(__A),
                                               (__v4di)__W);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_maskz_cvtepi8_epi64(__mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                               (__v4di)_mm256_cvtepi8_epi64(__A),
                                               (__v4di)_mm256_setzero_si256());
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_mask_cvtepi32_epi64(__m128i __W, __mmask8 __U, __m128i __X)
  {
    return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                               (__v2di)_mm_cvtepi32_epi64(__X),
                                               (__v2di)__W);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_maskz_cvtepi32_epi64(__mmask8 __U, __m128i __X)
  {
    return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                               (__v2di)_mm_cvtepi32_epi64(__X),
                                               (__v2di)_mm_setzero_si128());
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_mask_cvtepi32_epi64(__m256i __W, __mmask8 __U, __m128i __X)
  {
    return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                               (__v4di)_mm256_cvtepi32_epi64(__X),
                                               (__v4di)__W);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_maskz_cvtepi32_epi64(__mmask8 __U, __m128i __X)
  {
    return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                               (__v4di)_mm256_cvtepi32_epi64(__X),
                                               (__v4di)_mm256_setzero_si256());
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_mask_cvtepi16_epi32(__m128i __W, __mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                               (__v4si)_mm_cvtepi16_epi32(__A),
                                               (__v4si)__W);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_maskz_cvtepi16_epi32(__mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                               (__v4si)_mm_cvtepi16_epi32(__A),
                                               (__v4si)_mm_setzero_si128());
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_mask_cvtepi16_epi32(__m256i __W, __mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                               (__v8si)_mm256_cvtepi16_epi32(__A),
                                               (__v8si)__W);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_maskz_cvtepi16_epi32 (__mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                               (__v8si)_mm256_cvtepi16_epi32(__A),
                                               (__v8si)_mm256_setzero_si256());
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_mask_cvtepi16_epi64(__m128i __W, __mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                               (__v2di)_mm_cvtepi16_epi64(__A),
                                               (__v2di)__W);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_maskz_cvtepi16_epi64(__mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                               (__v2di)_mm_cvtepi16_epi64(__A),
                                               (__v2di)_mm_setzero_si128());
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_mask_cvtepi16_epi64(__m256i __W, __mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                               (__v4di)_mm256_cvtepi16_epi64(__A),
                                               (__v4di)__W);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_maskz_cvtepi16_epi64(__mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                               (__v4di)_mm256_cvtepi16_epi64(__A),
                                               (__v4di)_mm256_setzero_si256());
  }


  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_mask_cvtepu8_epi32(__m128i __W, __mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                               (__v4si)_mm_cvtepu8_epi32(__A),
                                               (__v4si)__W);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_maskz_cvtepu8_epi32(__mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                               (__v4si)_mm_cvtepu8_epi32(__A),
                                               (__v4si)_mm_setzero_si128());
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_mask_cvtepu8_epi32(__m256i __W, __mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                               (__v8si)_mm256_cvtepu8_epi32(__A),
                                               (__v8si)__W);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_maskz_cvtepu8_epi32(__mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                               (__v8si)_mm256_cvtepu8_epi32(__A),
                                               (__v8si)_mm256_setzero_si256());
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_mask_cvtepu8_epi64(__m128i __W, __mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                               (__v2di)_mm_cvtepu8_epi64(__A),
                                               (__v2di)__W);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_maskz_cvtepu8_epi64(__mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                               (__v2di)_mm_cvtepu8_epi64(__A),
                                               (__v2di)_mm_setzero_si128());
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_mask_cvtepu8_epi64(__m256i __W, __mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                               (__v4di)_mm256_cvtepu8_epi64(__A),
                                               (__v4di)__W);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_maskz_cvtepu8_epi64 (__mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                               (__v4di)_mm256_cvtepu8_epi64(__A),
                                               (__v4di)_mm256_setzero_si256());
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_mask_cvtepu32_epi64(__m128i __W, __mmask8 __U, __m128i __X)
  {
    return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                               (__v2di)_mm_cvtepu32_epi64(__X),
                                               (__v2di)__W);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_maskz_cvtepu32_epi64(__mmask8 __U, __m128i __X)
  {
    return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                               (__v2di)_mm_cvtepu32_epi64(__X),
                                               (__v2di)_mm_setzero_si128());
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_mask_cvtepu32_epi64(__m256i __W, __mmask8 __U, __m128i __X)
  {
    return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                               (__v4di)_mm256_cvtepu32_epi64(__X),
                                               (__v4di)__W);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_maskz_cvtepu32_epi64(__mmask8 __U, __m128i __X)
  {
    return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                               (__v4di)_mm256_cvtepu32_epi64(__X),
                                               (__v4di)_mm256_setzero_si256());
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_mask_cvtepu16_epi32(__m128i __W, __mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                               (__v4si)_mm_cvtepu16_epi32(__A),
                                               (__v4si)__W);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_maskz_cvtepu16_epi32(__mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                               (__v4si)_mm_cvtepu16_epi32(__A),
                                               (__v4si)_mm_setzero_si128());
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_mask_cvtepu16_epi32(__m256i __W, __mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                               (__v8si)_mm256_cvtepu16_epi32(__A),
                                               (__v8si)__W);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_maskz_cvtepu16_epi32(__mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                               (__v8si)_mm256_cvtepu16_epi32(__A),
                                               (__v8si)_mm256_setzero_si256());
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_mask_cvtepu16_epi64(__m128i __W, __mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                               (__v2di)_mm_cvtepu16_epi64(__A),
                                               (__v2di)__W);
  }

  static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
  _mm_maskz_cvtepu16_epi64(__mmask8 __U, __m128i __A)
  {
    return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                               (__v2di)_mm_cvtepu16_epi64(__A),
                                               (__v2di)_mm_setzero_si128());
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_mask_cvtepu16_epi64(__m256i __W, __mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                               (__v4di)_mm256_cvtepu16_epi64(__A),
                                               (__v4di)__W);
  }

  static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
  _mm256_maskz_cvtepu16_epi64(__mmask8 __U, __m128i __A)
  {
    return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                               (__v4di)_mm256_cvtepu16_epi64(__A),
                                               (__v4di)_mm256_setzero_si256());
  }
# 4365 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_rolv_epi32 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_prolvd128((__v4si)__A, (__v4si)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_rolv_epi32 (__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                             (__v4si)_mm_rolv_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_rolv_epi32 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                             (__v4si)_mm_rolv_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_rolv_epi32 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_prolvd256((__v8si)__A, (__v8si)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_rolv_epi32 (__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                            (__v8si)_mm256_rolv_epi32(__A, __B),
                                            (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_rolv_epi32 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                            (__v8si)_mm256_rolv_epi32(__A, __B),
                                            (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_rolv_epi64 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_prolvq128((__v2di)__A, (__v2di)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_rolv_epi64 (__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128(__U,
                                             (__v2di)_mm_rolv_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_rolv_epi64 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128(__U,
                                             (__v2di)_mm_rolv_epi64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_rolv_epi64 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_prolvq256((__v4di)__A, (__v4di)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_rolv_epi64 (__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256(__U,
                                            (__v4di)_mm256_rolv_epi64(__A, __B),
                                            (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_rolv_epi64 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256(__U,
                                            (__v4di)_mm256_rolv_epi64(__A, __B),
                                            (__v4di)_mm256_setzero_si256());
}
# 4505 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_sll_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_sll_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_sll_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_sll_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_sll_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_sll_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_sll_epi32(__mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_sll_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_slli_epi32(__m128i __W, __mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_slli_epi32(__A, (int)__B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_slli_epi32(__mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_slli_epi32(__A, (int)__B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_slli_epi32(__m256i __W, __mmask8 __U, __m256i __A, unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_slli_epi32(__A, (int)__B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_slli_epi32(__mmask8 __U, __m256i __A, unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_slli_epi32(__A, (int)__B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_sll_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_sll_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_sll_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_sll_epi64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_sll_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_sll_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_sll_epi64(__mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_sll_epi64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_slli_epi64(__m128i __W, __mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_slli_epi64(__A, (int)__B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_slli_epi64(__mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_slli_epi64(__A, (int)__B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_slli_epi64(__m256i __W, __mmask8 __U, __m256i __A, unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_slli_epi64(__A, (int)__B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_slli_epi64(__mmask8 __U, __m256i __A, unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_slli_epi64(__A, (int)__B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_rorv_epi32 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_prorvd128((__v4si)__A, (__v4si)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_rorv_epi32 (__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                             (__v4si)_mm_rorv_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_rorv_epi32 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                             (__v4si)_mm_rorv_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_rorv_epi32 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_prorvd256((__v8si)__A, (__v8si)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_rorv_epi32 (__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                            (__v8si)_mm256_rorv_epi32(__A, __B),
                                            (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_rorv_epi32 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                            (__v8si)_mm256_rorv_epi32(__A, __B),
                                            (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_rorv_epi64 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_prorvq128((__v2di)__A, (__v2di)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_rorv_epi64 (__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128(__U,
                                             (__v2di)_mm_rorv_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_rorv_epi64 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128(__U,
                                             (__v2di)_mm_rorv_epi64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_rorv_epi64 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_prorvq256((__v4di)__A, (__v4di)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_rorv_epi64 (__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256(__U,
                                            (__v4di)_mm256_rorv_epi64(__A, __B),
                                            (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_rorv_epi64 (__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256(__U,
                                            (__v4di)_mm256_rorv_epi64(__A, __B),
                                            (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_sllv_epi64(__m128i __W, __mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_sllv_epi64(__X, __Y),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_sllv_epi64(__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_sllv_epi64(__X, __Y),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_sllv_epi64(__m256i __W, __mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                            (__v4di)_mm256_sllv_epi64(__X, __Y),
                                            (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_sllv_epi64(__mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                            (__v4di)_mm256_sllv_epi64(__X, __Y),
                                            (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_sllv_epi32(__m128i __W, __mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_sllv_epi32(__X, __Y),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_sllv_epi32(__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_sllv_epi32(__X, __Y),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_sllv_epi32(__m256i __W, __mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                            (__v8si)_mm256_sllv_epi32(__X, __Y),
                                            (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_sllv_epi32(__mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                            (__v8si)_mm256_sllv_epi32(__X, __Y),
                                            (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_srlv_epi64(__m128i __W, __mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_srlv_epi64(__X, __Y),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_srlv_epi64(__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_srlv_epi64(__X, __Y),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_srlv_epi64(__m256i __W, __mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                            (__v4di)_mm256_srlv_epi64(__X, __Y),
                                            (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_srlv_epi64(__mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                            (__v4di)_mm256_srlv_epi64(__X, __Y),
                                            (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_srlv_epi32(__m128i __W, __mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                            (__v4si)_mm_srlv_epi32(__X, __Y),
                                            (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_srlv_epi32(__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                            (__v4si)_mm_srlv_epi32(__X, __Y),
                                            (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_srlv_epi32(__m256i __W, __mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                            (__v8si)_mm256_srlv_epi32(__X, __Y),
                                            (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_srlv_epi32(__mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                            (__v8si)_mm256_srlv_epi32(__X, __Y),
                                            (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_srl_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_srl_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_srl_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_srl_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_srl_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_srl_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_srl_epi32(__mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_srl_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_srli_epi32(__m128i __W, __mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_srli_epi32(__A, (int)__B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_srli_epi32(__mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_srli_epi32(__A, (int)__B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_srli_epi32(__m256i __W, __mmask8 __U, __m256i __A, unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_srli_epi32(__A, (int)__B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_srli_epi32(__mmask8 __U, __m256i __A, unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_srli_epi32(__A, (int)__B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_srl_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_srl_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_srl_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_srl_epi64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_srl_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_srl_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_srl_epi64(__mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_srl_epi64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_srli_epi64(__m128i __W, __mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_srli_epi64(__A, (int)__B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_srli_epi64(__mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_srli_epi64(__A, (int)__B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_srli_epi64(__m256i __W, __mmask8 __U, __m256i __A, unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_srli_epi64(__A, (int)__B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_srli_epi64(__mmask8 __U, __m256i __A, unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_srli_epi64(__A, (int)__B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_srav_epi32(__m128i __W, __mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                            (__v4si)_mm_srav_epi32(__X, __Y),
                                            (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_srav_epi32(__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                            (__v4si)_mm_srav_epi32(__X, __Y),
                                            (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_srav_epi32(__m256i __W, __mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                            (__v8si)_mm256_srav_epi32(__X, __Y),
                                            (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_srav_epi32(__mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                            (__v8si)_mm256_srav_epi32(__X, __Y),
                                            (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_srav_epi64(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_psravq128((__v2di)__X, (__v2di)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_srav_epi64(__m128i __W, __mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_srav_epi64(__X, __Y),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_srav_epi64(__mmask8 __U, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_srav_epi64(__X, __Y),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_srav_epi64(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_psravq256((__v4di)__X, (__v4di) __Y);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_srav_epi64(__m256i __W, __mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_srav_epi64(__X, __Y),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_srav_epi64 (__mmask8 __U, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_srav_epi64(__X, __Y),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_mov_epi32 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_selectd_128 ((__mmask8) __U,
                 (__v4si) __A,
                 (__v4si) __W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_mov_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_selectd_128 ((__mmask8) __U,
                 (__v4si) __A,
                 (__v4si) _mm_setzero_si128 ());
}


static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_mov_epi32 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_selectd_256 ((__mmask8) __U,
                 (__v8si) __A,
                 (__v8si) __W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_mov_epi32 (__mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_selectd_256 ((__mmask8) __U,
                 (__v8si) __A,
                 (__v8si) _mm256_setzero_si256 ());
}

static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_load_epi32 (void const *__P)
{
  return *(const __m128i *) __P;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_load_epi32 (__m128i __W, __mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_movdqa32load128_mask ((const __v4si *) __P,
              (__v4si) __W,
              (__mmask8)
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_load_epi32 (__mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_movdqa32load128_mask ((const __v4si *) __P,
              (__v4si)
              _mm_setzero_si128 (),
              (__mmask8)
              __U);
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_load_epi32 (void const *__P)
{
  return *(const __m256i *) __P;
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_load_epi32 (__m256i __W, __mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_movdqa32load256_mask ((const __v8si *) __P,
              (__v8si) __W,
              (__mmask8)
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_load_epi32 (__mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_movdqa32load256_mask ((const __v8si *) __P,
              (__v8si)
              _mm256_setzero_si256 (),
              (__mmask8)
              __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_store_epi32 (void *__P, __m128i __A)
{
  *(__m128i *) __P = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_store_epi32 (void *__P, __mmask8 __U, __m128i __A)
{
  __builtin_ia32_movdqa32store128_mask ((__v4si *) __P,
          (__v4si) __A,
          (__mmask8) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_store_epi32 (void *__P, __m256i __A)
{
  *(__m256i *) __P = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_store_epi32 (void *__P, __mmask8 __U, __m256i __A)
{
  __builtin_ia32_movdqa32store256_mask ((__v8si *) __P,
          (__v8si) __A,
          (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_mov_epi64 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_selectq_128 ((__mmask8) __U,
                 (__v2di) __A,
                 (__v2di) __W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_mov_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_selectq_128 ((__mmask8) __U,
                 (__v2di) __A,
                 (__v2di) _mm_setzero_si128 ());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_mov_epi64 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_selectq_256 ((__mmask8) __U,
                 (__v4di) __A,
                 (__v4di) __W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_mov_epi64 (__mmask8 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_selectq_256 ((__mmask8) __U,
                 (__v4di) __A,
                 (__v4di) _mm256_setzero_si256 ());
}

static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_load_epi64 (void const *__P)
{
  return *(const __m128i *) __P;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_load_epi64 (__m128i __W, __mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_movdqa64load128_mask ((const __v2di *) __P,
              (__v2di) __W,
              (__mmask8)
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_load_epi64 (__mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_movdqa64load128_mask ((const __v2di *) __P,
              (__v2di)
              _mm_setzero_si128 (),
              (__mmask8)
              __U);
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_load_epi64 (void const *__P)
{
  return *(const __m256i *) __P;
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_load_epi64 (__m256i __W, __mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_movdqa64load256_mask ((const __v4di *) __P,
              (__v4di) __W,
              (__mmask8)
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_load_epi64 (__mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_movdqa64load256_mask ((const __v4di *) __P,
              (__v4di)
              _mm256_setzero_si256 (),
              (__mmask8)
              __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_store_epi64 (void *__P, __m128i __A)
{
  *(__m128i *) __P = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_store_epi64 (void *__P, __mmask8 __U, __m128i __A)
{
  __builtin_ia32_movdqa64store128_mask ((__v2di *) __P,
          (__v2di) __A,
          (__mmask8) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_store_epi64 (void *__P, __m256i __A)
{
  *(__m256i *) __P = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_store_epi64 (void *__P, __mmask8 __U, __m256i __A)
{
  __builtin_ia32_movdqa64store256_mask ((__v4di *) __P,
          (__v4di) __A,
          (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_movedup_pd (__m128d __W, __mmask8 __U, __m128d __A)
{
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_movedup_pd(__A),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_movedup_pd (__mmask8 __U, __m128d __A)
{
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_movedup_pd(__A),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_movedup_pd (__m256d __W, __mmask8 __U, __m256d __A)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_movedup_pd(__A),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_movedup_pd (__mmask8 __U, __m256d __A)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_movedup_pd(__A),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_set1_epi32(__m128i __O, __mmask8 __M, int __A)
{
   return (__m128i)__builtin_ia32_selectd_128(__M,
                                              (__v4si) _mm_set1_epi32(__A),
                                              (__v4si)__O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_set1_epi32( __mmask8 __M, int __A)
{
   return (__m128i)__builtin_ia32_selectd_128(__M,
                                              (__v4si) _mm_set1_epi32(__A),
                                              (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_set1_epi32(__m256i __O, __mmask8 __M, int __A)
{
   return (__m256i)__builtin_ia32_selectd_256(__M,
                                              (__v8si) _mm256_set1_epi32(__A),
                                              (__v8si)__O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_set1_epi32( __mmask8 __M, int __A)
{
   return (__m256i)__builtin_ia32_selectd_256(__M,
                                              (__v8si) _mm256_set1_epi32(__A),
                                              (__v8si)_mm256_setzero_si256());
}


static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_set1_epi64 (__m128i __O, __mmask8 __M, long long __A)
{
  return (__m128i) __builtin_ia32_selectq_128(__M,
                                              (__v2di) _mm_set1_epi64x(__A),
                                              (__v2di) __O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_set1_epi64 (__mmask8 __M, long long __A)
{
  return (__m128i) __builtin_ia32_selectq_128(__M,
                                              (__v2di) _mm_set1_epi64x(__A),
                                              (__v2di) _mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_set1_epi64 (__m256i __O, __mmask8 __M, long long __A)
{
  return (__m256i) __builtin_ia32_selectq_256(__M,
                                              (__v4di) _mm256_set1_epi64x(__A),
                                              (__v4di) __O) ;
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_set1_epi64 (__mmask8 __M, long long __A)
{
   return (__m256i) __builtin_ia32_selectq_256(__M,
                                               (__v4di) _mm256_set1_epi64x(__A),
                                               (__v4di) _mm256_setzero_si256());
}
# 5443 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_load_pd (__m128d __W, __mmask8 __U, void const *__P)
{
  return (__m128d) __builtin_ia32_loadapd128_mask ((const __v2df *) __P,
               (__v2df) __W,
               (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_load_pd (__mmask8 __U, void const *__P)
{
  return (__m128d) __builtin_ia32_loadapd128_mask ((const __v2df *) __P,
               (__v2df)
               _mm_setzero_pd (),
               (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_load_pd (__m256d __W, __mmask8 __U, void const *__P)
{
  return (__m256d) __builtin_ia32_loadapd256_mask ((const __v4df *) __P,
               (__v4df) __W,
               (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_load_pd (__mmask8 __U, void const *__P)
{
  return (__m256d) __builtin_ia32_loadapd256_mask ((const __v4df *) __P,
               (__v4df)
               _mm256_setzero_pd (),
               (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_load_ps (__m128 __W, __mmask8 __U, void const *__P)
{
  return (__m128) __builtin_ia32_loadaps128_mask ((const __v4sf *) __P,
              (__v4sf) __W,
              (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_load_ps (__mmask8 __U, void const *__P)
{
  return (__m128) __builtin_ia32_loadaps128_mask ((const __v4sf *) __P,
              (__v4sf)
              _mm_setzero_ps (),
              (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_load_ps (__m256 __W, __mmask8 __U, void const *__P)
{
  return (__m256) __builtin_ia32_loadaps256_mask ((const __v8sf *) __P,
              (__v8sf) __W,
              (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_load_ps (__mmask8 __U, void const *__P)
{
  return (__m256) __builtin_ia32_loadaps256_mask ((const __v8sf *) __P,
              (__v8sf)
              _mm256_setzero_ps (),
              (__mmask8) __U);
}

static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_loadu_epi64 (void const *__P)
{
  struct __loadu_epi64 {
    __m128i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_epi64*)__P)->__v;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_loadu_epi64 (__m128i __W, __mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_loaddqudi128_mask ((const __v2di *) __P,
                 (__v2di) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_loadu_epi64 (__mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_loaddqudi128_mask ((const __v2di *) __P,
                 (__v2di)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_loadu_epi64 (void const *__P)
{
  struct __loadu_epi64 {
    __m256i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_epi64*)__P)->__v;
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_loadu_epi64 (__m256i __W, __mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_loaddqudi256_mask ((const __v4di *) __P,
                 (__v4di) __W,
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_loadu_epi64 (__mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_loaddqudi256_mask ((const __v4di *) __P,
                 (__v4di)
                 _mm256_setzero_si256 (),
                 (__mmask8) __U);
}

static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_loadu_epi32 (void const *__P)
{
  struct __loadu_epi32 {
    __m128i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_epi32*)__P)->__v;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_loadu_epi32 (__m128i __W, __mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_loaddqusi128_mask ((const __v4si *) __P,
                 (__v4si) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_loadu_epi32 (__mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_loaddqusi128_mask ((const __v4si *) __P,
                 (__v4si)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_loadu_epi32 (void const *__P)
{
  struct __loadu_epi32 {
    __m256i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_epi32*)__P)->__v;
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_loadu_epi32 (__m256i __W, __mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_loaddqusi256_mask ((const __v8si *) __P,
                 (__v8si) __W,
                 (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_loadu_epi32 (__mmask8 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_loaddqusi256_mask ((const __v8si *) __P,
                 (__v8si)
                 _mm256_setzero_si256 (),
                 (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_loadu_pd (__m128d __W, __mmask8 __U, void const *__P)
{
  return (__m128d) __builtin_ia32_loadupd128_mask ((const __v2df *) __P,
               (__v2df) __W,
               (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_loadu_pd (__mmask8 __U, void const *__P)
{
  return (__m128d) __builtin_ia32_loadupd128_mask ((const __v2df *) __P,
               (__v2df)
               _mm_setzero_pd (),
               (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_loadu_pd (__m256d __W, __mmask8 __U, void const *__P)
{
  return (__m256d) __builtin_ia32_loadupd256_mask ((const __v4df *) __P,
               (__v4df) __W,
               (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_loadu_pd (__mmask8 __U, void const *__P)
{
  return (__m256d) __builtin_ia32_loadupd256_mask ((const __v4df *) __P,
               (__v4df)
               _mm256_setzero_pd (),
               (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_loadu_ps (__m128 __W, __mmask8 __U, void const *__P)
{
  return (__m128) __builtin_ia32_loadups128_mask ((const __v4sf *) __P,
              (__v4sf) __W,
              (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_loadu_ps (__mmask8 __U, void const *__P)
{
  return (__m128) __builtin_ia32_loadups128_mask ((const __v4sf *) __P,
              (__v4sf)
              _mm_setzero_ps (),
              (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_loadu_ps (__m256 __W, __mmask8 __U, void const *__P)
{
  return (__m256) __builtin_ia32_loadups256_mask ((const __v8sf *) __P,
              (__v8sf) __W,
              (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_loadu_ps (__mmask8 __U, void const *__P)
{
  return (__m256) __builtin_ia32_loadups256_mask ((const __v8sf *) __P,
              (__v8sf)
              _mm256_setzero_ps (),
              (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_store_pd (void *__P, __mmask8 __U, __m128d __A)
{
  __builtin_ia32_storeapd128_mask ((__v2df *) __P,
           (__v2df) __A,
           (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_store_pd (void *__P, __mmask8 __U, __m256d __A)
{
  __builtin_ia32_storeapd256_mask ((__v4df *) __P,
           (__v4df) __A,
           (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_store_ps (void *__P, __mmask8 __U, __m128 __A)
{
  __builtin_ia32_storeaps128_mask ((__v4sf *) __P,
           (__v4sf) __A,
           (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_store_ps (void *__P, __mmask8 __U, __m256 __A)
{
  __builtin_ia32_storeaps256_mask ((__v8sf *) __P,
           (__v8sf) __A,
           (__mmask8) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_storeu_epi64 (void *__P, __m128i __A)
{
  struct __storeu_epi64 {
    __m128i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_epi64*)__P)->__v = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_storeu_epi64 (void *__P, __mmask8 __U, __m128i __A)
{
  __builtin_ia32_storedqudi128_mask ((__v2di *) __P,
             (__v2di) __A,
             (__mmask8) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_storeu_epi64 (void *__P, __m256i __A)
{
  struct __storeu_epi64 {
    __m256i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_epi64*)__P)->__v = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_storeu_epi64 (void *__P, __mmask8 __U, __m256i __A)
{
  __builtin_ia32_storedqudi256_mask ((__v4di *) __P,
             (__v4di) __A,
             (__mmask8) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_storeu_epi32 (void *__P, __m128i __A)
{
  struct __storeu_epi32 {
    __m128i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_epi32*)__P)->__v = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_storeu_epi32 (void *__P, __mmask8 __U, __m128i __A)
{
  __builtin_ia32_storedqusi128_mask ((__v4si *) __P,
             (__v4si) __A,
             (__mmask8) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_storeu_epi32 (void *__P, __m256i __A)
{
  struct __storeu_epi32 {
    __m256i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_epi32*)__P)->__v = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_storeu_epi32 (void *__P, __mmask8 __U, __m256i __A)
{
  __builtin_ia32_storedqusi256_mask ((__v8si *) __P,
             (__v8si) __A,
             (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_storeu_pd (void *__P, __mmask8 __U, __m128d __A)
{
  __builtin_ia32_storeupd128_mask ((__v2df *) __P,
           (__v2df) __A,
           (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_storeu_pd (void *__P, __mmask8 __U, __m256d __A)
{
  __builtin_ia32_storeupd256_mask ((__v4df *) __P,
           (__v4df) __A,
           (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_storeu_ps (void *__P, __mmask8 __U, __m128 __A)
{
  __builtin_ia32_storeups128_mask ((__v4sf *) __P,
           (__v4sf) __A,
           (__mmask8) __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_storeu_ps (void *__P, __mmask8 __U, __m256 __A)
{
  __builtin_ia32_storeups256_mask ((__v8sf *) __P,
           (__v8sf) __A,
           (__mmask8) __U);
}


static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_unpackhi_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_unpackhi_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_unpackhi_pd(__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_unpackhi_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_unpackhi_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                           (__v4df)_mm256_unpackhi_pd(__A, __B),
                                           (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_unpackhi_pd(__mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                           (__v4df)_mm256_unpackhi_pd(__A, __B),
                                           (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_unpackhi_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_unpackhi_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_unpackhi_ps(__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_unpackhi_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_unpackhi_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                           (__v8sf)_mm256_unpackhi_ps(__A, __B),
                                           (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_unpackhi_ps(__mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                           (__v8sf)_mm256_unpackhi_ps(__A, __B),
                                           (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_unpacklo_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_unpacklo_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_unpacklo_pd(__mmask8 __U, __m128d __A, __m128d __B)
{
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_unpacklo_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_unpacklo_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                           (__v4df)_mm256_unpacklo_pd(__A, __B),
                                           (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_unpacklo_pd(__mmask8 __U, __m256d __A, __m256d __B)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                           (__v4df)_mm256_unpacklo_pd(__A, __B),
                                           (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_unpacklo_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_unpacklo_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_unpacklo_ps(__mmask8 __U, __m128 __A, __m128 __B)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_unpacklo_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_unpacklo_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                           (__v8sf)_mm256_unpacklo_ps(__A, __B),
                                           (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_unpacklo_ps(__mmask8 __U, __m256 __A, __m256 __B)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                           (__v8sf)_mm256_unpacklo_ps(__A, __B),
                                           (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_rcp14_pd (__m128d __A)
{
  return (__m128d) __builtin_ia32_rcp14pd128_mask ((__v2df) __A,
                (__v2df)
                _mm_setzero_pd (),
                (__mmask8) -1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_rcp14_pd (__m128d __W, __mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_rcp14pd128_mask ((__v2df) __A,
                (__v2df) __W,
                (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_rcp14_pd (__mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_rcp14pd128_mask ((__v2df) __A,
                (__v2df)
                _mm_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_rcp14_pd (__m256d __A)
{
  return (__m256d) __builtin_ia32_rcp14pd256_mask ((__v4df) __A,
                (__v4df)
                _mm256_setzero_pd (),
                (__mmask8) -1);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_rcp14_pd (__m256d __W, __mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_rcp14pd256_mask ((__v4df) __A,
                (__v4df) __W,
                (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_rcp14_pd (__mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_rcp14pd256_mask ((__v4df) __A,
                (__v4df)
                _mm256_setzero_pd (),
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_rcp14_ps (__m128 __A)
{
  return (__m128) __builtin_ia32_rcp14ps128_mask ((__v4sf) __A,
               (__v4sf)
               _mm_setzero_ps (),
               (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_rcp14_ps (__m128 __W, __mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_rcp14ps128_mask ((__v4sf) __A,
               (__v4sf) __W,
               (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_rcp14_ps (__mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_rcp14ps128_mask ((__v4sf) __A,
               (__v4sf)
               _mm_setzero_ps (),
               (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_rcp14_ps (__m256 __A)
{
  return (__m256) __builtin_ia32_rcp14ps256_mask ((__v8sf) __A,
               (__v8sf)
               _mm256_setzero_ps (),
               (__mmask8) -1);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_rcp14_ps (__m256 __W, __mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_rcp14ps256_mask ((__v8sf) __A,
               (__v8sf) __W,
               (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_rcp14_ps (__mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_rcp14ps256_mask ((__v8sf) __A,
               (__v8sf)
               _mm256_setzero_ps (),
               (__mmask8) __U);
}
# 6088 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_permutevar_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128i __C)
{
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                            (__v2df)_mm_permutevar_pd(__A, __C),
                                            (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_permutevar_pd(__mmask8 __U, __m128d __A, __m128i __C)
{
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                            (__v2df)_mm_permutevar_pd(__A, __C),
                                            (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_permutevar_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256i __C)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                         (__v4df)_mm256_permutevar_pd(__A, __C),
                                         (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_permutevar_pd(__mmask8 __U, __m256d __A, __m256i __C)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                         (__v4df)_mm256_permutevar_pd(__A, __C),
                                         (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_permutevar_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128i __C)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                            (__v4sf)_mm_permutevar_ps(__A, __C),
                                            (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_permutevar_ps(__mmask8 __U, __m128 __A, __m128i __C)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                            (__v4sf)_mm_permutevar_ps(__A, __C),
                                            (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_permutevar_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256i __C)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                          (__v8sf)_mm256_permutevar_ps(__A, __C),
                                          (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_permutevar_ps(__mmask8 __U, __m256 __A, __m256i __C)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                          (__v8sf)_mm256_permutevar_ps(__A, __C),
                                          (__v8sf)_mm256_setzero_ps());
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_test_epi32_mask (__m128i __A, __m128i __B)
{
  return ((__mmask8)__builtin_ia32_cmpd128_mask((__v4si)(__m128i)((_mm_and_si128 (__A, __B))), (__v4si)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_NE), (__mmask8)-1));
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_test_epi32_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return ((__mmask8)__builtin_ia32_cmpd128_mask((__v4si)(__m128i)((_mm_and_si128 (__A, __B))), (__v4si)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_NE), (__mmask8)((__U))));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_test_epi32_mask (__m256i __A, __m256i __B)
{
  return ((__mmask8)__builtin_ia32_cmpd256_mask((__v8si)(__m256i)((_mm256_and_si256 (__A, __B))), (__v8si)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_NE), (__mmask8)-1));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_test_epi32_mask (__mmask8 __U, __m256i __A, __m256i __B)
{
  return ((__mmask8)__builtin_ia32_cmpd256_mask((__v8si)(__m256i)((_mm256_and_si256 (__A, __B))), (__v8si)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_NE), (__mmask8)((__U))));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_test_epi64_mask (__m128i __A, __m128i __B)
{
  return ((__mmask8)__builtin_ia32_cmpq128_mask((__v2di)(__m128i)((_mm_and_si128 (__A, __B))), (__v2di)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_NE), (__mmask8)-1));
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_test_epi64_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return ((__mmask8)__builtin_ia32_cmpq128_mask((__v2di)(__m128i)((_mm_and_si128 (__A, __B))), (__v2di)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_NE), (__mmask8)((__U))));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_test_epi64_mask (__m256i __A, __m256i __B)
{
  return ((__mmask8)__builtin_ia32_cmpq256_mask((__v4di)(__m256i)((_mm256_and_si256 (__A, __B))), (__v4di)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_NE), (__mmask8)-1));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_test_epi64_mask (__mmask8 __U, __m256i __A, __m256i __B)
{
  return ((__mmask8)__builtin_ia32_cmpq256_mask((__v4di)(__m256i)((_mm256_and_si256 (__A, __B))), (__v4di)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_NE), (__mmask8)((__U))));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_testn_epi32_mask (__m128i __A, __m128i __B)
{
  return ((__mmask8)__builtin_ia32_cmpd128_mask((__v4si)(__m128i)((_mm_and_si128 (__A, __B))), (__v4si)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_EQ), (__mmask8)-1));
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_testn_epi32_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return ((__mmask8)__builtin_ia32_cmpd128_mask((__v4si)(__m128i)((_mm_and_si128 (__A, __B))), (__v4si)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_EQ), (__mmask8)((__U))));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_testn_epi32_mask (__m256i __A, __m256i __B)
{
  return ((__mmask8)__builtin_ia32_cmpd256_mask((__v8si)(__m256i)((_mm256_and_si256 (__A, __B))), (__v8si)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_EQ), (__mmask8)-1));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_testn_epi32_mask (__mmask8 __U, __m256i __A, __m256i __B)
{
  return ((__mmask8)__builtin_ia32_cmpd256_mask((__v8si)(__m256i)((_mm256_and_si256 (__A, __B))), (__v8si)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_EQ), (__mmask8)((__U))));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_testn_epi64_mask (__m128i __A, __m128i __B)
{
  return ((__mmask8)__builtin_ia32_cmpq128_mask((__v2di)(__m128i)((_mm_and_si128 (__A, __B))), (__v2di)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_EQ), (__mmask8)-1));
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_testn_epi64_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return ((__mmask8)__builtin_ia32_cmpq128_mask((__v2di)(__m128i)((_mm_and_si128 (__A, __B))), (__v2di)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_EQ), (__mmask8)((__U))));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_testn_epi64_mask (__m256i __A, __m256i __B)
{
  return ((__mmask8)__builtin_ia32_cmpq256_mask((__v4di)(__m256i)((_mm256_and_si256 (__A, __B))), (__v4di)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_EQ), (__mmask8)-1));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_testn_epi64_mask (__mmask8 __U, __m256i __A, __m256i __B)
{
  return ((__mmask8)__builtin_ia32_cmpq256_mask((__v4di)(__m256i)((_mm256_and_si256 (__A, __B))), (__v4di)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_EQ), (__mmask8)((__U))));

}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_unpackhi_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                           (__v4si)_mm_unpackhi_epi32(__A, __B),
                                           (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_unpackhi_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                           (__v4si)_mm_unpackhi_epi32(__A, __B),
                                           (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_unpackhi_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                        (__v8si)_mm256_unpackhi_epi32(__A, __B),
                                        (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_unpackhi_epi32(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                        (__v8si)_mm256_unpackhi_epi32(__A, __B),
                                        (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_unpackhi_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                           (__v2di)_mm_unpackhi_epi64(__A, __B),
                                           (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_unpackhi_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                           (__v2di)_mm_unpackhi_epi64(__A, __B),
                                           (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_unpackhi_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                        (__v4di)_mm256_unpackhi_epi64(__A, __B),
                                        (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_unpackhi_epi64(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                        (__v4di)_mm256_unpackhi_epi64(__A, __B),
                                        (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_unpacklo_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                           (__v4si)_mm_unpacklo_epi32(__A, __B),
                                           (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_unpacklo_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                           (__v4si)_mm_unpacklo_epi32(__A, __B),
                                           (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_unpacklo_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                        (__v8si)_mm256_unpacklo_epi32(__A, __B),
                                        (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_unpacklo_epi32(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                        (__v8si)_mm256_unpacklo_epi32(__A, __B),
                                        (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_unpacklo_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                           (__v2di)_mm_unpacklo_epi64(__A, __B),
                                           (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_unpacklo_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                           (__v2di)_mm_unpacklo_epi64(__A, __B),
                                           (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_unpacklo_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                        (__v4di)_mm256_unpacklo_epi64(__A, __B),
                                        (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_unpacklo_epi64(__mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                        (__v4di)_mm256_unpacklo_epi64(__A, __B),
                                        (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_sra_epi32(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_sra_epi32(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_sra_epi32(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_sra_epi32(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_sra_epi32(__m256i __W, __mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_sra_epi32(__A, __B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_sra_epi32(__mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_sra_epi32(__A, __B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_srai_epi32(__m128i __W, __mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_srai_epi32(__A, (int)__B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_srai_epi32(__mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_srai_epi32(__A, (int)__B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_srai_epi32(__m256i __W, __mmask8 __U, __m256i __A, unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_srai_epi32(__A, (int)__B),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_srai_epi32(__mmask8 __U, __m256i __A, unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_srai_epi32(__A, (int)__B),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_sra_epi64(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_psraq128((__v2di)__A, (__v2di)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_sra_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U, (__v2di)_mm_sra_epi64(__A, __B), (__v2di)__W);


}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_sra_epi64(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U, (__v2di)_mm_sra_epi64(__A, __B), (__v2di)_mm_setzero_si128());


}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_sra_epi64(__m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_psraq256((__v4di) __A, (__v2di) __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_sra_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U, (__v4di)_mm256_sra_epi64(__A, __B), (__v4di)__W);


}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_sra_epi64(__mmask8 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U, (__v4di)_mm256_sra_epi64(__A, __B), (__v4di)_mm256_setzero_si256());


}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_srai_epi64(__m128i __A, unsigned int __imm)
{
  return (__m128i)__builtin_ia32_psraqi128((__v2di)__A, (int)__imm);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_srai_epi64(__m128i __W, __mmask8 __U, __m128i __A, unsigned int __imm)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U, (__v2di)_mm_srai_epi64(__A, __imm), (__v2di)__W);


}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_srai_epi64(__mmask8 __U, __m128i __A, unsigned int __imm)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U, (__v2di)_mm_srai_epi64(__A, __imm), (__v2di)_mm_setzero_si128());


}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_srai_epi64(__m256i __A, unsigned int __imm)
{
  return (__m256i)__builtin_ia32_psraqi256((__v4di)__A, (int)__imm);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_srai_epi64(__m256i __W, __mmask8 __U, __m256i __A,
                       unsigned int __imm)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U, (__v4di)_mm256_srai_epi64(__A, __imm), (__v4di)__W);


}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_srai_epi64(__mmask8 __U, __m256i __A, unsigned int __imm)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U, (__v4di)_mm256_srai_epi64(__A, __imm), (__v4di)_mm256_setzero_si256());


}
# 6698 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_rsqrt14_pd (__m128d __A)
{
  return (__m128d) __builtin_ia32_rsqrt14pd128_mask ((__v2df) __A,
                 (__v2df)
                 _mm_setzero_pd (),
                 (__mmask8) -1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_rsqrt14_pd (__m128d __W, __mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_rsqrt14pd128_mask ((__v2df) __A,
                 (__v2df) __W,
                 (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_rsqrt14_pd (__mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_rsqrt14pd128_mask ((__v2df) __A,
                 (__v2df)
                 _mm_setzero_pd (),
                 (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_rsqrt14_pd (__m256d __A)
{
  return (__m256d) __builtin_ia32_rsqrt14pd256_mask ((__v4df) __A,
                 (__v4df)
                 _mm256_setzero_pd (),
                 (__mmask8) -1);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_rsqrt14_pd (__m256d __W, __mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_rsqrt14pd256_mask ((__v4df) __A,
                 (__v4df) __W,
                 (__mmask8) __U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_rsqrt14_pd (__mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_rsqrt14pd256_mask ((__v4df) __A,
                 (__v4df)
                 _mm256_setzero_pd (),
                 (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_rsqrt14_ps (__m128 __A)
{
  return (__m128) __builtin_ia32_rsqrt14ps128_mask ((__v4sf) __A,
                (__v4sf)
                _mm_setzero_ps (),
                (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_rsqrt14_ps (__m128 __W, __mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_rsqrt14ps128_mask ((__v4sf) __A,
                (__v4sf) __W,
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_rsqrt14_ps (__mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_rsqrt14ps128_mask ((__v4sf) __A,
                (__v4sf)
                _mm_setzero_ps (),
                (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_rsqrt14_ps (__m256 __A)
{
  return (__m256) __builtin_ia32_rsqrt14ps256_mask ((__v8sf) __A,
                (__v8sf)
                _mm256_setzero_ps (),
                (__mmask8) -1);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_rsqrt14_ps (__m256 __W, __mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_rsqrt14ps256_mask ((__v8sf) __A,
                (__v8sf) __W,
                (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_rsqrt14_ps (__mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_rsqrt14ps256_mask ((__v8sf) __A,
                (__v8sf)
                _mm256_setzero_ps (),
                (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_broadcast_f32x4(__m128 __A)
{
  return (__m256)__builtin_shufflevector((__v4sf)__A, (__v4sf)__A,
                                         0, 1, 2, 3, 0, 1, 2, 3);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_broadcast_f32x4(__m256 __O, __mmask8 __M, __m128 __A)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__M,
                                            (__v8sf)_mm256_broadcast_f32x4(__A),
                                            (__v8sf)__O);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_broadcast_f32x4 (__mmask8 __M, __m128 __A)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__M,
                                            (__v8sf)_mm256_broadcast_f32x4(__A),
                                            (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_broadcast_i32x4(__m128i __A)
{
  return (__m256i)__builtin_shufflevector((__v4si)__A, (__v4si)__A,
                                          0, 1, 2, 3, 0, 1, 2, 3);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_broadcast_i32x4(__m256i __O, __mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                            (__v8si)_mm256_broadcast_i32x4(__A),
                                            (__v8si)__O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_broadcast_i32x4(__mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                            (__v8si)_mm256_broadcast_i32x4(__A),
                                            (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_broadcastsd_pd (__m256d __O, __mmask8 __M, __m128d __A)
{
  return (__m256d)__builtin_ia32_selectpd_256(__M,
                                              (__v4df) _mm256_broadcastsd_pd(__A),
                                              (__v4df) __O);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_broadcastsd_pd (__mmask8 __M, __m128d __A)
{
  return (__m256d)__builtin_ia32_selectpd_256(__M,
                                              (__v4df) _mm256_broadcastsd_pd(__A),
                                              (__v4df) _mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_broadcastss_ps (__m128 __O, __mmask8 __M, __m128 __A)
{
  return (__m128)__builtin_ia32_selectps_128(__M,
                                             (__v4sf) _mm_broadcastss_ps(__A),
                                             (__v4sf) __O);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_broadcastss_ps (__mmask8 __M, __m128 __A)
{
  return (__m128)__builtin_ia32_selectps_128(__M,
                                             (__v4sf) _mm_broadcastss_ps(__A),
                                             (__v4sf) _mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_broadcastss_ps (__m256 __O, __mmask8 __M, __m128 __A)
{
  return (__m256)__builtin_ia32_selectps_256(__M,
                                             (__v8sf) _mm256_broadcastss_ps(__A),
                                             (__v8sf) __O);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_broadcastss_ps (__mmask8 __M, __m128 __A)
{
  return (__m256)__builtin_ia32_selectps_256(__M,
                                             (__v8sf) _mm256_broadcastss_ps(__A),
                                             (__v8sf) _mm256_setzero_ps());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_broadcastd_epi32 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128(__M,
                                             (__v4si) _mm_broadcastd_epi32(__A),
                                             (__v4si) __O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_broadcastd_epi32 (__mmask8 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128(__M,
                                             (__v4si) _mm_broadcastd_epi32(__A),
                                             (__v4si) _mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_broadcastd_epi32 (__m256i __O, __mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256(__M,
                                             (__v8si) _mm256_broadcastd_epi32(__A),
                                             (__v8si) __O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_broadcastd_epi32 (__mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256(__M,
                                             (__v8si) _mm256_broadcastd_epi32(__A),
                                             (__v8si) _mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_broadcastq_epi64 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectq_128(__M,
                                             (__v2di) _mm_broadcastq_epi64(__A),
                                             (__v2di) __O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_broadcastq_epi64 (__mmask8 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectq_128(__M,
                                             (__v2di) _mm_broadcastq_epi64(__A),
                                             (__v2di) _mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_broadcastq_epi64 (__m256i __O, __mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectq_256(__M,
                                             (__v4di) _mm256_broadcastq_epi64(__A),
                                             (__v4di) __O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_broadcastq_epi64 (__mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectq_256(__M,
                                             (__v4di) _mm256_broadcastq_epi64(__A),
                                             (__v4di) _mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_cvtsepi32_epi8 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb128_mask ((__v4si) __A,
               (__v16qi)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtsepi32_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb128_mask ((__v4si) __A,
               (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtsepi32_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb128_mask ((__v4si) __A,
               (__v16qi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtsepi32_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovsdb128mem_mask ((__v16qi *) __P, (__v4si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_cvtsepi32_epi8 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb256_mask ((__v8si) __A,
               (__v16qi)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtsepi32_epi8 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb256_mask ((__v8si) __A,
               (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtsepi32_epi8 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsdb256_mask ((__v8si) __A,
               (__v16qi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtsepi32_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovsdb256mem_mask ((__v16qi *) __P, (__v8si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_cvtsepi32_epi16 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsdw128_mask ((__v4si) __A,
               (__v8hi)_mm_setzero_si128 (),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtsepi32_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsdw128_mask ((__v4si) __A,
               (__v8hi)__O,
               __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtsepi32_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsdw128_mask ((__v4si) __A,
               (__v8hi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtsepi32_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovsdw128mem_mask ((__v8hi *) __P, (__v4si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_cvtsepi32_epi16 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsdw256_mask ((__v8si) __A,
               (__v8hi)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtsepi32_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsdw256_mask ((__v8si) __A,
               (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtsepi32_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsdw256_mask ((__v8si) __A,
               (__v8hi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtsepi32_storeu_epi16 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovsdw256mem_mask ((__v8hi *) __P, (__v8si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_cvtsepi64_epi8 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb128_mask ((__v2di) __A,
               (__v16qi)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtsepi64_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb128_mask ((__v2di) __A,
               (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtsepi64_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb128_mask ((__v2di) __A,
               (__v16qi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtsepi64_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovsqb128mem_mask ((__v16qi *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_cvtsepi64_epi8 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb256_mask ((__v4di) __A,
               (__v16qi)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtsepi64_epi8 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb256_mask ((__v4di) __A,
               (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtsepi64_epi8 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqb256_mask ((__v4di) __A,
               (__v16qi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtsepi64_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovsqb256mem_mask ((__v16qi *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_cvtsepi64_epi32 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqd128_mask ((__v2di) __A,
               (__v4si)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtsepi64_epi32 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqd128_mask ((__v2di) __A,
               (__v4si) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtsepi64_epi32 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqd128_mask ((__v2di) __A,
               (__v4si) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtsepi64_storeu_epi32 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovsqd128mem_mask ((__v4si *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_cvtsepi64_epi32 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqd256_mask ((__v4di) __A,
               (__v4si)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtsepi64_epi32 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqd256_mask ((__v4di) __A,
               (__v4si)__O,
               __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtsepi64_epi32 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqd256_mask ((__v4di) __A,
               (__v4si) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtsepi64_storeu_epi32 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovsqd256mem_mask ((__v4si *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_cvtsepi64_epi16 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw128_mask ((__v2di) __A,
               (__v8hi)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtsepi64_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw128_mask ((__v2di) __A,
               (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtsepi64_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw128_mask ((__v2di) __A,
               (__v8hi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtsepi64_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovsqw128mem_mask ((__v8hi *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_cvtsepi64_epi16 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw256_mask ((__v4di) __A,
               (__v8hi)_mm_undefined_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtsepi64_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw256_mask ((__v4di) __A,
               (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtsepi64_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovsqw256_mask ((__v4di) __A,
               (__v8hi) _mm_setzero_si128 (),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtsepi64_storeu_epi16 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovsqw256mem_mask ((__v8hi *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_cvtusepi32_epi8 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb128_mask ((__v4si) __A,
                (__v16qi)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtusepi32_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb128_mask ((__v4si) __A,
                (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtusepi32_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb128_mask ((__v4si) __A,
                (__v16qi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtusepi32_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovusdb128mem_mask ((__v16qi *) __P, (__v4si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_cvtusepi32_epi8 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb256_mask ((__v8si) __A,
                (__v16qi)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtusepi32_epi8 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb256_mask ((__v8si) __A,
                (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtusepi32_epi8 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusdb256_mask ((__v8si) __A,
                (__v16qi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtusepi32_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovusdb256mem_mask ((__v16qi*) __P, (__v8si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_cvtusepi32_epi16 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusdw128_mask ((__v4si) __A,
                (__v8hi)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtusepi32_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusdw128_mask ((__v4si) __A,
                (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtusepi32_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusdw128_mask ((__v4si) __A,
                (__v8hi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtusepi32_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovusdw128mem_mask ((__v8hi *) __P, (__v4si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_cvtusepi32_epi16 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusdw256_mask ((__v8si) __A,
                (__v8hi) _mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtusepi32_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusdw256_mask ((__v8si) __A,
                (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtusepi32_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusdw256_mask ((__v8si) __A,
                (__v8hi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtusepi32_storeu_epi16 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovusdw256mem_mask ((__v8hi *) __P, (__v8si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_cvtusepi64_epi8 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb128_mask ((__v2di) __A,
                (__v16qi)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtusepi64_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb128_mask ((__v2di) __A,
                (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtusepi64_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb128_mask ((__v2di) __A,
                (__v16qi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtusepi64_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovusqb128mem_mask ((__v16qi *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_cvtusepi64_epi8 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb256_mask ((__v4di) __A,
                (__v16qi)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtusepi64_epi8 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb256_mask ((__v4di) __A,
                (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtusepi64_epi8 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqb256_mask ((__v4di) __A,
                (__v16qi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtusepi64_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovusqb256mem_mask ((__v16qi *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_cvtusepi64_epi32 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqd128_mask ((__v2di) __A,
                (__v4si)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtusepi64_epi32 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqd128_mask ((__v2di) __A,
                (__v4si) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtusepi64_epi32 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqd128_mask ((__v2di) __A,
                (__v4si) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtusepi64_storeu_epi32 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovusqd128mem_mask ((__v4si *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_cvtusepi64_epi32 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqd256_mask ((__v4di) __A,
                (__v4si)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtusepi64_epi32 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqd256_mask ((__v4di) __A,
                (__v4si) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtusepi64_epi32 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqd256_mask ((__v4di) __A,
                (__v4si) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtusepi64_storeu_epi32 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovusqd256mem_mask ((__v4si *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_cvtusepi64_epi16 (__m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw128_mask ((__v2di) __A,
                (__v8hi)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtusepi64_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw128_mask ((__v2di) __A,
                (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtusepi64_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw128_mask ((__v2di) __A,
                (__v8hi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtusepi64_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovusqw128mem_mask ((__v8hi *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_cvtusepi64_epi16 (__m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw256_mask ((__v4di) __A,
                (__v8hi)_mm_undefined_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtusepi64_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw256_mask ((__v4di) __A,
                (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtusepi64_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovusqw256_mask ((__v4di) __A,
                (__v8hi) _mm_setzero_si128 (),
                __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtusepi64_storeu_epi16 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovusqw256mem_mask ((__v8hi *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_cvtepi32_epi8 (__m128i __A)
{
  return (__m128i)__builtin_shufflevector(
      __builtin_convertvector((__v4si)__A, __v4qi), (__v4qi){0, 0, 0, 0}, 0, 1,
      2, 3, 4, 5, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtepi32_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovdb128_mask ((__v4si) __A,
              (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtepi32_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovdb128_mask ((__v4si) __A,
              (__v16qi)
              _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtepi32_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovdb128mem_mask ((__v16qi *) __P, (__v4si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_cvtepi32_epi8 (__m256i __A)
{
  return (__m128i)__builtin_shufflevector(
      __builtin_convertvector((__v8si)__A, __v8qi),
      (__v8qi){0, 0, 0, 0, 0, 0, 0, 0}, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
      12, 13, 14, 15);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtepi32_epi8 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovdb256_mask ((__v8si) __A,
              (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtepi32_epi8 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovdb256_mask ((__v8si) __A,
              (__v16qi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtepi32_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovdb256mem_mask ((__v16qi *) __P, (__v8si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_cvtepi32_epi16 (__m128i __A)
{
  return (__m128i)__builtin_shufflevector(
      __builtin_convertvector((__v4si)__A, __v4hi), (__v4hi){0, 0, 0, 0}, 0, 1,
      2, 3, 4, 5, 6, 7);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtepi32_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovdw128_mask ((__v4si) __A,
              (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtepi32_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovdw128_mask ((__v4si) __A,
              (__v8hi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtepi32_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovdw128mem_mask ((__v8hi *) __P, (__v4si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_cvtepi32_epi16 (__m256i __A)
{
  return (__m128i)__builtin_convertvector((__v8si)__A, __v8hi);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtepi32_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovdw256_mask ((__v8si) __A,
              (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtepi32_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovdw256_mask ((__v8si) __A,
              (__v8hi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtepi32_storeu_epi16 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovdw256mem_mask ((__v8hi *) __P, (__v8si) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_cvtepi64_epi8 (__m128i __A)
{
  return (__m128i)__builtin_shufflevector(
      __builtin_convertvector((__v2di)__A, __v2qi), (__v2qi){0, 0}, 0, 1, 2, 3,
      3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtepi64_epi8 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqb128_mask ((__v2di) __A,
              (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtepi64_epi8 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqb128_mask ((__v2di) __A,
              (__v16qi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtepi64_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovqb128mem_mask ((__v16qi *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_cvtepi64_epi8 (__m256i __A)
{
  return (__m128i)__builtin_shufflevector(
      __builtin_convertvector((__v4di)__A, __v4qi), (__v4qi){0, 0, 0, 0}, 0, 1,
      2, 3, 4, 5, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtepi64_epi8 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqb256_mask ((__v4di) __A,
              (__v16qi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtepi64_epi8 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqb256_mask ((__v4di) __A,
              (__v16qi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtepi64_storeu_epi8 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovqb256mem_mask ((__v16qi *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_cvtepi64_epi32 (__m128i __A)
{
  return (__m128i)__builtin_shufflevector(
      __builtin_convertvector((__v2di)__A, __v2si), (__v2si){0, 0}, 0, 1, 2, 3);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtepi64_epi32 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqd128_mask ((__v2di) __A,
              (__v4si) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtepi64_epi32 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqd128_mask ((__v2di) __A,
              (__v4si) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtepi64_storeu_epi32 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovqd128mem_mask ((__v4si *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_cvtepi64_epi32 (__m256i __A)
{
  return (__m128i)__builtin_convertvector((__v4di)__A, __v4si);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtepi64_epi32 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm256_cvtepi64_epi32(__A),
                                             (__v4si)__O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtepi64_epi32 (__mmask8 __M, __m256i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm256_cvtepi64_epi32(__A),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtepi64_storeu_epi32 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovqd256mem_mask ((__v4si *) __P, (__v4di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_cvtepi64_epi16 (__m128i __A)
{
  return (__m128i)__builtin_shufflevector(
      __builtin_convertvector((__v2di)__A, __v2hi), (__v2hi){0, 0}, 0, 1, 2, 3,
      3, 3, 3, 3);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtepi64_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqw128_mask ((__v2di) __A,
              (__v8hi)__O,
              __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtepi64_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i) __builtin_ia32_pmovqw128_mask ((__v2di) __A,
              (__v8hi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtepi64_storeu_epi16 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovqw128mem_mask ((__v8hi *) __P, (__v2di) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_cvtepi64_epi16 (__m256i __A)
{
  return (__m128i)__builtin_shufflevector(
      __builtin_convertvector((__v4di)__A, __v4hi), (__v4hi){0, 0, 0, 0}, 0, 1,
      2, 3, 4, 5, 6, 7);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtepi64_epi16 (__m128i __O, __mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqw256_mask ((__v4di) __A,
              (__v8hi) __O, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtepi64_epi16 (__mmask8 __M, __m256i __A)
{
  return (__m128i) __builtin_ia32_pmovqw256_mask ((__v4di) __A,
              (__v8hi) _mm_setzero_si128 (),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtepi64_storeu_epi16 (void * __P, __mmask8 __M, __m256i __A)
{
  __builtin_ia32_pmovqw256mem_mask ((__v8hi *) __P, (__v4di) __A, __M);
}
# 8093 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlintrin.h" 3
static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_permutexvar_pd (__m256i __X, __m256d __Y)
{
  return (__m256d)__builtin_ia32_permvardf256((__v4df)__Y, (__v4di)__X);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_permutexvar_pd (__m256d __W, __mmask8 __U, __m256i __X,
          __m256d __Y)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                        (__v4df)_mm256_permutexvar_pd(__X, __Y),
                                        (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_permutexvar_pd (__mmask8 __U, __m256i __X, __m256d __Y)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                        (__v4df)_mm256_permutexvar_pd(__X, __Y),
                                        (__v4df)_mm256_setzero_pd());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_permutexvar_epi64 ( __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_permvardi256((__v4di) __Y, (__v4di) __X);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_permutexvar_epi64 (__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                     (__v4di)_mm256_permutexvar_epi64(__X, __Y),
                                     (__v4di)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_permutexvar_epi64 (__m256i __W, __mmask8 __M, __m256i __X,
             __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                     (__v4di)_mm256_permutexvar_epi64(__X, __Y),
                                     (__v4di)__W);
}



static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_permutexvar_ps(__m256 __W, __mmask8 __U, __m256i __X, __m256 __Y)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                        (__v8sf)_mm256_permutevar8x32_ps((__Y), (__X)),
                                        (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_permutexvar_ps(__mmask8 __U, __m256i __X, __m256 __Y)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                        (__v8sf)_mm256_permutevar8x32_ps((__Y), (__X)),
                                        (__v8sf)_mm256_setzero_ps());
}



static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_permutexvar_epi32(__m256i __W, __mmask8 __M, __m256i __X,
                              __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                     (__v8si)_mm256_permutevar8x32_epi32((__Y), (__X)),
                                     (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_permutexvar_epi32(__mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                     (__v8si)_mm256_permutevar8x32_epi32((__Y), (__X)),
                                     (__v8si)_mm256_setzero_si256());
}
# 8232 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_movehdup_ps (__m128 __W, __mmask8 __U, __m128 __A)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_movehdup_ps(__A),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_movehdup_ps (__mmask8 __U, __m128 __A)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_movehdup_ps(__A),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_movehdup_ps (__m256 __W, __mmask8 __U, __m256 __A)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_movehdup_ps(__A),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_movehdup_ps (__mmask8 __U, __m256 __A)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_movehdup_ps(__A),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_moveldup_ps (__m128 __W, __mmask8 __U, __m128 __A)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_moveldup_ps(__A),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_moveldup_ps (__mmask8 __U, __m128 __A)
{
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_moveldup_ps(__A),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_moveldup_ps (__m256 __W, __mmask8 __U, __m256 __A)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_moveldup_ps(__A),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_moveldup_ps (__mmask8 __U, __m256 __A)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_moveldup_ps(__A),
                                             (__v8sf)_mm256_setzero_ps());
}
# 8316 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlintrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_mov_pd (__m128d __W, __mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_selectpd_128 ((__mmask8) __U,
              (__v2df) __A,
              (__v2df) __W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_mov_pd (__mmask8 __U, __m128d __A)
{
  return (__m128d) __builtin_ia32_selectpd_128 ((__mmask8) __U,
              (__v2df) __A,
              (__v2df) _mm_setzero_pd ());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_mov_pd (__m256d __W, __mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_selectpd_256 ((__mmask8) __U,
              (__v4df) __A,
              (__v4df) __W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_mov_pd (__mmask8 __U, __m256d __A)
{
  return (__m256d) __builtin_ia32_selectpd_256 ((__mmask8) __U,
              (__v4df) __A,
              (__v4df) _mm256_setzero_pd ());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_mov_ps (__m128 __W, __mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_selectps_128 ((__mmask8) __U,
             (__v4sf) __A,
             (__v4sf) __W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_mov_ps (__mmask8 __U, __m128 __A)
{
  return (__m128) __builtin_ia32_selectps_128 ((__mmask8) __U,
             (__v4sf) __A,
             (__v4sf) _mm_setzero_ps ());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_mov_ps (__m256 __W, __mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_selectps_256 ((__mmask8) __U,
             (__v8sf) __A,
             (__v8sf) __W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_mov_ps (__mmask8 __U, __m256 __A)
{
  return (__m256) __builtin_ia32_selectps_256 ((__mmask8) __U,
             (__v8sf) __A,
             (__v8sf) _mm256_setzero_ps ());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtph_ps (__m128 __W, __mmask8 __U, __m128i __A)
{
  return (__m128) __builtin_ia32_vcvtph2ps_mask ((__v8hi) __A,
             (__v4sf) __W,
             (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtph_ps (__mmask8 __U, __m128i __A)
{
  return (__m128) __builtin_ia32_vcvtph2ps_mask ((__v8hi) __A,
             (__v4sf)
             _mm_setzero_ps (),
             (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtph_ps (__m256 __W, __mmask8 __U, __m128i __A)
{
  return (__m256) __builtin_ia32_vcvtph2ps256_mask ((__v8hi) __A,
                (__v8sf) __W,
                (__mmask8) __U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtph_ps (__mmask8 __U, __m128i __A)
{
  return (__m256) __builtin_ia32_vcvtph2ps256_mask ((__v8hi) __A,
                (__v8sf)
                _mm256_setzero_ps (),
                (__mmask8) __U);
}
# 149 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512bwintrin.h" 1 3
# 30 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512bwintrin.h" 3
typedef unsigned int __mmask32;
typedef unsigned long long __mmask64;






static __inline __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_knot_mask32(__mmask32 __M)
{
  return __builtin_ia32_knotsi(__M);
}

static __inline __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_knot_mask64(__mmask64 __M)
{
  return __builtin_ia32_knotdi(__M);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_kand_mask32(__mmask32 __A, __mmask32 __B)
{
  return (__mmask32)__builtin_ia32_kandsi((__mmask32)__A, (__mmask32)__B);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_kand_mask64(__mmask64 __A, __mmask64 __B)
{
  return (__mmask64)__builtin_ia32_kanddi((__mmask64)__A, (__mmask64)__B);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_kandn_mask32(__mmask32 __A, __mmask32 __B)
{
  return (__mmask32)__builtin_ia32_kandnsi((__mmask32)__A, (__mmask32)__B);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_kandn_mask64(__mmask64 __A, __mmask64 __B)
{
  return (__mmask64)__builtin_ia32_kandndi((__mmask64)__A, (__mmask64)__B);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_kor_mask32(__mmask32 __A, __mmask32 __B)
{
  return (__mmask32)__builtin_ia32_korsi((__mmask32)__A, (__mmask32)__B);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_kor_mask64(__mmask64 __A, __mmask64 __B)
{
  return (__mmask64)__builtin_ia32_kordi((__mmask64)__A, (__mmask64)__B);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_kxnor_mask32(__mmask32 __A, __mmask32 __B)
{
  return (__mmask32)__builtin_ia32_kxnorsi((__mmask32)__A, (__mmask32)__B);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_kxnor_mask64(__mmask64 __A, __mmask64 __B)
{
  return (__mmask64)__builtin_ia32_kxnordi((__mmask64)__A, (__mmask64)__B);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_kxor_mask32(__mmask32 __A, __mmask32 __B)
{
  return (__mmask32)__builtin_ia32_kxorsi((__mmask32)__A, (__mmask32)__B);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_kxor_mask64(__mmask64 __A, __mmask64 __B)
{
  return (__mmask64)__builtin_ia32_kxordi((__mmask64)__A, (__mmask64)__B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_kortestc_mask32_u8(__mmask32 __A, __mmask32 __B)
{
  return (unsigned char)__builtin_ia32_kortestcsi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_kortestz_mask32_u8(__mmask32 __A, __mmask32 __B)
{
  return (unsigned char)__builtin_ia32_kortestzsi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_kortest_mask32_u8(__mmask32 __A, __mmask32 __B, unsigned char *__C) {
  *__C = (unsigned char)__builtin_ia32_kortestcsi(__A, __B);
  return (unsigned char)__builtin_ia32_kortestzsi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_kortestc_mask64_u8(__mmask64 __A, __mmask64 __B)
{
  return (unsigned char)__builtin_ia32_kortestcdi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_kortestz_mask64_u8(__mmask64 __A, __mmask64 __B)
{
  return (unsigned char)__builtin_ia32_kortestzdi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_kortest_mask64_u8(__mmask64 __A, __mmask64 __B, unsigned char *__C) {
  *__C = (unsigned char)__builtin_ia32_kortestcdi(__A, __B);
  return (unsigned char)__builtin_ia32_kortestzdi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_ktestc_mask32_u8(__mmask32 __A, __mmask32 __B)
{
  return (unsigned char)__builtin_ia32_ktestcsi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_ktestz_mask32_u8(__mmask32 __A, __mmask32 __B)
{
  return (unsigned char)__builtin_ia32_ktestzsi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_ktest_mask32_u8(__mmask32 __A, __mmask32 __B, unsigned char *__C) {
  *__C = (unsigned char)__builtin_ia32_ktestcsi(__A, __B);
  return (unsigned char)__builtin_ia32_ktestzsi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_ktestc_mask64_u8(__mmask64 __A, __mmask64 __B)
{
  return (unsigned char)__builtin_ia32_ktestcdi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_ktestz_mask64_u8(__mmask64 __A, __mmask64 __B)
{
  return (unsigned char)__builtin_ia32_ktestzdi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_ktest_mask64_u8(__mmask64 __A, __mmask64 __B, unsigned char *__C) {
  *__C = (unsigned char)__builtin_ia32_ktestcdi(__A, __B);
  return (unsigned char)__builtin_ia32_ktestzdi(__A, __B);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_kadd_mask32(__mmask32 __A, __mmask32 __B)
{
  return (__mmask32)__builtin_ia32_kaddsi((__mmask32)__A, (__mmask32)__B);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_kadd_mask64(__mmask64 __A, __mmask64 __B)
{
  return (__mmask64)__builtin_ia32_kadddi((__mmask64)__A, (__mmask64)__B);
}
# 206 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512bwintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_cvtmask32_u32(__mmask32 __A) {
  return (unsigned int)__builtin_ia32_kmovd((__mmask32)__A);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_cvtmask64_u64(__mmask64 __A) {
  return (unsigned long long)__builtin_ia32_kmovq((__mmask64)__A);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_cvtu32_mask32(unsigned int __A) {
  return (__mmask32)__builtin_ia32_kmovd((__mmask32)__A);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_cvtu64_mask64(unsigned long long __A) {
  return (__mmask64)__builtin_ia32_kmovq((__mmask64)__A);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_load_mask32(__mmask32 *__A) {
  return (__mmask32)__builtin_ia32_kmovd(*(__mmask32 *)__A);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_load_mask64(__mmask64 *__A) {
  return (__mmask64)__builtin_ia32_kmovq(*(__mmask64 *)__A);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_store_mask32(__mmask32 *__A, __mmask32 __B) {
  *(__mmask32 *)__A = __builtin_ia32_kmovd((__mmask32)__B);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_store_mask64(__mmask64 *__A, __mmask64 __B) {
  *(__mmask64 *)__A = __builtin_ia32_kmovq((__mmask64)__B);
}
# 388 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512bwintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_add_epi8 (__m512i __A, __m512i __B) {
  return (__m512i) ((__v64qu) __A + (__v64qu) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_add_epi8(__m512i __W, __mmask64 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                             (__v64qi)_mm512_add_epi8(__A, __B),
                                             (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_add_epi8(__mmask64 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                             (__v64qi)_mm512_add_epi8(__A, __B),
                                             (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_sub_epi8 (__m512i __A, __m512i __B) {
  return (__m512i) ((__v64qu) __A - (__v64qu) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_sub_epi8(__m512i __W, __mmask64 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                             (__v64qi)_mm512_sub_epi8(__A, __B),
                                             (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_sub_epi8(__mmask64 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                             (__v64qi)_mm512_sub_epi8(__A, __B),
                                             (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_add_epi16 (__m512i __A, __m512i __B) {
  return (__m512i) ((__v32hu) __A + (__v32hu) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_add_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_add_epi16(__A, __B),
                                             (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_add_epi16(__mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_add_epi16(__A, __B),
                                             (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_sub_epi16 (__m512i __A, __m512i __B) {
  return (__m512i) ((__v32hu) __A - (__v32hu) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_sub_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_sub_epi16(__A, __B),
                                             (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_sub_epi16(__mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_sub_epi16(__A, __B),
                                             (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mullo_epi16 (__m512i __A, __m512i __B) {
  return (__m512i) ((__v32hu) __A * (__v32hu) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_mullo_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_mullo_epi16(__A, __B),
                                             (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_mullo_epi16(__mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_mullo_epi16(__A, __B),
                                             (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_blend_epi8 (__mmask64 __U, __m512i __A, __m512i __W)
{
  return (__m512i) __builtin_ia32_selectb_512 ((__mmask64) __U,
              (__v64qi) __W,
              (__v64qi) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_blend_epi16 (__mmask32 __U, __m512i __A, __m512i __W)
{
  return (__m512i) __builtin_ia32_selectw_512 ((__mmask32) __U,
              (__v32hi) __W,
              (__v32hi) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_abs_epi8 (__m512i __A)
{
  return (__m512i)__builtin_elementwise_abs((__v64qs)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_abs_epi8 (__m512i __W, __mmask64 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                             (__v64qi)_mm512_abs_epi8(__A),
                                             (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_abs_epi8 (__mmask64 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                             (__v64qi)_mm512_abs_epi8(__A),
                                             (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_abs_epi16 (__m512i __A)
{
  return (__m512i)__builtin_elementwise_abs((__v32hi)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_abs_epi16 (__m512i __W, __mmask32 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_abs_epi16(__A),
                                             (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_abs_epi16 (__mmask32 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_abs_epi16(__A),
                                             (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_packs_epi32(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_packssdw512((__v16si)__A, (__v16si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_packs_epi32(__mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                       (__v32hi)_mm512_packs_epi32(__A, __B),
                                       (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_packs_epi32(__m512i __W, __mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                       (__v32hi)_mm512_packs_epi32(__A, __B),
                                       (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_packs_epi16(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_packsswb512((__v32hi)__A, (__v32hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_packs_epi16(__m512i __W, __mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                        (__v64qi)_mm512_packs_epi16(__A, __B),
                                        (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_packs_epi16(__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                        (__v64qi)_mm512_packs_epi16(__A, __B),
                                        (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_packus_epi32(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_packusdw512((__v16si) __A, (__v16si) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_packus_epi32(__mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                       (__v32hi)_mm512_packus_epi32(__A, __B),
                                       (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_packus_epi32(__m512i __W, __mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                       (__v32hi)_mm512_packus_epi32(__A, __B),
                                       (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_packus_epi16(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_packuswb512((__v32hi) __A, (__v32hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_packus_epi16(__m512i __W, __mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                        (__v64qi)_mm512_packus_epi16(__A, __B),
                                        (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_packus_epi16(__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                        (__v64qi)_mm512_packus_epi16(__A, __B),
                                        (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_adds_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_add_sat((__v64qs)__A, (__v64qs)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_adds_epi8 (__m512i __W, __mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_adds_epi8(__A, __B),
                                        (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_adds_epi8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_adds_epi8(__A, __B),
                                        (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_adds_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_add_sat((__v32hi)__A, (__v32hi)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_adds_epi16 (__m512i __W, __mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                        (__v32hi)_mm512_adds_epi16(__A, __B),
                                        (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_adds_epi16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                        (__v32hi)_mm512_adds_epi16(__A, __B),
                                        (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_adds_epu8 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_add_sat((__v64qu) __A, (__v64qu) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_adds_epu8 (__m512i __W, __mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_adds_epu8(__A, __B),
                                        (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_adds_epu8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_adds_epu8(__A, __B),
                                        (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_adds_epu16 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_add_sat((__v32hu) __A, (__v32hu) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_adds_epu16 (__m512i __W, __mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                        (__v32hi)_mm512_adds_epu16(__A, __B),
                                        (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_adds_epu16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                        (__v32hi)_mm512_adds_epu16(__A, __B),
                                        (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_avg_epu8 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_pavgb512((__v64qi)__A, (__v64qi)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_avg_epu8 (__m512i __W, __mmask64 __U, __m512i __A,
          __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
              (__v64qi)_mm512_avg_epu8(__A, __B),
              (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_avg_epu8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
              (__v64qi)_mm512_avg_epu8(__A, __B),
              (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_avg_epu16 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_pavgw512((__v32hi)__A, (__v32hi)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_avg_epu16 (__m512i __W, __mmask32 __U, __m512i __A,
           __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
              (__v32hi)_mm512_avg_epu16(__A, __B),
              (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_avg_epu16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
              (__v32hi)_mm512_avg_epu16(__A, __B),
              (__v32hi) _mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_max_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_max((__v64qs) __A, (__v64qs) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_max_epi8 (__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                             (__v64qi)_mm512_max_epi8(__A, __B),
                                             (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_max_epi8 (__m512i __W, __mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                             (__v64qi)_mm512_max_epi8(__A, __B),
                                             (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_max_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_max((__v32hi) __A, (__v32hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_max_epi16 (__mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                            (__v32hi)_mm512_max_epi16(__A, __B),
                                            (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_max_epi16 (__m512i __W, __mmask32 __M, __m512i __A,
           __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                            (__v32hi)_mm512_max_epi16(__A, __B),
                                            (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_max_epu8 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_max((__v64qu)__A, (__v64qu)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_max_epu8 (__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                             (__v64qi)_mm512_max_epu8(__A, __B),
                                             (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_max_epu8 (__m512i __W, __mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                             (__v64qi)_mm512_max_epu8(__A, __B),
                                             (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_max_epu16 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_max((__v32hu)__A, (__v32hu)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_max_epu16 (__mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                            (__v32hi)_mm512_max_epu16(__A, __B),
                                            (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_max_epu16 (__m512i __W, __mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                            (__v32hi)_mm512_max_epu16(__A, __B),
                                            (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_min_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_min((__v64qs) __A, (__v64qs) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_min_epi8 (__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                             (__v64qi)_mm512_min_epi8(__A, __B),
                                             (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_min_epi8 (__m512i __W, __mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                             (__v64qi)_mm512_min_epi8(__A, __B),
                                             (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_min_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_min((__v32hi) __A, (__v32hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_min_epi16 (__mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                            (__v32hi)_mm512_min_epi16(__A, __B),
                                            (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_min_epi16 (__m512i __W, __mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                            (__v32hi)_mm512_min_epi16(__A, __B),
                                            (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_min_epu8 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_min((__v64qu)__A, (__v64qu)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_min_epu8 (__mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                             (__v64qi)_mm512_min_epu8(__A, __B),
                                             (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_min_epu8 (__m512i __W, __mmask64 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                             (__v64qi)_mm512_min_epu8(__A, __B),
                                             (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_min_epu16 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_min((__v32hu)__A, (__v32hu)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_min_epu16 (__mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                            (__v32hi)_mm512_min_epu16(__A, __B),
                                            (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_min_epu16 (__m512i __W, __mmask32 __M, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                            (__v32hi)_mm512_min_epu16(__A, __B),
                                            (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_shuffle_epi8(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_pshufb512((__v64qi)__A,(__v64qi)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_shuffle_epi8(__m512i __W, __mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                         (__v64qi)_mm512_shuffle_epi8(__A, __B),
                                         (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_shuffle_epi8(__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                         (__v64qi)_mm512_shuffle_epi8(__A, __B),
                                         (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_subs_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_sub_sat((__v64qs)__A, (__v64qs)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_subs_epi8 (__m512i __W, __mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_subs_epi8(__A, __B),
                                        (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_subs_epi8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_subs_epi8(__A, __B),
                                        (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_subs_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_sub_sat((__v32hi)__A, (__v32hi)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_subs_epi16 (__m512i __W, __mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                        (__v32hi)_mm512_subs_epi16(__A, __B),
                                        (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_subs_epi16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                        (__v32hi)_mm512_subs_epi16(__A, __B),
                                        (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_subs_epu8 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_sub_sat((__v64qu) __A, (__v64qu) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_subs_epu8 (__m512i __W, __mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_subs_epu8(__A, __B),
                                        (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_subs_epu8 (__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_subs_epu8(__A, __B),
                                        (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_subs_epu16 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_elementwise_sub_sat((__v32hu) __A, (__v32hu) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_subs_epu16 (__m512i __W, __mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                        (__v32hi)_mm512_subs_epu16(__A, __B),
                                        (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_subs_epu16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                        (__v32hi)_mm512_subs_epu16(__A, __B),
                                        (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_permutex2var_epi16(__m512i __A, __m512i __I, __m512i __B)
{
  return (__m512i)__builtin_ia32_vpermi2varhi512((__v32hi)__A, (__v32hi)__I,
                                                 (__v32hi)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_permutex2var_epi16(__m512i __A, __mmask32 __U, __m512i __I,
                               __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512(__U,
                              (__v32hi)_mm512_permutex2var_epi16(__A, __I, __B),
                              (__v32hi)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask2_permutex2var_epi16(__m512i __A, __m512i __I, __mmask32 __U,
                                __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512(__U,
                              (__v32hi)_mm512_permutex2var_epi16(__A, __I, __B),
                              (__v32hi)__I);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_permutex2var_epi16(__mmask32 __U, __m512i __A, __m512i __I,
                                __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512(__U,
                              (__v32hi)_mm512_permutex2var_epi16(__A, __I, __B),
                              (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mulhrs_epi16(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_pmulhrsw512((__v32hi)__A, (__v32hi)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_mulhrs_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                         (__v32hi)_mm512_mulhrs_epi16(__A, __B),
                                         (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_mulhrs_epi16(__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                         (__v32hi)_mm512_mulhrs_epi16(__A, __B),
                                         (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mulhi_epi16(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_pmulhw512((__v32hi) __A, (__v32hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_mulhi_epi16(__m512i __W, __mmask32 __U, __m512i __A,
       __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_mulhi_epi16(__A, __B),
                                          (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_mulhi_epi16(__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_mulhi_epi16(__A, __B),
                                          (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mulhi_epu16(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_pmulhuw512((__v32hi) __A, (__v32hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_mulhi_epu16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_mulhi_epu16(__A, __B),
                                          (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_mulhi_epu16 (__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_mulhi_epu16(__A, __B),
                                          (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maddubs_epi16(__m512i __X, __m512i __Y) {
  return (__m512i)__builtin_ia32_pmaddubsw512((__v64qi)__X, (__v64qi)__Y);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_maddubs_epi16(__m512i __W, __mmask32 __U, __m512i __X,
                          __m512i __Y) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32) __U,
                                        (__v32hi)_mm512_maddubs_epi16(__X, __Y),
                                        (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_maddubs_epi16(__mmask32 __U, __m512i __X, __m512i __Y) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32) __U,
                                        (__v32hi)_mm512_maddubs_epi16(__X, __Y),
                                        (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_madd_epi16(__m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_pmaddwd512((__v32hi)__A, (__v32hi)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_madd_epi16(__m512i __W, __mmask16 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                           (__v16si)_mm512_madd_epi16(__A, __B),
                                           (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_madd_epi16(__mmask16 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                           (__v16si)_mm512_madd_epi16(__A, __B),
                                           (__v16si)_mm512_setzero_si512());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_cvtsepi16_epi8 (__m512i __A) {
  return (__m256i) __builtin_ia32_pmovswb512_mask ((__v32hi) __A,
               (__v32qi)_mm256_setzero_si256(),
               (__mmask32) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_cvtsepi16_epi8 (__m256i __O, __mmask32 __M, __m512i __A) {
  return (__m256i) __builtin_ia32_pmovswb512_mask ((__v32hi) __A,
               (__v32qi)__O,
               __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_cvtsepi16_epi8 (__mmask32 __M, __m512i __A) {
  return (__m256i) __builtin_ia32_pmovswb512_mask ((__v32hi) __A,
               (__v32qi) _mm256_setzero_si256(),
               __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_cvtusepi16_epi8 (__m512i __A) {
  return (__m256i) __builtin_ia32_pmovuswb512_mask ((__v32hi) __A,
                (__v32qi) _mm256_setzero_si256(),
                (__mmask32) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_cvtusepi16_epi8 (__m256i __O, __mmask32 __M, __m512i __A) {
  return (__m256i) __builtin_ia32_pmovuswb512_mask ((__v32hi) __A,
                (__v32qi) __O,
                __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_cvtusepi16_epi8 (__mmask32 __M, __m512i __A) {
  return (__m256i) __builtin_ia32_pmovuswb512_mask ((__v32hi) __A,
                (__v32qi) _mm256_setzero_si256(),
                __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_cvtepi16_epi8 (__m512i __A) {
  return (__m256i) __builtin_ia32_pmovwb512_mask ((__v32hi) __A,
              (__v32qi) _mm256_undefined_si256(),
              (__mmask32) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_cvtepi16_epi8 (__m256i __O, __mmask32 __M, __m512i __A) {
  return (__m256i) __builtin_ia32_pmovwb512_mask ((__v32hi) __A,
              (__v32qi) __O,
              __M);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_cvtepi16_epi8 (__mmask32 __M, __m512i __A) {
  return (__m256i) __builtin_ia32_pmovwb512_mask ((__v32hi) __A,
              (__v32qi) _mm256_setzero_si256(),
              __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_cvtepi16_storeu_epi8 (void * __P, __mmask32 __M, __m512i __A)
{
  __builtin_ia32_pmovwb512mem_mask ((__v32qi *) __P, (__v32hi) __A, __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_cvtsepi16_storeu_epi8 (void * __P, __mmask32 __M, __m512i __A)
{
  __builtin_ia32_pmovswb512mem_mask ((__v32qi *) __P, (__v32hi) __A, __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_cvtusepi16_storeu_epi8 (void * __P, __mmask32 __M, __m512i __A)
{
  __builtin_ia32_pmovuswb512mem_mask ((__v32qi *) __P, (__v32hi) __A, __M);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_unpackhi_epi8(__m512i __A, __m512i __B) {
  return (__m512i)__builtin_shufflevector((__v64qi)__A, (__v64qi)__B,
                                          8, 64+8, 9, 64+9,
                                          10, 64+10, 11, 64+11,
                                          12, 64+12, 13, 64+13,
                                          14, 64+14, 15, 64+15,
                                          24, 64+24, 25, 64+25,
                                          26, 64+26, 27, 64+27,
                                          28, 64+28, 29, 64+29,
                                          30, 64+30, 31, 64+31,
                                          40, 64+40, 41, 64+41,
                                          42, 64+42, 43, 64+43,
                                          44, 64+44, 45, 64+45,
                                          46, 64+46, 47, 64+47,
                                          56, 64+56, 57, 64+57,
                                          58, 64+58, 59, 64+59,
                                          60, 64+60, 61, 64+61,
                                          62, 64+62, 63, 64+63);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_unpackhi_epi8(__m512i __W, __mmask64 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_unpackhi_epi8(__A, __B),
                                        (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_unpackhi_epi8(__mmask64 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_unpackhi_epi8(__A, __B),
                                        (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_unpackhi_epi16(__m512i __A, __m512i __B) {
  return (__m512i)__builtin_shufflevector((__v32hi)__A, (__v32hi)__B,
                                          4, 32+4, 5, 32+5,
                                          6, 32+6, 7, 32+7,
                                          12, 32+12, 13, 32+13,
                                          14, 32+14, 15, 32+15,
                                          20, 32+20, 21, 32+21,
                                          22, 32+22, 23, 32+23,
                                          28, 32+28, 29, 32+29,
                                          30, 32+30, 31, 32+31);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_unpackhi_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                       (__v32hi)_mm512_unpackhi_epi16(__A, __B),
                                       (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_unpackhi_epi16(__mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                       (__v32hi)_mm512_unpackhi_epi16(__A, __B),
                                       (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_unpacklo_epi8(__m512i __A, __m512i __B) {
  return (__m512i)__builtin_shufflevector((__v64qi)__A, (__v64qi)__B,
                                          0, 64+0, 1, 64+1,
                                          2, 64+2, 3, 64+3,
                                          4, 64+4, 5, 64+5,
                                          6, 64+6, 7, 64+7,
                                          16, 64+16, 17, 64+17,
                                          18, 64+18, 19, 64+19,
                                          20, 64+20, 21, 64+21,
                                          22, 64+22, 23, 64+23,
                                          32, 64+32, 33, 64+33,
                                          34, 64+34, 35, 64+35,
                                          36, 64+36, 37, 64+37,
                                          38, 64+38, 39, 64+39,
                                          48, 64+48, 49, 64+49,
                                          50, 64+50, 51, 64+51,
                                          52, 64+52, 53, 64+53,
                                          54, 64+54, 55, 64+55);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_unpacklo_epi8(__m512i __W, __mmask64 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_unpacklo_epi8(__A, __B),
                                        (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_unpacklo_epi8(__mmask64 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__U,
                                        (__v64qi)_mm512_unpacklo_epi8(__A, __B),
                                        (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_unpacklo_epi16(__m512i __A, __m512i __B) {
  return (__m512i)__builtin_shufflevector((__v32hi)__A, (__v32hi)__B,
                                          0, 32+0, 1, 32+1,
                                          2, 32+2, 3, 32+3,
                                          8, 32+8, 9, 32+9,
                                          10, 32+10, 11, 32+11,
                                          16, 32+16, 17, 32+17,
                                          18, 32+18, 19, 32+19,
                                          24, 32+24, 25, 32+25,
                                          26, 32+26, 27, 32+27);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_unpacklo_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                       (__v32hi)_mm512_unpacklo_epi16(__A, __B),
                                       (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_unpacklo_epi16(__mmask32 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                       (__v32hi)_mm512_unpacklo_epi16(__A, __B),
                                       (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_cvtepi8_epi16(__m256i __A)
{


  return (__m512i)__builtin_convertvector((__v32qs)__A, __v32hi);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_cvtepi8_epi16(__m512i __W, __mmask32 __U, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_cvtepi8_epi16(__A),
                                             (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_cvtepi8_epi16(__mmask32 __U, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_cvtepi8_epi16(__A),
                                             (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_cvtepu8_epi16(__m256i __A)
{
  return (__m512i)__builtin_convertvector((__v32qu)__A, __v32hi);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_cvtepu8_epi16(__m512i __W, __mmask32 __U, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_cvtepu8_epi16(__A),
                                             (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_cvtepu8_epi16(__mmask32 __U, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                             (__v32hi)_mm512_cvtepu8_epi16(__A),
                                             (__v32hi)_mm512_setzero_si512());
}
# 1476 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512bwintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_sllv_epi16(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_psllv32hi((__v32hi) __A, (__v32hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_sllv_epi16 (__m512i __W, __mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                           (__v32hi)_mm512_sllv_epi16(__A, __B),
                                           (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_sllv_epi16(__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                           (__v32hi)_mm512_sllv_epi16(__A, __B),
                                           (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_sll_epi16(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_psllw512((__v32hi) __A, (__v8hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_sll_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_sll_epi16(__A, __B),
                                          (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_sll_epi16(__mmask32 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_sll_epi16(__A, __B),
                                          (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_slli_epi16(__m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_psllwi512((__v32hi)__A, (int)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_slli_epi16(__m512i __W, __mmask32 __U, __m512i __A,
                       unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                         (__v32hi)_mm512_slli_epi16(__A, __B),
                                         (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_slli_epi16(__mmask32 __U, __m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                         (__v32hi)_mm512_slli_epi16(__A, __B),
                                         (__v32hi)_mm512_setzero_si512());
}




static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_srlv_epi16(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_psrlv32hi((__v32hi)__A, (__v32hi)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_srlv_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                           (__v32hi)_mm512_srlv_epi16(__A, __B),
                                           (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_srlv_epi16(__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                           (__v32hi)_mm512_srlv_epi16(__A, __B),
                                           (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_srav_epi16(__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_psrav32hi((__v32hi)__A, (__v32hi)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_srav_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                           (__v32hi)_mm512_srav_epi16(__A, __B),
                                           (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_srav_epi16(__mmask32 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                           (__v32hi)_mm512_srav_epi16(__A, __B),
                                           (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_sra_epi16(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_psraw512((__v32hi) __A, (__v8hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_sra_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_sra_epi16(__A, __B),
                                          (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_sra_epi16(__mmask32 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_sra_epi16(__A, __B),
                                          (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_srai_epi16(__m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_psrawi512((__v32hi)__A, (int)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_srai_epi16(__m512i __W, __mmask32 __U, __m512i __A,
                       unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                         (__v32hi)_mm512_srai_epi16(__A, __B),
                                         (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_srai_epi16(__mmask32 __U, __m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                         (__v32hi)_mm512_srai_epi16(__A, __B),
                                         (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_srl_epi16(__m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_psrlw512((__v32hi) __A, (__v8hi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_srl_epi16(__m512i __W, __mmask32 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_srl_epi16(__A, __B),
                                          (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_srl_epi16(__mmask32 __U, __m512i __A, __m128i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                          (__v32hi)_mm512_srl_epi16(__A, __B),
                                          (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_srli_epi16(__m512i __A, unsigned int __B)
{
  return (__m512i)__builtin_ia32_psrlwi512((__v32hi)__A, (int)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_srli_epi16(__m512i __W, __mmask32 __U, __m512i __A,
                       unsigned int __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                         (__v32hi)_mm512_srli_epi16(__A, __B),
                                         (__v32hi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_srli_epi16(__mmask32 __U, __m512i __A, int __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__U,
                                         (__v32hi)_mm512_srli_epi16(__A, (unsigned int)__B),
                                         (__v32hi)_mm512_setzero_si512());
}




static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_mov_epi16 (__m512i __W, __mmask32 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_selectw_512 ((__mmask32) __U,
                (__v32hi) __A,
                (__v32hi) __W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_mov_epi16 (__mmask32 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_selectw_512 ((__mmask32) __U,
                (__v32hi) __A,
                (__v32hi) _mm512_setzero_si512 ());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_mov_epi8 (__m512i __W, __mmask64 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_selectb_512 ((__mmask64) __U,
                (__v64qi) __A,
                (__v64qi) __W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_mov_epi8 (__mmask64 __U, __m512i __A)
{
  return (__m512i) __builtin_ia32_selectb_512 ((__mmask64) __U,
                (__v64qi) __A,
                (__v64qi) _mm512_setzero_si512 ());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_set1_epi8 (__m512i __O, __mmask64 __M, char __A)
{
  return (__m512i) __builtin_ia32_selectb_512(__M,
                                              (__v64qi)_mm512_set1_epi8(__A),
                                              (__v64qi) __O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_set1_epi8 (__mmask64 __M, char __A)
{
  return (__m512i) __builtin_ia32_selectb_512(__M,
                                              (__v64qi) _mm512_set1_epi8(__A),
                                              (__v64qi) _mm512_setzero_si512());
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_kunpackd (__mmask64 __A, __mmask64 __B)
{
  return (__mmask64) __builtin_ia32_kunpckdi ((__mmask64) __A,
                (__mmask64) __B);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw")))
_mm512_kunpackw (__mmask32 __A, __mmask32 __B)
{
  return (__mmask32) __builtin_ia32_kunpcksi ((__mmask32) __A,
                (__mmask32) __B);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_loadu_epi16 (void const *__P)
{
  struct __loadu_epi16 {
    __m512i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_epi16*)__P)->__v;
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_loadu_epi16 (__m512i __W, __mmask32 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_loaddquhi512_mask ((const __v32hi *) __P,
                 (__v32hi) __W,
                 (__mmask32) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_loadu_epi16 (__mmask32 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_loaddquhi512_mask ((const __v32hi *) __P,
                 (__v32hi)
                 _mm512_setzero_si512 (),
                 (__mmask32) __U);
}

static __inline __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_loadu_epi8 (void const *__P)
{
  struct __loadu_epi8 {
    __m512i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_epi8*)__P)->__v;
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_loadu_epi8 (__m512i __W, __mmask64 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_loaddquqi512_mask ((const __v64qi *) __P,
                 (__v64qi) __W,
                 (__mmask64) __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_loadu_epi8 (__mmask64 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_loaddquqi512_mask ((const __v64qi *) __P,
                 (__v64qi)
                 _mm512_setzero_si512 (),
                 (__mmask64) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_storeu_epi16 (void *__P, __m512i __A)
{
  struct __storeu_epi16 {
    __m512i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_epi16*)__P)->__v = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_storeu_epi16 (void *__P, __mmask32 __U, __m512i __A)
{
  __builtin_ia32_storedquhi512_mask ((__v32hi *) __P,
             (__v32hi) __A,
             (__mmask32) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_storeu_epi8 (void *__P, __m512i __A)
{
  struct __storeu_epi8 {
    __m512i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_epi8*)__P)->__v = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_storeu_epi8 (void *__P, __mmask64 __U, __m512i __A)
{
  __builtin_ia32_storedquqi512_mask ((__v64qi *) __P,
             (__v64qi) __A,
             (__mmask64) __U);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_test_epi8_mask (__m512i __A, __m512i __B)
{
  return ((__mmask64)__builtin_ia32_cmpb512_mask((__v64qi)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v64qi)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_NE), (__mmask64)-1));

}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_test_epi8_mask (__mmask64 __U, __m512i __A, __m512i __B)
{
  return ((__mmask64)__builtin_ia32_cmpb512_mask((__v64qi)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v64qi)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_NE), (__mmask64)((__U))));

}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_test_epi16_mask (__m512i __A, __m512i __B)
{
  return ((__mmask32)__builtin_ia32_cmpw512_mask((__v32hi)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v32hi)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_NE), (__mmask32)-1));

}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_test_epi16_mask (__mmask32 __U, __m512i __A, __m512i __B)
{
  return ((__mmask32)__builtin_ia32_cmpw512_mask((__v32hi)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v32hi)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_NE), (__mmask32)((__U))));

}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_testn_epi8_mask (__m512i __A, __m512i __B)
{
  return ((__mmask64)__builtin_ia32_cmpb512_mask((__v64qi)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v64qi)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_EQ), (__mmask64)-1));
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_testn_epi8_mask (__mmask64 __U, __m512i __A, __m512i __B)
{
  return ((__mmask64)__builtin_ia32_cmpb512_mask((__v64qi)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v64qi)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_EQ), (__mmask64)((__U))));

}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_testn_epi16_mask (__m512i __A, __m512i __B)
{
  return ((__mmask32)__builtin_ia32_cmpw512_mask((__v32hi)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v32hi)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_EQ), (__mmask32)-1));

}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_testn_epi16_mask (__mmask32 __U, __m512i __A, __m512i __B)
{
  return ((__mmask32)__builtin_ia32_cmpw512_mask((__v32hi)(__m512i)((_mm512_and_epi32 (__A, __B))), (__v32hi)(__m512i)((_mm512_setzero_si512())), (int)(_MM_CMPINT_EQ), (__mmask32)((__U))));

}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_movepi8_mask (__m512i __A)
{
  return (__mmask64) __builtin_ia32_cvtb2mask512 ((__v64qi) __A);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_movepi16_mask (__m512i __A)
{
  return (__mmask32) __builtin_ia32_cvtw2mask512 ((__v32hi) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_movm_epi8 (__mmask64 __A)
{
  return (__m512i) __builtin_ia32_cvtmask2b512 (__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_movm_epi16 (__mmask32 __A)
{
  return (__m512i) __builtin_ia32_cvtmask2w512 (__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_broadcastb_epi8 (__m128i __A)
{
  return (__m512i)__builtin_shufflevector((__v16qi) __A, (__v16qi) __A,
                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_broadcastb_epi8 (__m512i __O, __mmask64 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectb_512(__M,
                                             (__v64qi) _mm512_broadcastb_epi8(__A),
                                             (__v64qi) __O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_broadcastb_epi8 (__mmask64 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectb_512(__M,
                                             (__v64qi) _mm512_broadcastb_epi8(__A),
                                             (__v64qi) _mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_set1_epi16 (__m512i __O, __mmask32 __M, short __A)
{
  return (__m512i) __builtin_ia32_selectw_512(__M,
                                              (__v32hi) _mm512_set1_epi16(__A),
                                              (__v32hi) __O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_set1_epi16 (__mmask32 __M, short __A)
{
  return (__m512i) __builtin_ia32_selectw_512(__M,
                                              (__v32hi) _mm512_set1_epi16(__A),
                                              (__v32hi) _mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_broadcastw_epi16 (__m128i __A)
{
  return (__m512i)__builtin_shufflevector((__v8hi) __A, (__v8hi) __A,
                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_broadcastw_epi16 (__m512i __O, __mmask32 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectw_512(__M,
                                             (__v32hi) _mm512_broadcastw_epi16(__A),
                                             (__v32hi) __O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_broadcastw_epi16 (__mmask32 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectw_512(__M,
                                             (__v32hi) _mm512_broadcastw_epi16(__A),
                                             (__v32hi) _mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_permutexvar_epi16 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_permvarhi512((__v32hi)__B, (__v32hi)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_maskz_permutexvar_epi16 (__mmask32 __M, __m512i __A,
        __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                    (__v32hi)_mm512_permutexvar_epi16(__A, __B),
                                    (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_mask_permutexvar_epi16 (__m512i __W, __mmask32 __M, __m512i __A,
             __m512i __B)
{
  return (__m512i)__builtin_ia32_selectw_512((__mmask32)__M,
                                    (__v32hi)_mm512_permutexvar_epi16(__A, __B),
                                    (__v32hi)__W);
}
# 2028 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512bwintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw"), __min_vector_width__(512)))
_mm512_sad_epu8 (__m512i __A, __m512i __B)
{
 return (__m512i) __builtin_ia32_psadbw512 ((__v64qi) __A,
               (__v64qi) __B);
}
# 154 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512bitalgintrin.h" 1 3
# 20 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512bitalgintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bitalg"), __min_vector_width__(512)))
_mm512_popcnt_epi16(__m512i __A)
{
  return (__m512i) __builtin_ia32_vpopcntw_512((__v32hi) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bitalg"), __min_vector_width__(512)))
_mm512_mask_popcnt_epi16(__m512i __A, __mmask32 __U, __m512i __B)
{
  return (__m512i) __builtin_ia32_selectw_512((__mmask32) __U,
              (__v32hi) _mm512_popcnt_epi16(__B),
              (__v32hi) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bitalg"), __min_vector_width__(512)))
_mm512_maskz_popcnt_epi16(__mmask32 __U, __m512i __B)
{
  return _mm512_mask_popcnt_epi16((__m512i) _mm512_setzero_si512(),
              __U,
              __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bitalg"), __min_vector_width__(512)))
_mm512_popcnt_epi8(__m512i __A)
{
  return (__m512i) __builtin_ia32_vpopcntb_512((__v64qi) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bitalg"), __min_vector_width__(512)))
_mm512_mask_popcnt_epi8(__m512i __A, __mmask64 __U, __m512i __B)
{
  return (__m512i) __builtin_ia32_selectb_512((__mmask64) __U,
              (__v64qi) _mm512_popcnt_epi8(__B),
              (__v64qi) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bitalg"), __min_vector_width__(512)))
_mm512_maskz_popcnt_epi8(__mmask64 __U, __m512i __B)
{
  return _mm512_mask_popcnt_epi8((__m512i) _mm512_setzero_si512(),
              __U,
              __B);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bitalg"), __min_vector_width__(512)))
_mm512_mask_bitshuffle_epi64_mask(__mmask64 __U, __m512i __A, __m512i __B)
{
  return (__mmask64) __builtin_ia32_vpshufbitqmb512_mask((__v64qi) __A,
              (__v64qi) __B,
              __U);
}

static __inline__ __mmask64 __attribute__((__always_inline__, __nodebug__, __target__("avx512bitalg"), __min_vector_width__(512)))
_mm512_bitshuffle_epi64_mask(__m512i __A, __m512i __B)
{
  return _mm512_mask_bitshuffle_epi64_mask((__mmask64) -1,
              __A,
              __B);
}
# 159 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512cdintrin.h" 1 3
# 20 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512cdintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd"), __min_vector_width__(512)))
_mm512_conflict_epi64 (__m512i __A)
{
  return (__m512i) __builtin_ia32_vpconflictdi_512 ((__v8di) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd"), __min_vector_width__(512)))
_mm512_mask_conflict_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_conflict_epi64(__A),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd"), __min_vector_width__(512)))
_mm512_maskz_conflict_epi64 (__mmask8 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_conflict_epi64(__A),
                                             (__v8di)_mm512_setzero_si512 ());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd"), __min_vector_width__(512)))
_mm512_conflict_epi32 (__m512i __A)
{
  return (__m512i) __builtin_ia32_vpconflictsi_512 ((__v16si) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd"), __min_vector_width__(512)))
_mm512_mask_conflict_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                            (__v16si)_mm512_conflict_epi32(__A),
                                            (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd"), __min_vector_width__(512)))
_mm512_maskz_conflict_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                            (__v16si)_mm512_conflict_epi32(__A),
                                            (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd"), __min_vector_width__(512)))
_mm512_lzcnt_epi32 (__m512i __A)
{
  return (__m512i) __builtin_ia32_vplzcntd_512 ((__v16si) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd"), __min_vector_width__(512)))
_mm512_mask_lzcnt_epi32 (__m512i __W, __mmask16 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_lzcnt_epi32(__A),
                                             (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd"), __min_vector_width__(512)))
_mm512_maskz_lzcnt_epi32 (__mmask16 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__U,
                                             (__v16si)_mm512_lzcnt_epi32(__A),
                                             (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd"), __min_vector_width__(512)))
_mm512_lzcnt_epi64 (__m512i __A)
{
  return (__m512i) __builtin_ia32_vplzcntq_512 ((__v8di) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd"), __min_vector_width__(512)))
_mm512_mask_lzcnt_epi64 (__m512i __W, __mmask8 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_lzcnt_epi64(__A),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd"), __min_vector_width__(512)))
_mm512_maskz_lzcnt_epi64 (__mmask8 __U, __m512i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_lzcnt_epi64(__A),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd"), __min_vector_width__(512)))
_mm512_broadcastmb_epi64 (__mmask8 __A)
{
  return (__m512i) _mm512_set1_epi64((long long) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512cd"), __min_vector_width__(512)))
_mm512_broadcastmw_epi32 (__mmask16 __A)
{
  return (__m512i) _mm512_set1_epi32((int) __A);

}
# 164 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vpopcntdqintrin.h" 1 3
# 22 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vpopcntdqintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq"), __min_vector_width__(512))) _mm512_popcnt_epi64(__m512i __A) {
  return (__m512i)__builtin_ia32_vpopcntq_512((__v8di)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq"), __min_vector_width__(512)))
_mm512_mask_popcnt_epi64(__m512i __W, __mmask8 __U, __m512i __A) {
  return (__m512i)__builtin_ia32_selectq_512(
      (__mmask8)__U, (__v8di)_mm512_popcnt_epi64(__A), (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq"), __min_vector_width__(512)))
_mm512_maskz_popcnt_epi64(__mmask8 __U, __m512i __A) {
  return _mm512_mask_popcnt_epi64((__m512i)_mm512_setzero_si512(), __U, __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq"), __min_vector_width__(512))) _mm512_popcnt_epi32(__m512i __A) {
  return (__m512i)__builtin_ia32_vpopcntd_512((__v16si)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq"), __min_vector_width__(512)))
_mm512_mask_popcnt_epi32(__m512i __W, __mmask16 __U, __m512i __A) {
  return (__m512i)__builtin_ia32_selectd_512(
      (__mmask16)__U, (__v16si)_mm512_popcnt_epi32(__A), (__v16si)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq"), __min_vector_width__(512)))
_mm512_maskz_popcnt_epi32(__mmask16 __U, __m512i __A) {
  return _mm512_mask_popcnt_epi32((__m512i)_mm512_setzero_si512(), __U, __A);
}
# 169 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3





# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vpopcntdqvlintrin.h" 1 3
# 39 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vpopcntdqvlintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl"), __min_vector_width__(128)))
_mm_popcnt_epi64(__m128i __A) {
  return (__m128i)__builtin_ia32_vpopcntq_128((__v2di)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl"), __min_vector_width__(128)))
_mm_mask_popcnt_epi64(__m128i __W, __mmask8 __U, __m128i __A) {
  return (__m128i)__builtin_ia32_selectq_128(
      (__mmask8)__U, (__v2di)_mm_popcnt_epi64(__A), (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl"), __min_vector_width__(128)))
_mm_maskz_popcnt_epi64(__mmask8 __U, __m128i __A) {
  return _mm_mask_popcnt_epi64((__m128i)_mm_setzero_si128(), __U, __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl"), __min_vector_width__(128)))
_mm_popcnt_epi32(__m128i __A) {
  return (__m128i)__builtin_ia32_vpopcntd_128((__v4si)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl"), __min_vector_width__(128)))
_mm_mask_popcnt_epi32(__m128i __W, __mmask8 __U, __m128i __A) {
  return (__m128i)__builtin_ia32_selectd_128(
      (__mmask8)__U, (__v4si)_mm_popcnt_epi32(__A), (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl"), __min_vector_width__(128)))
_mm_maskz_popcnt_epi32(__mmask8 __U, __m128i __A) {
  return _mm_mask_popcnt_epi32((__m128i)_mm_setzero_si128(), __U, __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl"), __min_vector_width__(256)))
_mm256_popcnt_epi64(__m256i __A) {
  return (__m256i)__builtin_ia32_vpopcntq_256((__v4di)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl"), __min_vector_width__(256)))
_mm256_mask_popcnt_epi64(__m256i __W, __mmask8 __U, __m256i __A) {
  return (__m256i)__builtin_ia32_selectq_256(
      (__mmask8)__U, (__v4di)_mm256_popcnt_epi64(__A), (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl"), __min_vector_width__(256)))
_mm256_maskz_popcnt_epi64(__mmask8 __U, __m256i __A) {
  return _mm256_mask_popcnt_epi64((__m256i)_mm256_setzero_si256(), __U, __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl"), __min_vector_width__(256)))
_mm256_popcnt_epi32(__m256i __A) {
  return (__m256i)__builtin_ia32_vpopcntd_256((__v8si)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl"), __min_vector_width__(256)))
_mm256_mask_popcnt_epi32(__m256i __W, __mmask8 __U, __m256i __A) {
  return (__m256i)__builtin_ia32_selectd_256(
      (__mmask8)__U, (__v8si)_mm256_popcnt_epi32(__A), (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vpopcntdq,avx512vl"), __min_vector_width__(256)))
_mm256_maskz_popcnt_epi32(__mmask8 __U, __m256i __A) {
  return _mm256_mask_popcnt_epi32((__m256i)_mm256_setzero_si256(), __U, __A);
}
# 175 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vnniintrin.h" 1 3
# 21 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vnniintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni"), __min_vector_width__(512)))
_mm512_dpbusd_epi32(__m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_vpdpbusd512((__v16si)__S, (__v16si)__A,
                                             (__v16si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni"), __min_vector_width__(512)))
_mm512_mask_dpbusd_epi32(__m512i __S, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                    (__v16si)_mm512_dpbusd_epi32(__S, __A, __B),
                                    (__v16si)__S);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni"), __min_vector_width__(512)))
_mm512_maskz_dpbusd_epi32(__mmask16 __U, __m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                    (__v16si)_mm512_dpbusd_epi32(__S, __A, __B),
                                    (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni"), __min_vector_width__(512)))
_mm512_dpbusds_epi32(__m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_vpdpbusds512((__v16si)__S, (__v16si)__A,
                                              (__v16si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni"), __min_vector_width__(512)))
_mm512_mask_dpbusds_epi32(__m512i __S, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                   (__v16si)_mm512_dpbusds_epi32(__S, __A, __B),
                                   (__v16si)__S);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni"), __min_vector_width__(512)))
_mm512_maskz_dpbusds_epi32(__mmask16 __U, __m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                   (__v16si)_mm512_dpbusds_epi32(__S, __A, __B),
                                   (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni"), __min_vector_width__(512)))
_mm512_dpwssd_epi32(__m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_vpdpwssd512((__v16si)__S, (__v16si)__A,
                                             (__v16si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni"), __min_vector_width__(512)))
_mm512_mask_dpwssd_epi32(__m512i __S, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                    (__v16si)_mm512_dpwssd_epi32(__S, __A, __B),
                                    (__v16si)__S);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni"), __min_vector_width__(512)))
_mm512_maskz_dpwssd_epi32(__mmask16 __U, __m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                    (__v16si)_mm512_dpwssd_epi32(__S, __A, __B),
                                    (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni"), __min_vector_width__(512)))
_mm512_dpwssds_epi32(__m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_vpdpwssds512((__v16si)__S, (__v16si)__A,
                                              (__v16si)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni"), __min_vector_width__(512)))
_mm512_mask_dpwssds_epi32(__m512i __S, __mmask16 __U, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                   (__v16si)_mm512_dpwssds_epi32(__S, __A, __B),
                                   (__v16si)__S);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vnni"), __min_vector_width__(512)))
_mm512_maskz_dpwssds_epi32(__mmask16 __U, __m512i __S, __m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                   (__v16si)_mm512_dpwssds_epi32(__S, __A, __B),
                                   (__v16si)_mm512_setzero_si512());
}
# 180 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3





# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlvnniintrin.h" 1 3
# 187 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlvnniintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni"), __min_vector_width__(256)))
_mm256_mask_dpbusd_epi32(__m256i __S, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                     (__v8si)((__m256i)__builtin_ia32_vpdpbusd256((__v8si)(__S), (__v8si)(__A), (__v8si)(__B))),
                                     (__v8si)__S);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni"), __min_vector_width__(256)))
_mm256_maskz_dpbusd_epi32(__mmask8 __U, __m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                     (__v8si)((__m256i)__builtin_ia32_vpdpbusd256((__v8si)(__S), (__v8si)(__A), (__v8si)(__B))),
                                     (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni"), __min_vector_width__(256)))
_mm256_mask_dpbusds_epi32(__m256i __S, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                    (__v8si)((__m256i)__builtin_ia32_vpdpbusds256((__v8si)(__S), (__v8si)(__A), (__v8si)(__B))),
                                    (__v8si)__S);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni"), __min_vector_width__(256)))
_mm256_maskz_dpbusds_epi32(__mmask8 __U, __m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                     (__v8si)((__m256i)__builtin_ia32_vpdpbusds256((__v8si)(__S), (__v8si)(__A), (__v8si)(__B))),
                                     (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni"), __min_vector_width__(256)))
_mm256_mask_dpwssd_epi32(__m256i __S, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                     (__v8si)((__m256i)__builtin_ia32_vpdpwssd256((__v8si)(__S), (__v8si)(__A), (__v8si)(__B))),
                                     (__v8si)__S);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni"), __min_vector_width__(256)))
_mm256_maskz_dpwssd_epi32(__mmask8 __U, __m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                     (__v8si)((__m256i)__builtin_ia32_vpdpwssd256((__v8si)(__S), (__v8si)(__A), (__v8si)(__B))),
                                     (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni"), __min_vector_width__(256)))
_mm256_mask_dpwssds_epi32(__m256i __S, __mmask8 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                    (__v8si)((__m256i)__builtin_ia32_vpdpwssds256((__v8si)(__S), (__v8si)(__A), (__v8si)(__B))),
                                    (__v8si)__S);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni"), __min_vector_width__(256)))
_mm256_maskz_dpwssds_epi32(__mmask8 __U, __m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                    (__v8si)((__m256i)__builtin_ia32_vpdpwssds256((__v8si)(__S), (__v8si)(__A), (__v8si)(__B))),
                                    (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni"), __min_vector_width__(128)))
_mm_mask_dpbusd_epi32(__m128i __S, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                        (__v4si)((__m128i)__builtin_ia32_vpdpbusd128((__v4si)(__S), (__v4si)(__A), (__v4si)(__B))),
                                        (__v4si)__S);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni"), __min_vector_width__(128)))
_mm_maskz_dpbusd_epi32(__mmask8 __U, __m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                        (__v4si)((__m128i)__builtin_ia32_vpdpbusd128((__v4si)(__S), (__v4si)(__A), (__v4si)(__B))),
                                        (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni"), __min_vector_width__(128)))
_mm_mask_dpbusds_epi32(__m128i __S, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                       (__v4si)((__m128i)__builtin_ia32_vpdpbusds128((__v4si)(__S), (__v4si)(__A), (__v4si)(__B))),
                                       (__v4si)__S);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni"), __min_vector_width__(128)))
_mm_maskz_dpbusds_epi32(__mmask8 __U, __m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                       (__v4si)((__m128i)__builtin_ia32_vpdpbusds128((__v4si)(__S), (__v4si)(__A), (__v4si)(__B))),
                                       (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni"), __min_vector_width__(128)))
_mm_mask_dpwssd_epi32(__m128i __S, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                        (__v4si)((__m128i)__builtin_ia32_vpdpwssd128((__v4si)(__S), (__v4si)(__A), (__v4si)(__B))),
                                        (__v4si)__S);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni"), __min_vector_width__(128)))
_mm_maskz_dpwssd_epi32(__mmask8 __U, __m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                        (__v4si)((__m128i)__builtin_ia32_vpdpwssd128((__v4si)(__S), (__v4si)(__A), (__v4si)(__B))),
                                        (__v4si)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni"), __min_vector_width__(128)))
_mm_mask_dpwssds_epi32(__m128i __S, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                       (__v4si)((__m128i)__builtin_ia32_vpdpwssds128((__v4si)(__S), (__v4si)(__A), (__v4si)(__B))),
                                       (__v4si)__S);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vnni"), __min_vector_width__(128)))
_mm_maskz_dpwssds_epi32(__mmask8 __U, __m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                       (__v4si)((__m128i)__builtin_ia32_vpdpwssds128((__v4si)(__S), (__v4si)(__A), (__v4si)(__B))),
                                       (__v4si)_mm_setzero_si128());
}
# 186 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxvnniintrin.h" 1 3
# 63 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxvnniintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnni"), __min_vector_width__(256)))
_mm256_dpbusd_avx_epi32(__m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_vpdpbusd256((__v8si)__S, (__v8si)__A, (__v8si)__B);
}
# 86 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxvnniintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnni"), __min_vector_width__(256)))
_mm256_dpbusds_avx_epi32(__m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_vpdpbusds256((__v8si)__S, (__v8si)__A, (__v8si)__B);
}
# 107 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxvnniintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnni"), __min_vector_width__(256)))
_mm256_dpwssd_avx_epi32(__m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_vpdpwssd256((__v8si)__S, (__v8si)__A, (__v8si)__B);
}
# 128 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxvnniintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnni"), __min_vector_width__(256)))
_mm256_dpwssds_avx_epi32(__m256i __S, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_vpdpwssds256((__v8si)__S, (__v8si)__A, (__v8si)__B);
}
# 151 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxvnniintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnni"), __min_vector_width__(128)))
_mm_dpbusd_avx_epi32(__m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpdpbusd128((__v4si)__S, (__v4si)__A, (__v4si)__B);
}
# 174 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxvnniintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnni"), __min_vector_width__(128)))
_mm_dpbusds_avx_epi32(__m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpdpbusds128((__v4si)__S, (__v4si)__A, (__v4si)__B);
}
# 195 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxvnniintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnni"), __min_vector_width__(128)))
_mm_dpwssd_avx_epi32(__m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpdpwssd128((__v4si)__S, (__v4si)__A, (__v4si)__B);
}
# 216 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxvnniintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnni"), __min_vector_width__(128)))
_mm_dpwssds_avx_epi32(__m128i __S, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpdpwssds128((__v4si)__S, (__v4si)__A, (__v4si)__B);
}
# 191 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512dqintrin.h" 1 3
# 35 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512dqintrin.h" 3
static __inline __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_knot_mask8(__mmask8 __M)
{
  return __builtin_ia32_knotqi(__M);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_kand_mask8(__mmask8 __A, __mmask8 __B)
{
  return (__mmask8)__builtin_ia32_kandqi((__mmask8)__A, (__mmask8)__B);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_kandn_mask8(__mmask8 __A, __mmask8 __B)
{
  return (__mmask8)__builtin_ia32_kandnqi((__mmask8)__A, (__mmask8)__B);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_kor_mask8(__mmask8 __A, __mmask8 __B)
{
  return (__mmask8)__builtin_ia32_korqi((__mmask8)__A, (__mmask8)__B);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_kxnor_mask8(__mmask8 __A, __mmask8 __B)
{
  return (__mmask8)__builtin_ia32_kxnorqi((__mmask8)__A, (__mmask8)__B);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_kxor_mask8(__mmask8 __A, __mmask8 __B)
{
  return (__mmask8)__builtin_ia32_kxorqi((__mmask8)__A, (__mmask8)__B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_kortestc_mask8_u8(__mmask8 __A, __mmask8 __B)
{
  return (unsigned char)__builtin_ia32_kortestcqi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_kortestz_mask8_u8(__mmask8 __A, __mmask8 __B)
{
  return (unsigned char)__builtin_ia32_kortestzqi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_kortest_mask8_u8(__mmask8 __A, __mmask8 __B, unsigned char *__C) {
  *__C = (unsigned char)__builtin_ia32_kortestcqi(__A, __B);
  return (unsigned char)__builtin_ia32_kortestzqi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_ktestc_mask8_u8(__mmask8 __A, __mmask8 __B)
{
  return (unsigned char)__builtin_ia32_ktestcqi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_ktestz_mask8_u8(__mmask8 __A, __mmask8 __B)
{
  return (unsigned char)__builtin_ia32_ktestzqi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_ktest_mask8_u8(__mmask8 __A, __mmask8 __B, unsigned char *__C) {
  *__C = (unsigned char)__builtin_ia32_ktestcqi(__A, __B);
  return (unsigned char)__builtin_ia32_ktestzqi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_ktestc_mask16_u8(__mmask16 __A, __mmask16 __B)
{
  return (unsigned char)__builtin_ia32_ktestchi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_ktestz_mask16_u8(__mmask16 __A, __mmask16 __B)
{
  return (unsigned char)__builtin_ia32_ktestzhi(__A, __B);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_ktest_mask16_u8(__mmask16 __A, __mmask16 __B, unsigned char *__C) {
  *__C = (unsigned char)__builtin_ia32_ktestchi(__A, __B);
  return (unsigned char)__builtin_ia32_ktestzhi(__A, __B);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_kadd_mask8(__mmask8 __A, __mmask8 __B)
{
  return (__mmask8)__builtin_ia32_kaddqi((__mmask8)__A, (__mmask8)__B);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_kadd_mask16(__mmask16 __A, __mmask16 __B)
{
  return (__mmask16)__builtin_ia32_kaddhi((__mmask16)__A, (__mmask16)__B);
}







static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_cvtmask8_u32(__mmask8 __A) {
  return (unsigned int)__builtin_ia32_kmovb((__mmask8)__A);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_cvtu32_mask8(unsigned int __A) {
  return (__mmask8)__builtin_ia32_kmovb((__mmask8)__A);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_load_mask8(__mmask8 *__A) {
  return (__mmask8)__builtin_ia32_kmovb(*(__mmask8 *)__A);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512dq")))
_store_mask8(__mmask8 *__A, __mmask8 __B) {
  *(__mmask8 *)__A = __builtin_ia32_kmovb((__mmask8)__B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mullo_epi64 (__m512i __A, __m512i __B) {
  return (__m512i) ((__v8du) __A * (__v8du) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_mullo_epi64(__m512i __W, __mmask8 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_mullo_epi64(__A, __B),
                                             (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_mullo_epi64(__mmask8 __U, __m512i __A, __m512i __B) {
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__U,
                                             (__v8di)_mm512_mullo_epi64(__A, __B),
                                             (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_xor_pd(__m512d __A, __m512d __B) {
  return (__m512d)((__v8du)__A ^ (__v8du)__B);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_xor_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_xor_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_xor_pd(__mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_xor_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_xor_ps (__m512 __A, __m512 __B) {
  return (__m512)((__v16su)__A ^ (__v16su)__B);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_xor_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_xor_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_xor_ps(__mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_xor_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_or_pd(__m512d __A, __m512d __B) {
  return (__m512d)((__v8du)__A | (__v8du)__B);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_or_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_or_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_or_pd(__mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_or_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_or_ps(__m512 __A, __m512 __B) {
  return (__m512)((__v16su)__A | (__v16su)__B);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_or_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_or_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_or_ps(__mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_or_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_and_pd(__m512d __A, __m512d __B) {
  return (__m512d)((__v8du)__A & (__v8du)__B);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_and_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_and_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_and_pd(__mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_and_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_and_ps(__m512 __A, __m512 __B) {
  return (__m512)((__v16su)__A & (__v16su)__B);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_and_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_and_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_and_ps(__mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_and_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_andnot_pd(__m512d __A, __m512d __B) {
  return (__m512d)(~(__v8du)__A & (__v8du)__B);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_andnot_pd(__m512d __W, __mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_andnot_pd(__A, __B),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_andnot_pd(__mmask8 __U, __m512d __A, __m512d __B) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_andnot_pd(__A, __B),
                                              (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_andnot_ps(__m512 __A, __m512 __B) {
  return (__m512)(~(__v16su)__A & (__v16su)__B);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_andnot_ps(__m512 __W, __mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_andnot_ps(__A, __B),
                                             (__v16sf)__W);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_andnot_ps(__mmask16 __U, __m512 __A, __m512 __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                             (__v16sf)_mm512_andnot_ps(__A, __B),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_cvtpd_epi64 (__m512d __A) {
  return (__m512i) __builtin_ia32_cvtpd2qq512_mask ((__v8df) __A,
                (__v8di) _mm512_setzero_si512(),
                (__mmask8) -1,
                0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_cvtpd_epi64 (__m512i __W, __mmask8 __U, __m512d __A) {
  return (__m512i) __builtin_ia32_cvtpd2qq512_mask ((__v8df) __A,
                (__v8di) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_cvtpd_epi64 (__mmask8 __U, __m512d __A) {
  return (__m512i) __builtin_ia32_cvtpd2qq512_mask ((__v8df) __A,
                (__v8di) _mm512_setzero_si512(),
                (__mmask8) __U,
                0x04);
}
# 373 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512dqintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_cvtpd_epu64 (__m512d __A) {
  return (__m512i) __builtin_ia32_cvtpd2uqq512_mask ((__v8df) __A,
                 (__v8di) _mm512_setzero_si512(),
                 (__mmask8) -1,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_cvtpd_epu64 (__m512i __W, __mmask8 __U, __m512d __A) {
  return (__m512i) __builtin_ia32_cvtpd2uqq512_mask ((__v8df) __A,
                 (__v8di) __W,
                 (__mmask8) __U,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_cvtpd_epu64 (__mmask8 __U, __m512d __A) {
  return (__m512i) __builtin_ia32_cvtpd2uqq512_mask ((__v8df) __A,
                 (__v8di) _mm512_setzero_si512(),
                 (__mmask8) __U,
                 0x04);
}
# 412 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512dqintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_cvtps_epi64 (__m256 __A) {
  return (__m512i) __builtin_ia32_cvtps2qq512_mask ((__v8sf) __A,
                (__v8di) _mm512_setzero_si512(),
                (__mmask8) -1,
                0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_cvtps_epi64 (__m512i __W, __mmask8 __U, __m256 __A) {
  return (__m512i) __builtin_ia32_cvtps2qq512_mask ((__v8sf) __A,
                (__v8di) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_cvtps_epi64 (__mmask8 __U, __m256 __A) {
  return (__m512i) __builtin_ia32_cvtps2qq512_mask ((__v8sf) __A,
                (__v8di) _mm512_setzero_si512(),
                (__mmask8) __U,
                0x04);
}
# 451 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512dqintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_cvtps_epu64 (__m256 __A) {
  return (__m512i) __builtin_ia32_cvtps2uqq512_mask ((__v8sf) __A,
                 (__v8di) _mm512_setzero_si512(),
                 (__mmask8) -1,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_cvtps_epu64 (__m512i __W, __mmask8 __U, __m256 __A) {
  return (__m512i) __builtin_ia32_cvtps2uqq512_mask ((__v8sf) __A,
                 (__v8di) __W,
                 (__mmask8) __U,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_cvtps_epu64 (__mmask8 __U, __m256 __A) {
  return (__m512i) __builtin_ia32_cvtps2uqq512_mask ((__v8sf) __A,
                 (__v8di) _mm512_setzero_si512(),
                 (__mmask8) __U,
                 0x04);
}
# 491 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512dqintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_cvtepi64_pd (__m512i __A) {
  return (__m512d)__builtin_convertvector((__v8di)__A, __v8df);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_cvtepi64_pd (__m512d __W, __mmask8 __U, __m512i __A) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_cvtepi64_pd(__A),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_cvtepi64_pd (__mmask8 __U, __m512i __A) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_cvtepi64_pd(__A),
                                              (__v8df)_mm512_setzero_pd());
}
# 525 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512dqintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_cvtepi64_ps (__m512i __A) {
  return (__m256) __builtin_ia32_cvtqq2ps512_mask ((__v8di) __A,
               (__v8sf) _mm256_setzero_ps(),
               (__mmask8) -1,
               0x04);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_cvtepi64_ps (__m256 __W, __mmask8 __U, __m512i __A) {
  return (__m256) __builtin_ia32_cvtqq2ps512_mask ((__v8di) __A,
               (__v8sf) __W,
               (__mmask8) __U,
               0x04);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_cvtepi64_ps (__mmask8 __U, __m512i __A) {
  return (__m256) __builtin_ia32_cvtqq2ps512_mask ((__v8di) __A,
               (__v8sf) _mm256_setzero_ps(),
               (__mmask8) __U,
               0x04);
}
# 565 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512dqintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_cvttpd_epi64 (__m512d __A) {
  return (__m512i) __builtin_ia32_cvttpd2qq512_mask ((__v8df) __A,
                 (__v8di) _mm512_setzero_si512(),
                 (__mmask8) -1,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_cvttpd_epi64 (__m512i __W, __mmask8 __U, __m512d __A) {
  return (__m512i) __builtin_ia32_cvttpd2qq512_mask ((__v8df) __A,
                 (__v8di) __W,
                 (__mmask8) __U,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_cvttpd_epi64 (__mmask8 __U, __m512d __A) {
  return (__m512i) __builtin_ia32_cvttpd2qq512_mask ((__v8df) __A,
                 (__v8di) _mm512_setzero_si512(),
                 (__mmask8) __U,
                 0x04);
}
# 604 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512dqintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_cvttpd_epu64 (__m512d __A) {
  return (__m512i) __builtin_ia32_cvttpd2uqq512_mask ((__v8df) __A,
                  (__v8di) _mm512_setzero_si512(),
                  (__mmask8) -1,
                  0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_cvttpd_epu64 (__m512i __W, __mmask8 __U, __m512d __A) {
  return (__m512i) __builtin_ia32_cvttpd2uqq512_mask ((__v8df) __A,
                  (__v8di) __W,
                  (__mmask8) __U,
                  0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_cvttpd_epu64 (__mmask8 __U, __m512d __A) {
  return (__m512i) __builtin_ia32_cvttpd2uqq512_mask ((__v8df) __A,
                  (__v8di) _mm512_setzero_si512(),
                  (__mmask8) __U,
                  0x04);
}
# 643 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512dqintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_cvttps_epi64 (__m256 __A) {
  return (__m512i) __builtin_ia32_cvttps2qq512_mask ((__v8sf) __A,
                 (__v8di) _mm512_setzero_si512(),
                 (__mmask8) -1,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_cvttps_epi64 (__m512i __W, __mmask8 __U, __m256 __A) {
  return (__m512i) __builtin_ia32_cvttps2qq512_mask ((__v8sf) __A,
                 (__v8di) __W,
                 (__mmask8) __U,
                 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_cvttps_epi64 (__mmask8 __U, __m256 __A) {
  return (__m512i) __builtin_ia32_cvttps2qq512_mask ((__v8sf) __A,
                 (__v8di) _mm512_setzero_si512(),
                 (__mmask8) __U,
                 0x04);
}
# 682 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512dqintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_cvttps_epu64 (__m256 __A) {
  return (__m512i) __builtin_ia32_cvttps2uqq512_mask ((__v8sf) __A,
                  (__v8di) _mm512_setzero_si512(),
                  (__mmask8) -1,
                  0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_cvttps_epu64 (__m512i __W, __mmask8 __U, __m256 __A) {
  return (__m512i) __builtin_ia32_cvttps2uqq512_mask ((__v8sf) __A,
                  (__v8di) __W,
                  (__mmask8) __U,
                  0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_cvttps_epu64 (__mmask8 __U, __m256 __A) {
  return (__m512i) __builtin_ia32_cvttps2uqq512_mask ((__v8sf) __A,
                  (__v8di) _mm512_setzero_si512(),
                  (__mmask8) __U,
                  0x04);
}
# 721 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512dqintrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_cvtepu64_pd (__m512i __A) {
  return (__m512d)__builtin_convertvector((__v8du)__A, __v8df);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_cvtepu64_pd (__m512d __W, __mmask8 __U, __m512i __A) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_cvtepu64_pd(__A),
                                              (__v8df)__W);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_cvtepu64_pd (__mmask8 __U, __m512i __A) {
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__U,
                                              (__v8df)_mm512_cvtepu64_pd(__A),
                                              (__v8df)_mm512_setzero_pd());
}
# 757 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512dqintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_cvtepu64_ps (__m512i __A) {
  return (__m256) __builtin_ia32_cvtuqq2ps512_mask ((__v8di) __A,
                (__v8sf) _mm256_setzero_ps(),
                (__mmask8) -1,
                0x04);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_cvtepu64_ps (__m256 __W, __mmask8 __U, __m512i __A) {
  return (__m256) __builtin_ia32_cvtuqq2ps512_mask ((__v8di) __A,
                (__v8sf) __W,
                (__mmask8) __U,
                0x04);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_cvtepu64_ps (__mmask8 __U, __m512i __A) {
  return (__m256) __builtin_ia32_cvtuqq2ps512_mask ((__v8di) __A,
                (__v8sf) _mm256_setzero_ps(),
                (__mmask8) __U,
                0x04);
}
# 1067 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512dqintrin.h" 3
static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_movepi32_mask (__m512i __A)
{
  return (__mmask16) __builtin_ia32_cvtd2mask512 ((__v16si) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_movm_epi32 (__mmask16 __A)
{
  return (__m512i) __builtin_ia32_cvtmask2d512 (__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_movm_epi64 (__mmask8 __A)
{
  return (__m512i) __builtin_ia32_cvtmask2q512 (__A);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_movepi64_mask (__m512i __A)
{
  return (__mmask8) __builtin_ia32_cvtq2mask512 ((__v8di) __A);
}


static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_broadcast_f32x2 (__m128 __A)
{
  return (__m512)__builtin_shufflevector((__v4sf)__A, (__v4sf)__A,
                                         0, 1, 0, 1, 0, 1, 0, 1,
                                         0, 1, 0, 1, 0, 1, 0, 1);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_broadcast_f32x2 (__m512 __O, __mmask16 __M, __m128 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__M,
                                             (__v16sf)_mm512_broadcast_f32x2(__A),
                                             (__v16sf)__O);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_broadcast_f32x2 (__mmask16 __M, __m128 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__M,
                                             (__v16sf)_mm512_broadcast_f32x2(__A),
                                             (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_broadcast_f32x8(__m256 __A)
{
  return (__m512)__builtin_shufflevector((__v8sf)__A, (__v8sf)__A,
                                         0, 1, 2, 3, 4, 5, 6, 7,
                                         0, 1, 2, 3, 4, 5, 6, 7);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_broadcast_f32x8(__m512 __O, __mmask16 __M, __m256 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__M,
                                           (__v16sf)_mm512_broadcast_f32x8(__A),
                                           (__v16sf)__O);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_broadcast_f32x8(__mmask16 __M, __m256 __A)
{
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__M,
                                           (__v16sf)_mm512_broadcast_f32x8(__A),
                                           (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_broadcast_f64x2(__m128d __A)
{
  return (__m512d)__builtin_shufflevector((__v2df)__A, (__v2df)__A,
                                          0, 1, 0, 1, 0, 1, 0, 1);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_broadcast_f64x2(__m512d __O, __mmask8 __M, __m128d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__M,
                                            (__v8df)_mm512_broadcast_f64x2(__A),
                                            (__v8df)__O);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_broadcast_f64x2(__mmask8 __M, __m128d __A)
{
  return (__m512d)__builtin_ia32_selectpd_512((__mmask8)__M,
                                            (__v8df)_mm512_broadcast_f64x2(__A),
                                            (__v8df)_mm512_setzero_pd());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_broadcast_i32x2 (__m128i __A)
{
  return (__m512i)__builtin_shufflevector((__v4si)__A, (__v4si)__A,
                                          0, 1, 0, 1, 0, 1, 0, 1,
                                          0, 1, 0, 1, 0, 1, 0, 1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_broadcast_i32x2 (__m512i __O, __mmask16 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                             (__v16si)_mm512_broadcast_i32x2(__A),
                                             (__v16si)__O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_broadcast_i32x2 (__mmask16 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                             (__v16si)_mm512_broadcast_i32x2(__A),
                                             (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_broadcast_i32x8(__m256i __A)
{
  return (__m512i)__builtin_shufflevector((__v8si)__A, (__v8si)__A,
                                          0, 1, 2, 3, 4, 5, 6, 7,
                                          0, 1, 2, 3, 4, 5, 6, 7);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_broadcast_i32x8(__m512i __O, __mmask16 __M, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                           (__v16si)_mm512_broadcast_i32x8(__A),
                                           (__v16si)__O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_broadcast_i32x8(__mmask16 __M, __m256i __A)
{
  return (__m512i)__builtin_ia32_selectd_512((__mmask16)__M,
                                           (__v16si)_mm512_broadcast_i32x8(__A),
                                           (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_broadcast_i64x2(__m128i __A)
{
  return (__m512i)__builtin_shufflevector((__v2di)__A, (__v2di)__A,
                                          0, 1, 0, 1, 0, 1, 0, 1);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_mask_broadcast_i64x2(__m512i __O, __mmask8 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                            (__v8di)_mm512_broadcast_i64x2(__A),
                                            (__v8di)__O);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512dq"), __min_vector_width__(512)))
_mm512_maskz_broadcast_i64x2(__mmask8 __M, __m128i __A)
{
  return (__m512i)__builtin_ia32_selectq_512((__mmask8)__M,
                                            (__v8di)_mm512_broadcast_i64x2(__A),
                                            (__v8di)_mm512_setzero_si512());
}
# 196 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3





# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbitalgintrin.h" 1 3
# 36 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbitalgintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg"), __min_vector_width__(256)))
_mm256_popcnt_epi16(__m256i __A)
{
  return (__m256i) __builtin_ia32_vpopcntw_256((__v16hi) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg"), __min_vector_width__(256)))
_mm256_mask_popcnt_epi16(__m256i __A, __mmask16 __U, __m256i __B)
{
  return (__m256i) __builtin_ia32_selectw_256((__mmask16) __U,
              (__v16hi) _mm256_popcnt_epi16(__B),
              (__v16hi) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg"), __min_vector_width__(256)))
_mm256_maskz_popcnt_epi16(__mmask16 __U, __m256i __B)
{
  return _mm256_mask_popcnt_epi16((__m256i) _mm256_setzero_si256(),
              __U,
              __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg"), __min_vector_width__(128)))
_mm_popcnt_epi16(__m128i __A)
{
  return (__m128i) __builtin_ia32_vpopcntw_128((__v8hi) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg"), __min_vector_width__(128)))
_mm_mask_popcnt_epi16(__m128i __A, __mmask8 __U, __m128i __B)
{
  return (__m128i) __builtin_ia32_selectw_128((__mmask8) __U,
              (__v8hi) _mm_popcnt_epi16(__B),
              (__v8hi) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg"), __min_vector_width__(128)))
_mm_maskz_popcnt_epi16(__mmask8 __U, __m128i __B)
{
  return _mm_mask_popcnt_epi16((__m128i) _mm_setzero_si128(),
              __U,
              __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg"), __min_vector_width__(256)))
_mm256_popcnt_epi8(__m256i __A)
{
  return (__m256i) __builtin_ia32_vpopcntb_256((__v32qi) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg"), __min_vector_width__(256)))
_mm256_mask_popcnt_epi8(__m256i __A, __mmask32 __U, __m256i __B)
{
  return (__m256i) __builtin_ia32_selectb_256((__mmask32) __U,
              (__v32qi) _mm256_popcnt_epi8(__B),
              (__v32qi) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg"), __min_vector_width__(256)))
_mm256_maskz_popcnt_epi8(__mmask32 __U, __m256i __B)
{
  return _mm256_mask_popcnt_epi8((__m256i) _mm256_setzero_si256(),
              __U,
              __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg"), __min_vector_width__(128)))
_mm_popcnt_epi8(__m128i __A)
{
  return (__m128i) __builtin_ia32_vpopcntb_128((__v16qi) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg"), __min_vector_width__(128)))
_mm_mask_popcnt_epi8(__m128i __A, __mmask16 __U, __m128i __B)
{
  return (__m128i) __builtin_ia32_selectb_128((__mmask16) __U,
              (__v16qi) _mm_popcnt_epi8(__B),
              (__v16qi) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg"), __min_vector_width__(128)))
_mm_maskz_popcnt_epi8(__mmask16 __U, __m128i __B)
{
  return _mm_mask_popcnt_epi8((__m128i) _mm_setzero_si128(),
              __U,
              __B);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg"), __min_vector_width__(256)))
_mm256_mask_bitshuffle_epi64_mask(__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__mmask32) __builtin_ia32_vpshufbitqmb256_mask((__v32qi) __A,
              (__v32qi) __B,
              __U);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg"), __min_vector_width__(256)))
_mm256_bitshuffle_epi64_mask(__m256i __A, __m256i __B)
{
  return _mm256_mask_bitshuffle_epi64_mask((__mmask32) -1,
              __A,
              __B);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg"), __min_vector_width__(128)))
_mm_mask_bitshuffle_epi64_mask(__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__mmask16) __builtin_ia32_vpshufbitqmb128_mask((__v16qi) __A,
              (__v16qi) __B,
              __U);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bitalg"), __min_vector_width__(128)))
_mm_bitshuffle_epi64_mask(__m128i __A, __m128i __B)
{
  return _mm_mask_bitshuffle_epi64_mask((__mmask16) -1,
              __A,
              __B);
}
# 202 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3





# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbwintrin.h" 1 3
# 316 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbwintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_add_epi8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B){
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                             (__v32qi)_mm256_add_epi8(__A, __B),
                                             (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_add_epi8(__mmask32 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                             (__v32qi)_mm256_add_epi8(__A, __B),
                                             (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_add_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_add_epi16(__A, __B),
                                             (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_add_epi16(__mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_add_epi16(__A, __B),
                                             (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_sub_epi8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                             (__v32qi)_mm256_sub_epi8(__A, __B),
                                             (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_sub_epi8(__mmask32 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                             (__v32qi)_mm256_sub_epi8(__A, __B),
                                             (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_sub_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_sub_epi16(__A, __B),
                                             (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_sub_epi16(__mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_sub_epi16(__A, __B),
                                             (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_add_epi8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_add_epi8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_add_epi8(__mmask16 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_add_epi8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_add_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_add_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_add_epi16(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_add_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_sub_epi8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_sub_epi8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_sub_epi8(__mmask16 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_sub_epi8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_sub_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_sub_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_sub_epi16(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_sub_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_mullo_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_mullo_epi16(__A, __B),
                                             (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_mullo_epi16(__mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_mullo_epi16(__A, __B),
                                             (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_mullo_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_mullo_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_mullo_epi16(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_mullo_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_blend_epi8 (__mmask16 __U, __m128i __A, __m128i __W)
{
  return (__m128i) __builtin_ia32_selectb_128 ((__mmask16) __U,
              (__v16qi) __W,
              (__v16qi) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_blend_epi8 (__mmask32 __U, __m256i __A, __m256i __W)
{
  return (__m256i) __builtin_ia32_selectb_256 ((__mmask32) __U,
               (__v32qi) __W,
               (__v32qi) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_blend_epi16 (__mmask8 __U, __m128i __A, __m128i __W)
{
  return (__m128i) __builtin_ia32_selectw_128 ((__mmask8) __U,
               (__v8hi) __W,
               (__v8hi) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_blend_epi16 (__mmask16 __U, __m256i __A, __m256i __W)
{
  return (__m256i) __builtin_ia32_selectw_256 ((__mmask16) __U,
               (__v16hi) __W,
               (__v16hi) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_abs_epi8(__m128i __W, __mmask16 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_abs_epi8(__A),
                                             (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_abs_epi8(__mmask16 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_abs_epi8(__A),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_abs_epi8(__m256i __W, __mmask32 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                             (__v32qi)_mm256_abs_epi8(__A),
                                             (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_abs_epi8 (__mmask32 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                             (__v32qi)_mm256_abs_epi8(__A),
                                             (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_abs_epi16(__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_abs_epi16(__A),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_abs_epi16(__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_abs_epi16(__A),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_abs_epi16(__m256i __W, __mmask16 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_abs_epi16(__A),
                                             (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_abs_epi16(__mmask16 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_abs_epi16(__A),
                                             (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_packs_epi32(__mmask8 __M, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_packs_epi32(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_packs_epi32(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_packs_epi32(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_packs_epi32(__mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                          (__v16hi)_mm256_packs_epi32(__A, __B),
                                          (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_packs_epi32(__m256i __W, __mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                          (__v16hi)_mm256_packs_epi32(__A, __B),
                                          (__v16hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_packs_epi16(__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_packs_epi16(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_packs_epi16(__m128i __W, __mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_packs_epi16(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_packs_epi16(__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                          (__v32qi)_mm256_packs_epi16(__A, __B),
                                          (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_packs_epi16(__m256i __W, __mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                          (__v32qi)_mm256_packs_epi16(__A, __B),
                                          (__v32qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_packus_epi32(__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_packus_epi32(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_packus_epi32(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_packus_epi32(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_packus_epi32(__mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                         (__v16hi)_mm256_packus_epi32(__A, __B),
                                         (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_packus_epi32(__m256i __W, __mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                         (__v16hi)_mm256_packus_epi32(__A, __B),
                                         (__v16hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_packus_epi16(__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                            (__v16qi)_mm_packus_epi16(__A, __B),
                                            (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_packus_epi16(__m128i __W, __mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                            (__v16qi)_mm_packus_epi16(__A, __B),
                                            (__v16qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_packus_epi16(__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                         (__v32qi)_mm256_packus_epi16(__A, __B),
                                         (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_packus_epi16(__m256i __W, __mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                         (__v32qi)_mm256_packus_epi16(__A, __B),
                                         (__v32qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_adds_epi8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_adds_epi8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_adds_epi8(__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_adds_epi8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_adds_epi8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                            (__v32qi)_mm256_adds_epi8(__A, __B),
                                            (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_adds_epi8(__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                            (__v32qi)_mm256_adds_epi8(__A, __B),
                                            (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_adds_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_adds_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_adds_epi16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_adds_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_adds_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_adds_epi16(__A, __B),
                                           (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_adds_epi16(__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_adds_epi16(__A, __B),
                                           (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_adds_epu8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_adds_epu8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_adds_epu8(__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_adds_epu8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_adds_epu8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                            (__v32qi)_mm256_adds_epu8(__A, __B),
                                            (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_adds_epu8(__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                            (__v32qi)_mm256_adds_epu8(__A, __B),
                                            (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_adds_epu16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_adds_epu16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_adds_epu16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_adds_epu16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_adds_epu16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_adds_epu16(__A, __B),
                                           (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_adds_epu16(__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_adds_epu16(__A, __B),
                                           (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_avg_epu8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_avg_epu8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_avg_epu8(__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_avg_epu8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_avg_epu8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                             (__v32qi)_mm256_avg_epu8(__A, __B),
                                             (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_avg_epu8(__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                             (__v32qi)_mm256_avg_epu8(__A, __B),
                                             (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_avg_epu16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_avg_epu16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_avg_epu16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_avg_epu16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_avg_epu16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                            (__v16hi)_mm256_avg_epu16(__A, __B),
                                            (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_avg_epu16(__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                            (__v16hi)_mm256_avg_epu16(__A, __B),
                                            (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_max_epi8(__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_max_epi8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_max_epi8(__m128i __W, __mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_max_epi8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_max_epi8(__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                             (__v32qi)_mm256_max_epi8(__A, __B),
                                             (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_max_epi8(__m256i __W, __mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                             (__v32qi)_mm256_max_epi8(__A, __B),
                                             (__v32qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_max_epi16(__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_max_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_max_epi16(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_max_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_max_epi16(__mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                            (__v16hi)_mm256_max_epi16(__A, __B),
                                            (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_max_epi16(__m256i __W, __mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                            (__v16hi)_mm256_max_epi16(__A, __B),
                                            (__v16hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_max_epu8(__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_max_epu8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_max_epu8(__m128i __W, __mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_max_epu8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_max_epu8 (__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                             (__v32qi)_mm256_max_epu8(__A, __B),
                                             (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_max_epu8(__m256i __W, __mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                             (__v32qi)_mm256_max_epu8(__A, __B),
                                             (__v32qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_max_epu16(__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_max_epu16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_max_epu16(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_max_epu16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_max_epu16(__mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                            (__v16hi)_mm256_max_epu16(__A, __B),
                                            (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_max_epu16(__m256i __W, __mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                            (__v16hi)_mm256_max_epu16(__A, __B),
                                            (__v16hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_min_epi8(__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_min_epi8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_min_epi8(__m128i __W, __mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_min_epi8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_min_epi8(__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                             (__v32qi)_mm256_min_epi8(__A, __B),
                                             (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_min_epi8(__m256i __W, __mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                             (__v32qi)_mm256_min_epi8(__A, __B),
                                             (__v32qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_min_epi16(__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_min_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_min_epi16(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_min_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_min_epi16(__mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                            (__v16hi)_mm256_min_epi16(__A, __B),
                                            (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_min_epi16(__m256i __W, __mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                            (__v16hi)_mm256_min_epi16(__A, __B),
                                            (__v16hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_min_epu8(__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_min_epu8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_min_epu8(__m128i __W, __mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm_min_epu8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_min_epu8 (__mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                             (__v32qi)_mm256_min_epu8(__A, __B),
                                             (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_min_epu8(__m256i __W, __mmask32 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                             (__v32qi)_mm256_min_epu8(__A, __B),
                                             (__v32qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_min_epu16(__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_min_epu16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_min_epu16(__m128i __W, __mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                             (__v8hi)_mm_min_epu16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_min_epu16(__mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                            (__v16hi)_mm256_min_epu16(__A, __B),
                                            (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_min_epu16(__m256i __W, __mmask16 __M, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                            (__v16hi)_mm256_min_epu16(__A, __B),
                                            (__v16hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_shuffle_epi8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                            (__v16qi)_mm_shuffle_epi8(__A, __B),
                                            (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_shuffle_epi8(__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                            (__v16qi)_mm_shuffle_epi8(__A, __B),
                                            (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_shuffle_epi8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                         (__v32qi)_mm256_shuffle_epi8(__A, __B),
                                         (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_shuffle_epi8(__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                         (__v32qi)_mm256_shuffle_epi8(__A, __B),
                                         (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_subs_epi8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_subs_epi8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_subs_epi8(__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_subs_epi8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_subs_epi8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                            (__v32qi)_mm256_subs_epi8(__A, __B),
                                            (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_subs_epi8(__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                            (__v32qi)_mm256_subs_epi8(__A, __B),
                                            (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_subs_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_subs_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_subs_epi16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_subs_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_subs_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_subs_epi16(__A, __B),
                                           (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_subs_epi16(__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_subs_epi16(__A, __B),
                                           (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_subs_epu8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_subs_epu8(__A, __B),
                                             (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_subs_epu8(__mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                             (__v16qi)_mm_subs_epu8(__A, __B),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_subs_epu8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                            (__v32qi)_mm256_subs_epu8(__A, __B),
                                            (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_subs_epu8(__mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                            (__v32qi)_mm256_subs_epu8(__A, __B),
                                            (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_subs_epu16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_subs_epu16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_subs_epu16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_subs_epu16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_subs_epu16(__m256i __W, __mmask16 __U, __m256i __A,
      __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_subs_epu16(__A, __B),
                                           (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_subs_epu16(__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_subs_epu16(__A, __B),
                                           (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_permutex2var_epi16(__m128i __A, __m128i __I, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpermi2varhi128((__v8hi)__A, (__v8hi)__I,
                                                 (__v8hi) __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_permutex2var_epi16(__m128i __A, __mmask8 __U, __m128i __I,
                            __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128(__U,
                                  (__v8hi)_mm_permutex2var_epi16(__A, __I, __B),
                                  (__v8hi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask2_permutex2var_epi16(__m128i __A, __m128i __I, __mmask8 __U,
                             __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128(__U,
                                  (__v8hi)_mm_permutex2var_epi16(__A, __I, __B),
                                  (__v8hi)__I);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_permutex2var_epi16 (__mmask8 __U, __m128i __A, __m128i __I,
            __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128(__U,
                                  (__v8hi)_mm_permutex2var_epi16(__A, __I, __B),
                                  (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_permutex2var_epi16(__m256i __A, __m256i __I, __m256i __B)
{
  return (__m256i)__builtin_ia32_vpermi2varhi256((__v16hi)__A, (__v16hi)__I,
                                                 (__v16hi)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_permutex2var_epi16(__m256i __A, __mmask16 __U, __m256i __I,
                               __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256(__U,
                              (__v16hi)_mm256_permutex2var_epi16(__A, __I, __B),
                              (__v16hi)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask2_permutex2var_epi16(__m256i __A, __m256i __I, __mmask16 __U,
                                __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256(__U,
                              (__v16hi)_mm256_permutex2var_epi16(__A, __I, __B),
                              (__v16hi)__I);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_permutex2var_epi16 (__mmask16 __U, __m256i __A, __m256i __I,
                                 __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256(__U,
                              (__v16hi)_mm256_permutex2var_epi16(__A, __I, __B),
                              (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_maddubs_epi16(__m128i __W, __mmask8 __U, __m128i __X, __m128i __Y) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                            (__v8hi)_mm_maddubs_epi16(__X, __Y),
                                            (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_maddubs_epi16(__mmask8 __U, __m128i __X, __m128i __Y) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                            (__v8hi)_mm_maddubs_epi16(__X, __Y),
                                            (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_maddubs_epi16(__m256i __W, __mmask16 __U, __m256i __X,
                          __m256i __Y) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                        (__v16hi)_mm256_maddubs_epi16(__X, __Y),
                                        (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_maddubs_epi16(__mmask16 __U, __m256i __X, __m256i __Y) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                        (__v16hi)_mm256_maddubs_epi16(__X, __Y),
                                        (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_madd_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_madd_epi16(__A, __B),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_madd_epi16(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_madd_epi16(__A, __B),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_madd_epi16(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                            (__v8si)_mm256_madd_epi16(__A, __B),
                                            (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_madd_epi16(__mmask8 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                            (__v8si)_mm256_madd_epi16(__A, __B),
                                            (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_cvtsepi16_epi8 (__m128i __A) {
  return (__m128i) __builtin_ia32_pmovswb128_mask ((__v8hi) __A,
               (__v16qi) _mm_setzero_si128(),
               (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_cvtsepi16_epi8 (__m128i __O, __mmask8 __M, __m128i __A) {
  return (__m128i) __builtin_ia32_pmovswb128_mask ((__v8hi) __A,
               (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_cvtsepi16_epi8 (__mmask8 __M, __m128i __A) {
  return (__m128i) __builtin_ia32_pmovswb128_mask ((__v8hi) __A,
               (__v16qi) _mm_setzero_si128(),
               __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_cvtsepi16_epi8 (__m256i __A) {
  return (__m128i) __builtin_ia32_pmovswb256_mask ((__v16hi) __A,
               (__v16qi) _mm_setzero_si128(),
               (__mmask16) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_cvtsepi16_epi8 (__m128i __O, __mmask16 __M, __m256i __A) {
  return (__m128i) __builtin_ia32_pmovswb256_mask ((__v16hi) __A,
               (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_cvtsepi16_epi8 (__mmask16 __M, __m256i __A) {
  return (__m128i) __builtin_ia32_pmovswb256_mask ((__v16hi) __A,
               (__v16qi) _mm_setzero_si128(),
               __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_cvtusepi16_epi8 (__m128i __A) {
  return (__m128i) __builtin_ia32_pmovuswb128_mask ((__v8hi) __A,
                (__v16qi) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_cvtusepi16_epi8 (__m128i __O, __mmask8 __M, __m128i __A) {
  return (__m128i) __builtin_ia32_pmovuswb128_mask ((__v8hi) __A,
                (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_cvtusepi16_epi8 (__mmask8 __M, __m128i __A) {
  return (__m128i) __builtin_ia32_pmovuswb128_mask ((__v8hi) __A,
                (__v16qi) _mm_setzero_si128(),
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_cvtusepi16_epi8 (__m256i __A) {
  return (__m128i) __builtin_ia32_pmovuswb256_mask ((__v16hi) __A,
                (__v16qi) _mm_setzero_si128(),
                (__mmask16) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_cvtusepi16_epi8 (__m128i __O, __mmask16 __M, __m256i __A) {
  return (__m128i) __builtin_ia32_pmovuswb256_mask ((__v16hi) __A,
                (__v16qi) __O,
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_cvtusepi16_epi8 (__mmask16 __M, __m256i __A) {
  return (__m128i) __builtin_ia32_pmovuswb256_mask ((__v16hi) __A,
                (__v16qi) _mm_setzero_si128(),
                __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_cvtepi16_epi8 (__m128i __A) {
  return (__m128i)__builtin_shufflevector(
      __builtin_convertvector((__v8hi)__A, __v8qi),
      (__v8qi){0, 0, 0, 0, 0, 0, 0, 0}, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
      12, 13, 14, 15);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_cvtepi16_epi8 (__m128i __O, __mmask8 __M, __m128i __A) {
  return (__m128i) __builtin_ia32_pmovwb128_mask ((__v8hi) __A,
               (__v16qi) __O,
               __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_cvtepi16_epi8 (__mmask8 __M, __m128i __A) {
  return (__m128i) __builtin_ia32_pmovwb128_mask ((__v8hi) __A,
               (__v16qi) _mm_setzero_si128(),
               __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_cvtepi16_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovwb128mem_mask ((__v16qi *) __P, (__v8hi) __A, __M);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_cvtsepi16_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovswb128mem_mask ((__v16qi *) __P, (__v8hi) __A, __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_cvtusepi16_storeu_epi8 (void * __P, __mmask8 __M, __m128i __A)
{
  __builtin_ia32_pmovuswb128mem_mask ((__v16qi *) __P, (__v8hi) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_cvtepi16_epi8 (__m256i __A) {
  return (__m128i)__builtin_convertvector((__v16hi) __A, __v16qi);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_cvtepi16_epi8 (__m128i __O, __mmask16 __M, __m256i __A) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm256_cvtepi16_epi8(__A),
                                             (__v16qi)__O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_cvtepi16_epi8 (__mmask16 __M, __m256i __A) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                             (__v16qi)_mm256_cvtepi16_epi8(__A),
                                             (__v16qi)_mm_setzero_si128());
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_cvtepi16_storeu_epi8 (void * __P, __mmask16 __M, __m256i __A)
{
  __builtin_ia32_pmovwb256mem_mask ((__v16qi *) __P, (__v16hi) __A, __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_cvtsepi16_storeu_epi8 (void * __P, __mmask16 __M, __m256i __A)
{
  __builtin_ia32_pmovswb256mem_mask ((__v16qi *) __P, (__v16hi) __A, __M);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_cvtusepi16_storeu_epi8 (void * __P, __mmask16 __M, __m256i __A)
{
  __builtin_ia32_pmovuswb256mem_mask ((__v16qi*) __P, (__v16hi) __A, __M);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_mulhrs_epi16(__m128i __W, __mmask8 __U, __m128i __X, __m128i __Y) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_mulhrs_epi16(__X, __Y),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_mulhrs_epi16(__mmask8 __U, __m128i __X, __m128i __Y) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_mulhrs_epi16(__X, __Y),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_mulhrs_epi16(__m256i __W, __mmask16 __U, __m256i __X, __m256i __Y) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                         (__v16hi)_mm256_mulhrs_epi16(__X, __Y),
                                         (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_mulhrs_epi16(__mmask16 __U, __m256i __X, __m256i __Y) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                         (__v16hi)_mm256_mulhrs_epi16(__X, __Y),
                                         (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_mulhi_epu16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_mulhi_epu16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_mulhi_epu16(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_mulhi_epu16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_mulhi_epu16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_mulhi_epu16(__A, __B),
                                          (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_mulhi_epu16(__mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_mulhi_epu16(__A, __B),
                                          (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_mulhi_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_mulhi_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_mulhi_epi16(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_mulhi_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_mulhi_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_mulhi_epi16(__A, __B),
                                          (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_mulhi_epi16(__mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_mulhi_epi16(__A, __B),
                                          (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_unpackhi_epi8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                           (__v16qi)_mm_unpackhi_epi8(__A, __B),
                                           (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_unpackhi_epi8(__mmask16 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                           (__v16qi)_mm_unpackhi_epi8(__A, __B),
                                           (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_unpackhi_epi8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                        (__v32qi)_mm256_unpackhi_epi8(__A, __B),
                                        (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_unpackhi_epi8(__mmask32 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                        (__v32qi)_mm256_unpackhi_epi8(__A, __B),
                                        (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_unpackhi_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                           (__v8hi)_mm_unpackhi_epi16(__A, __B),
                                           (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_unpackhi_epi16(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                           (__v8hi)_mm_unpackhi_epi16(__A, __B),
                                           (__v8hi) _mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_unpackhi_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                       (__v16hi)_mm256_unpackhi_epi16(__A, __B),
                                       (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_unpackhi_epi16(__mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                       (__v16hi)_mm256_unpackhi_epi16(__A, __B),
                                       (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_unpacklo_epi8(__m128i __W, __mmask16 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                           (__v16qi)_mm_unpacklo_epi8(__A, __B),
                                           (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_unpacklo_epi8(__mmask16 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__U,
                                           (__v16qi)_mm_unpacklo_epi8(__A, __B),
                                           (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_unpacklo_epi8(__m256i __W, __mmask32 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                        (__v32qi)_mm256_unpacklo_epi8(__A, __B),
                                        (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_unpacklo_epi8(__mmask32 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__U,
                                        (__v32qi)_mm256_unpacklo_epi8(__A, __B),
                                        (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_unpacklo_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                           (__v8hi)_mm_unpacklo_epi16(__A, __B),
                                           (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_unpacklo_epi16(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                           (__v8hi)_mm_unpacklo_epi16(__A, __B),
                                           (__v8hi) _mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_unpacklo_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                       (__v16hi)_mm256_unpacklo_epi16(__A, __B),
                                       (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_unpacklo_epi16(__mmask16 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                       (__v16hi)_mm256_unpacklo_epi16(__A, __B),
                                       (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_cvtepi8_epi16(__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_cvtepi8_epi16(__A),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_cvtepi8_epi16(__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_cvtepi8_epi16(__A),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_cvtepi8_epi16(__m256i __W, __mmask16 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_cvtepi8_epi16(__A),
                                             (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_cvtepi8_epi16(__mmask16 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_cvtepi8_epi16(__A),
                                             (__v16hi)_mm256_setzero_si256());
}


static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_cvtepu8_epi16(__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_cvtepu8_epi16(__A),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_cvtepu8_epi16(__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_cvtepu8_epi16(__A),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_cvtepu8_epi16(__m256i __W, __mmask16 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_cvtepu8_epi16(__A),
                                             (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_cvtepu8_epi16 (__mmask16 __U, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                             (__v16hi)_mm256_cvtepu8_epi16(__A),
                                             (__v16hi)_mm256_setzero_si256());
}
# 1878 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbwintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_sllv_epi16(__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_psllv16hi((__v16hi)__A, (__v16hi)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_sllv_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_sllv_epi16(__A, __B),
                                           (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_sllv_epi16(__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_sllv_epi16(__A, __B),
                                           (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_sllv_epi16(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_psllv8hi((__v8hi)__A, (__v8hi)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_sllv_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_sllv_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_sllv_epi16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_sllv_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_sll_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_sll_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_sll_epi16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_sll_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_sll_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_sll_epi16(__A, __B),
                                          (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_sll_epi16(__mmask16 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_sll_epi16(__A, __B),
                                          (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_slli_epi16(__m128i __W, __mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_slli_epi16(__A, (int)__B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_slli_epi16 (__mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_slli_epi16(__A, (int)__B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_slli_epi16(__m256i __W, __mmask16 __U, __m256i __A,
                       unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                         (__v16hi)_mm256_slli_epi16(__A, (int)__B),
                                         (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_slli_epi16(__mmask16 __U, __m256i __A, unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                         (__v16hi)_mm256_slli_epi16(__A, (int)__B),
                                         (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_srlv_epi16(__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_psrlv16hi((__v16hi)__A, (__v16hi)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_srlv_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_srlv_epi16(__A, __B),
                                           (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_srlv_epi16(__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_srlv_epi16(__A, __B),
                                           (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_srlv_epi16(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_psrlv8hi((__v8hi)__A, (__v8hi)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_srlv_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srlv_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_srlv_epi16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srlv_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_srav_epi16(__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_psrav16hi((__v16hi)__A, (__v16hi)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_srav_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_srav_epi16(__A, __B),
                                           (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_srav_epi16(__mmask16 __U, __m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                           (__v16hi)_mm256_srav_epi16(__A, __B),
                                           (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_srav_epi16(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_psrav8hi((__v8hi)__A, (__v8hi)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_srav_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srav_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_srav_epi16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srav_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_sra_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_sra_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_sra_epi16(__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_sra_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_sra_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_sra_epi16(__A, __B),
                                          (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_sra_epi16(__mmask16 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_sra_epi16(__A, __B),
                                          (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_srai_epi16(__m128i __W, __mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srai_epi16(__A, (int)__B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_srai_epi16(__mmask8 __U, __m128i __A, unsigned int __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srai_epi16(__A, (int)__B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_srai_epi16(__m256i __W, __mmask16 __U, __m256i __A,
                       unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                         (__v16hi)_mm256_srai_epi16(__A, (int)__B),
                                         (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_srai_epi16(__mmask16 __U, __m256i __A, unsigned int __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                         (__v16hi)_mm256_srai_epi16(__A, (int)__B),
                                         (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_srl_epi16(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srl_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_srl_epi16 (__mmask8 __U, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srl_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_srl_epi16(__m256i __W, __mmask16 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_srl_epi16(__A, __B),
                                          (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_srl_epi16(__mmask16 __U, __m256i __A, __m128i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                          (__v16hi)_mm256_srl_epi16(__A, __B),
                                          (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_srli_epi16(__m128i __W, __mmask8 __U, __m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srli_epi16(__A, __B),
                                             (__v8hi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_srli_epi16 (__mmask8 __U, __m128i __A, int __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__U,
                                             (__v8hi)_mm_srli_epi16(__A, __B),
                                             (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_srli_epi16(__m256i __W, __mmask16 __U, __m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                         (__v16hi)_mm256_srli_epi16(__A, __B),
                                         (__v16hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_srli_epi16(__mmask16 __U, __m256i __A, int __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__U,
                                         (__v16hi)_mm256_srli_epi16(__A, __B),
                                         (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_mov_epi16 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_selectw_128 ((__mmask8) __U,
                (__v8hi) __A,
                (__v8hi) __W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_mov_epi16 (__mmask8 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_selectw_128 ((__mmask8) __U,
                (__v8hi) __A,
                (__v8hi) _mm_setzero_si128 ());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_mov_epi16 (__m256i __W, __mmask16 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_selectw_256 ((__mmask16) __U,
                (__v16hi) __A,
                (__v16hi) __W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_mov_epi16 (__mmask16 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_selectw_256 ((__mmask16) __U,
                (__v16hi) __A,
                (__v16hi) _mm256_setzero_si256 ());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_mov_epi8 (__m128i __W, __mmask16 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_selectb_128 ((__mmask16) __U,
                (__v16qi) __A,
                (__v16qi) __W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_mov_epi8 (__mmask16 __U, __m128i __A)
{
  return (__m128i) __builtin_ia32_selectb_128 ((__mmask16) __U,
                (__v16qi) __A,
                (__v16qi) _mm_setzero_si128 ());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_mov_epi8 (__m256i __W, __mmask32 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_selectb_256 ((__mmask32) __U,
                (__v32qi) __A,
                (__v32qi) __W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_mov_epi8 (__mmask32 __U, __m256i __A)
{
  return (__m256i) __builtin_ia32_selectb_256 ((__mmask32) __U,
                (__v32qi) __A,
                (__v32qi) _mm256_setzero_si256 ());
}


static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_set1_epi8 (__m128i __O, __mmask16 __M, char __A)
{
  return (__m128i) __builtin_ia32_selectb_128(__M,
                                              (__v16qi) _mm_set1_epi8(__A),
                                              (__v16qi) __O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_set1_epi8 (__mmask16 __M, char __A)
{
 return (__m128i) __builtin_ia32_selectb_128(__M,
                                             (__v16qi) _mm_set1_epi8(__A),
                                             (__v16qi) _mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_set1_epi8 (__m256i __O, __mmask32 __M, char __A)
{
  return (__m256i) __builtin_ia32_selectb_256(__M,
                                              (__v32qi) _mm256_set1_epi8(__A),
                                              (__v32qi) __O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_set1_epi8 (__mmask32 __M, char __A)
{
  return (__m256i) __builtin_ia32_selectb_256(__M,
                                              (__v32qi) _mm256_set1_epi8(__A),
                                              (__v32qi) _mm256_setzero_si256());
}

static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_loadu_epi16 (void const *__P)
{
  struct __loadu_epi16 {
    __m128i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_epi16*)__P)->__v;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_loadu_epi16 (__m128i __W, __mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_loaddquhi128_mask ((const __v8hi *) __P,
                 (__v8hi) __W,
                 (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_loadu_epi16 (__mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_loaddquhi128_mask ((const __v8hi *) __P,
                 (__v8hi)
                 _mm_setzero_si128 (),
                 (__mmask8) __U);
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_loadu_epi16 (void const *__P)
{
  struct __loadu_epi16 {
    __m256i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_epi16*)__P)->__v;
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_loadu_epi16 (__m256i __W, __mmask16 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_loaddquhi256_mask ((const __v16hi *) __P,
                 (__v16hi) __W,
                 (__mmask16) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_loadu_epi16 (__mmask16 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_loaddquhi256_mask ((const __v16hi *) __P,
                 (__v16hi)
                 _mm256_setzero_si256 (),
                 (__mmask16) __U);
}

static __inline __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_loadu_epi8 (void const *__P)
{
  struct __loadu_epi8 {
    __m128i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_epi8*)__P)->__v;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_loadu_epi8 (__m128i __W, __mmask16 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_loaddquqi128_mask ((const __v16qi *) __P,
                 (__v16qi) __W,
                 (__mmask16) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_loadu_epi8 (__mmask16 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_loaddquqi128_mask ((const __v16qi *) __P,
                 (__v16qi)
                 _mm_setzero_si128 (),
                 (__mmask16) __U);
}

static __inline __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_loadu_epi8 (void const *__P)
{
  struct __loadu_epi8 {
    __m256i_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_epi8*)__P)->__v;
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_loadu_epi8 (__m256i __W, __mmask32 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_loaddquqi256_mask ((const __v32qi *) __P,
                 (__v32qi) __W,
                 (__mmask32) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_loadu_epi8 (__mmask32 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_loaddquqi256_mask ((const __v32qi *) __P,
                 (__v32qi)
                 _mm256_setzero_si256 (),
                 (__mmask32) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_storeu_epi16 (void *__P, __m128i __A)
{
  struct __storeu_epi16 {
    __m128i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_epi16*)__P)->__v = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_storeu_epi16 (void *__P, __mmask8 __U, __m128i __A)
{
  __builtin_ia32_storedquhi128_mask ((__v8hi *) __P,
             (__v8hi) __A,
             (__mmask8) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_storeu_epi16 (void *__P, __m256i __A)
{
  struct __storeu_epi16 {
    __m256i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_epi16*)__P)->__v = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_storeu_epi16 (void *__P, __mmask16 __U, __m256i __A)
{
  __builtin_ia32_storedquhi256_mask ((__v16hi *) __P,
             (__v16hi) __A,
             (__mmask16) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_storeu_epi8 (void *__P, __m128i __A)
{
  struct __storeu_epi8 {
    __m128i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_epi8*)__P)->__v = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_storeu_epi8 (void *__P, __mmask16 __U, __m128i __A)
{
  __builtin_ia32_storedquqi128_mask ((__v16qi *) __P,
             (__v16qi) __A,
             (__mmask16) __U);
}

static __inline void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_storeu_epi8 (void *__P, __m256i __A)
{
  struct __storeu_epi8 {
    __m256i_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_epi8*)__P)->__v = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_storeu_epi8 (void *__P, __mmask32 __U, __m256i __A)
{
  __builtin_ia32_storedquqi256_mask ((__v32qi *) __P,
             (__v32qi) __A,
             (__mmask32) __U);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_test_epi8_mask (__m128i __A, __m128i __B)
{
  return ((__mmask16)__builtin_ia32_cmpb128_mask((__v16qi)(__m128i)((_mm_and_si128(__A, __B))), (__v16qi)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_NE), (__mmask16)-1));
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_test_epi8_mask (__mmask16 __U, __m128i __A, __m128i __B)
{
  return ((__mmask16)__builtin_ia32_cmpb128_mask((__v16qi)(__m128i)((_mm_and_si128 (__A, __B))), (__v16qi)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_NE), (__mmask16)((__U))));

}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_test_epi8_mask (__m256i __A, __m256i __B)
{
  return ((__mmask32)__builtin_ia32_cmpb256_mask((__v32qi)(__m256i)((_mm256_and_si256(__A, __B))), (__v32qi)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_NE), (__mmask32)-1));

}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_test_epi8_mask (__mmask32 __U, __m256i __A, __m256i __B)
{
  return ((__mmask32)__builtin_ia32_cmpb256_mask((__v32qi)(__m256i)((_mm256_and_si256(__A, __B))), (__v32qi)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_NE), (__mmask32)((__U))));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_test_epi16_mask (__m128i __A, __m128i __B)
{
  return ((__mmask8)__builtin_ia32_cmpw128_mask((__v8hi)(__m128i)((_mm_and_si128 (__A, __B))), (__v8hi)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_NE), (__mmask8)-1));
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_test_epi16_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return ((__mmask8)__builtin_ia32_cmpw128_mask((__v8hi)(__m128i)((_mm_and_si128 (__A, __B))), (__v8hi)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_NE), (__mmask8)((__U))));

}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_test_epi16_mask (__m256i __A, __m256i __B)
{
  return ((__mmask16)__builtin_ia32_cmpw256_mask((__v16hi)(__m256i)((_mm256_and_si256 (__A, __B))), (__v16hi)(__m256i)((_mm256_setzero_si256 ())), (int)(_MM_CMPINT_NE), (__mmask16)-1));

}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_test_epi16_mask (__mmask16 __U, __m256i __A, __m256i __B)
{
  return ((__mmask16)__builtin_ia32_cmpw256_mask((__v16hi)(__m256i)((_mm256_and_si256(__A, __B))), (__v16hi)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_NE), (__mmask16)((__U))));

}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_testn_epi8_mask (__m128i __A, __m128i __B)
{
  return ((__mmask16)__builtin_ia32_cmpb128_mask((__v16qi)(__m128i)((_mm_and_si128 (__A, __B))), (__v16qi)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_EQ), (__mmask16)-1));
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_testn_epi8_mask (__mmask16 __U, __m128i __A, __m128i __B)
{
  return ((__mmask16)__builtin_ia32_cmpb128_mask((__v16qi)(__m128i)((_mm_and_si128 (__A, __B))), (__v16qi)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_EQ), (__mmask16)((__U))));

}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_testn_epi8_mask (__m256i __A, __m256i __B)
{
  return ((__mmask32)__builtin_ia32_cmpb256_mask((__v32qi)(__m256i)((_mm256_and_si256 (__A, __B))), (__v32qi)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_EQ), (__mmask32)-1));

}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_testn_epi8_mask (__mmask32 __U, __m256i __A, __m256i __B)
{
  return ((__mmask32)__builtin_ia32_cmpb256_mask((__v32qi)(__m256i)((_mm256_and_si256 (__A, __B))), (__v32qi)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_EQ), (__mmask32)((__U))));

}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_testn_epi16_mask (__m128i __A, __m128i __B)
{
  return ((__mmask8)__builtin_ia32_cmpw128_mask((__v8hi)(__m128i)((_mm_and_si128 (__A, __B))), (__v8hi)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_EQ), (__mmask8)-1));
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_testn_epi16_mask (__mmask8 __U, __m128i __A, __m128i __B)
{
  return ((__mmask8)__builtin_ia32_cmpw128_mask((__v8hi)(__m128i)((_mm_and_si128(__A, __B))), (__v8hi)(__m128i)((_mm_setzero_si128())), (int)(_MM_CMPINT_EQ), (__mmask8)((__U))));
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_testn_epi16_mask (__m256i __A, __m256i __B)
{
  return ((__mmask16)__builtin_ia32_cmpw256_mask((__v16hi)(__m256i)((_mm256_and_si256(__A, __B))), (__v16hi)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_EQ), (__mmask16)-1));

}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_testn_epi16_mask (__mmask16 __U, __m256i __A, __m256i __B)
{
  return ((__mmask16)__builtin_ia32_cmpw256_mask((__v16hi)(__m256i)((_mm256_and_si256 (__A, __B))), (__v16hi)(__m256i)((_mm256_setzero_si256())), (int)(_MM_CMPINT_EQ), (__mmask16)((__U))));

}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_movepi8_mask (__m128i __A)
{
  return (__mmask16) __builtin_ia32_cvtb2mask128 ((__v16qi) __A);
}

static __inline__ __mmask32 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_movepi8_mask (__m256i __A)
{
  return (__mmask32) __builtin_ia32_cvtb2mask256 ((__v32qi) __A);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_movepi16_mask (__m128i __A)
{
  return (__mmask8) __builtin_ia32_cvtw2mask128 ((__v8hi) __A);
}

static __inline__ __mmask16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_movepi16_mask (__m256i __A)
{
  return (__mmask16) __builtin_ia32_cvtw2mask256 ((__v16hi) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_movm_epi8 (__mmask16 __A)
{
  return (__m128i) __builtin_ia32_cvtmask2b128 (__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_movm_epi8 (__mmask32 __A)
{
  return (__m256i) __builtin_ia32_cvtmask2b256 (__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_movm_epi16 (__mmask8 __A)
{
  return (__m128i) __builtin_ia32_cvtmask2w128 (__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_movm_epi16 (__mmask16 __A)
{
  return (__m256i) __builtin_ia32_cvtmask2w256 (__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_broadcastb_epi8 (__m128i __O, __mmask16 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectb_128(__M,
                                             (__v16qi) _mm_broadcastb_epi8(__A),
                                             (__v16qi) __O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_broadcastb_epi8 (__mmask16 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectb_128(__M,
                                             (__v16qi) _mm_broadcastb_epi8(__A),
                                             (__v16qi) _mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_broadcastb_epi8 (__m256i __O, __mmask32 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectb_256(__M,
                                             (__v32qi) _mm256_broadcastb_epi8(__A),
                                             (__v32qi) __O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_broadcastb_epi8 (__mmask32 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectb_256(__M,
                                             (__v32qi) _mm256_broadcastb_epi8(__A),
                                             (__v32qi) _mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_broadcastw_epi16 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectw_128(__M,
                                             (__v8hi) _mm_broadcastw_epi16(__A),
                                             (__v8hi) __O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_broadcastw_epi16 (__mmask8 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectw_128(__M,
                                             (__v8hi) _mm_broadcastw_epi16(__A),
                                             (__v8hi) _mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_broadcastw_epi16 (__m256i __O, __mmask16 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectw_256(__M,
                                             (__v16hi) _mm256_broadcastw_epi16(__A),
                                             (__v16hi) __O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_broadcastw_epi16 (__mmask16 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectw_256(__M,
                                             (__v16hi) _mm256_broadcastw_epi16(__A),
                                             (__v16hi) _mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_set1_epi16 (__m256i __O, __mmask16 __M, short __A)
{
  return (__m256i) __builtin_ia32_selectw_256 (__M,
                                               (__v16hi) _mm256_set1_epi16(__A),
                                               (__v16hi) __O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_set1_epi16 (__mmask16 __M, short __A)
{
  return (__m256i) __builtin_ia32_selectw_256(__M,
                                              (__v16hi)_mm256_set1_epi16(__A),
                                              (__v16hi) _mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_set1_epi16 (__m128i __O, __mmask8 __M, short __A)
{
  return (__m128i) __builtin_ia32_selectw_128(__M,
                                              (__v8hi) _mm_set1_epi16(__A),
                                              (__v8hi) __O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_set1_epi16 (__mmask8 __M, short __A)
{
  return (__m128i) __builtin_ia32_selectw_128(__M,
                                              (__v8hi) _mm_set1_epi16(__A),
                                              (__v8hi) _mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_permutexvar_epi16 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_permvarhi128((__v8hi) __B, (__v8hi) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_maskz_permutexvar_epi16 (__mmask8 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                        (__v8hi)_mm_permutexvar_epi16(__A, __B),
                                        (__v8hi) _mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_permutexvar_epi16 (__m128i __W, __mmask8 __M, __m128i __A,
          __m128i __B)
{
  return (__m128i)__builtin_ia32_selectw_128((__mmask8)__M,
                                        (__v8hi)_mm_permutexvar_epi16(__A, __B),
                                        (__v8hi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_permutexvar_epi16 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_permvarhi256((__v16hi) __B, (__v16hi) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_maskz_permutexvar_epi16 (__mmask16 __M, __m256i __A,
        __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                    (__v16hi)_mm256_permutexvar_epi16(__A, __B),
                                    (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_permutexvar_epi16 (__m256i __W, __mmask16 __M, __m256i __A,
             __m256i __B)
{
  return (__m256i)__builtin_ia32_selectw_256((__mmask16)__M,
                                    (__v16hi)_mm256_permutexvar_epi16(__A, __B),
                                    (__v16hi)__W);
}
# 2839 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbwintrin.h" 3
static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_reduce_add_epi16(__m128i __W) {
  __v8hu __t1 = (__v8hu)__W; __v8hu __t2 = __builtin_shufflevector(__t1, __t1, 4, 5, 6, 7, 4, 5, 6, 7); __v8hu __t3 = __t1 + __t2; __v8hu __t4 = __builtin_shufflevector(__t3, __t3, 2, 3, 2, 3, 4, 5, 6, 7); __v8hu __t5 = __t3 + __t4; __v8hu __t6 = __builtin_shufflevector(__t5, __t5, 1, 1, 2, 3, 4, 5, 6, 7); __v8hu __t7 = __t5 + __t6; return __t7[0];
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_reduce_mul_epi16(__m128i __W) {
  __v8hu __t1 = (__v8hu)__W; __v8hu __t2 = __builtin_shufflevector(__t1, __t1, 4, 5, 6, 7, 4, 5, 6, 7); __v8hu __t3 = __t1 * __t2; __v8hu __t4 = __builtin_shufflevector(__t3, __t3, 2, 3, 2, 3, 4, 5, 6, 7); __v8hu __t5 = __t3 * __t4; __v8hu __t6 = __builtin_shufflevector(__t5, __t5, 1, 1, 2, 3, 4, 5, 6, 7); __v8hu __t7 = __t5 * __t6; return __t7[0];
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_reduce_and_epi16(__m128i __W) {
  __v8hu __t1 = (__v8hu)__W; __v8hu __t2 = __builtin_shufflevector(__t1, __t1, 4, 5, 6, 7, 4, 5, 6, 7); __v8hu __t3 = __t1 & __t2; __v8hu __t4 = __builtin_shufflevector(__t3, __t3, 2, 3, 2, 3, 4, 5, 6, 7); __v8hu __t5 = __t3 & __t4; __v8hu __t6 = __builtin_shufflevector(__t5, __t5, 1, 1, 2, 3, 4, 5, 6, 7); __v8hu __t7 = __t5 & __t6; return __t7[0];
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_reduce_or_epi16(__m128i __W) {
  __v8hu __t1 = (__v8hu)__W; __v8hu __t2 = __builtin_shufflevector(__t1, __t1, 4, 5, 6, 7, 4, 5, 6, 7); __v8hu __t3 = __t1 | __t2; __v8hu __t4 = __builtin_shufflevector(__t3, __t3, 2, 3, 2, 3, 4, 5, 6, 7); __v8hu __t5 = __t3 | __t4; __v8hu __t6 = __builtin_shufflevector(__t5, __t5, 1, 1, 2, 3, 4, 5, 6, 7); __v8hu __t7 = __t5 | __t6; return __t7[0];
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_reduce_add_epi16( __mmask8 __M, __m128i __W) {
  __W = _mm_maskz_mov_epi16(__M, __W);
  __v8hu __t1 = (__v8hu)__W; __v8hu __t2 = __builtin_shufflevector(__t1, __t1, 4, 5, 6, 7, 4, 5, 6, 7); __v8hu __t3 = __t1 + __t2; __v8hu __t4 = __builtin_shufflevector(__t3, __t3, 2, 3, 2, 3, 4, 5, 6, 7); __v8hu __t5 = __t3 + __t4; __v8hu __t6 = __builtin_shufflevector(__t5, __t5, 1, 1, 2, 3, 4, 5, 6, 7); __v8hu __t7 = __t5 + __t6; return __t7[0];
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_reduce_mul_epi16( __mmask8 __M, __m128i __W) {
  __W = _mm_mask_mov_epi16(_mm_set1_epi16(1), __M, __W);
  __v8hu __t1 = (__v8hu)__W; __v8hu __t2 = __builtin_shufflevector(__t1, __t1, 4, 5, 6, 7, 4, 5, 6, 7); __v8hu __t3 = __t1 * __t2; __v8hu __t4 = __builtin_shufflevector(__t3, __t3, 2, 3, 2, 3, 4, 5, 6, 7); __v8hu __t5 = __t3 * __t4; __v8hu __t6 = __builtin_shufflevector(__t5, __t5, 1, 1, 2, 3, 4, 5, 6, 7); __v8hu __t7 = __t5 * __t6; return __t7[0];
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_reduce_and_epi16( __mmask8 __M, __m128i __W) {
  __W = _mm_mask_mov_epi16(_mm_set1_epi16(-1), __M, __W);
  __v8hu __t1 = (__v8hu)__W; __v8hu __t2 = __builtin_shufflevector(__t1, __t1, 4, 5, 6, 7, 4, 5, 6, 7); __v8hu __t3 = __t1 & __t2; __v8hu __t4 = __builtin_shufflevector(__t3, __t3, 2, 3, 2, 3, 4, 5, 6, 7); __v8hu __t5 = __t3 & __t4; __v8hu __t6 = __builtin_shufflevector(__t5, __t5, 1, 1, 2, 3, 4, 5, 6, 7); __v8hu __t7 = __t5 & __t6; return __t7[0];
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_reduce_or_epi16(__mmask8 __M, __m128i __W) {
  __W = _mm_maskz_mov_epi16(__M, __W);
  __v8hu __t1 = (__v8hu)__W; __v8hu __t2 = __builtin_shufflevector(__t1, __t1, 4, 5, 6, 7, 4, 5, 6, 7); __v8hu __t3 = __t1 | __t2; __v8hu __t4 = __builtin_shufflevector(__t3, __t3, 2, 3, 2, 3, 4, 5, 6, 7); __v8hu __t5 = __t3 | __t4; __v8hu __t6 = __builtin_shufflevector(__t5, __t5, 1, 1, 2, 3, 4, 5, 6, 7); __v8hu __t7 = __t5 | __t6; return __t7[0];
}
# 2893 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbwintrin.h" 3
static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_reduce_max_epi16(__m128i __V) {
  __m128i __t1 = (__m128i)__builtin_shufflevector((__v8hi)__V, (__v8hi)__V, 4, 5, 6, 7, 4, 5, 6, 7); __m128i __t2 = _mm_max_epi16(__V, __t1); __m128i __t3 = (__m128i)__builtin_shufflevector((__v8hi)__t2, (__v8hi)__t2, 2, 3, 2, 3, 4, 5, 6, 7); __m128i __t4 = _mm_max_epi16(__t2, __t3); __m128i __t5 = (__m128i)__builtin_shufflevector((__v8hi)__t4, (__v8hi)__t4, 1, 1, 2, 3, 4, 5, 6, 7); __v8hi __t6 = (__v8hi)_mm_max_epi16(__t4, __t5); return __t6[0];
}

static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_reduce_max_epu16(__m128i __V) {
  __m128i __t1 = (__m128i)__builtin_shufflevector((__v8hi)__V, (__v8hi)__V, 4, 5, 6, 7, 4, 5, 6, 7); __m128i __t2 = _mm_max_epu16(__V, __t1); __m128i __t3 = (__m128i)__builtin_shufflevector((__v8hi)__t2, (__v8hi)__t2, 2, 3, 2, 3, 4, 5, 6, 7); __m128i __t4 = _mm_max_epu16(__t2, __t3); __m128i __t5 = (__m128i)__builtin_shufflevector((__v8hi)__t4, (__v8hi)__t4, 1, 1, 2, 3, 4, 5, 6, 7); __v8hi __t6 = (__v8hi)_mm_max_epu16(__t4, __t5); return __t6[0];
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_reduce_min_epi16(__m128i __V) {
  __m128i __t1 = (__m128i)__builtin_shufflevector((__v8hi)__V, (__v8hi)__V, 4, 5, 6, 7, 4, 5, 6, 7); __m128i __t2 = _mm_min_epi16(__V, __t1); __m128i __t3 = (__m128i)__builtin_shufflevector((__v8hi)__t2, (__v8hi)__t2, 2, 3, 2, 3, 4, 5, 6, 7); __m128i __t4 = _mm_min_epi16(__t2, __t3); __m128i __t5 = (__m128i)__builtin_shufflevector((__v8hi)__t4, (__v8hi)__t4, 1, 1, 2, 3, 4, 5, 6, 7); __v8hi __t6 = (__v8hi)_mm_min_epi16(__t4, __t5); return __t6[0];
}

static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_reduce_min_epu16(__m128i __V) {
  __m128i __t1 = (__m128i)__builtin_shufflevector((__v8hi)__V, (__v8hi)__V, 4, 5, 6, 7, 4, 5, 6, 7); __m128i __t2 = _mm_min_epu16(__V, __t1); __m128i __t3 = (__m128i)__builtin_shufflevector((__v8hi)__t2, (__v8hi)__t2, 2, 3, 2, 3, 4, 5, 6, 7); __m128i __t4 = _mm_min_epu16(__t2, __t3); __m128i __t5 = (__m128i)__builtin_shufflevector((__v8hi)__t4, (__v8hi)__t4, 1, 1, 2, 3, 4, 5, 6, 7); __v8hi __t6 = (__v8hi)_mm_min_epu16(__t4, __t5); return __t6[0];
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_reduce_max_epi16(__mmask16 __M, __m128i __V) {
  __V = _mm_mask_mov_epi16(_mm_set1_epi16(-32767-1), __M, __V);
  __m128i __t1 = (__m128i)__builtin_shufflevector((__v8hi)__V, (__v8hi)__V, 4, 5, 6, 7, 4, 5, 6, 7); __m128i __t2 = _mm_max_epi16(__V, __t1); __m128i __t3 = (__m128i)__builtin_shufflevector((__v8hi)__t2, (__v8hi)__t2, 2, 3, 2, 3, 4, 5, 6, 7); __m128i __t4 = _mm_max_epi16(__t2, __t3); __m128i __t5 = (__m128i)__builtin_shufflevector((__v8hi)__t4, (__v8hi)__t4, 1, 1, 2, 3, 4, 5, 6, 7); __v8hi __t6 = (__v8hi)_mm_max_epi16(__t4, __t5); return __t6[0];
}

static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_reduce_max_epu16(__mmask16 __M, __m128i __V) {
  __V = _mm_maskz_mov_epi16(__M, __V);
  __m128i __t1 = (__m128i)__builtin_shufflevector((__v8hi)__V, (__v8hi)__V, 4, 5, 6, 7, 4, 5, 6, 7); __m128i __t2 = _mm_max_epu16(__V, __t1); __m128i __t3 = (__m128i)__builtin_shufflevector((__v8hi)__t2, (__v8hi)__t2, 2, 3, 2, 3, 4, 5, 6, 7); __m128i __t4 = _mm_max_epu16(__t2, __t3); __m128i __t5 = (__m128i)__builtin_shufflevector((__v8hi)__t4, (__v8hi)__t4, 1, 1, 2, 3, 4, 5, 6, 7); __v8hi __t6 = (__v8hi)_mm_max_epu16(__t4, __t5); return __t6[0];
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_reduce_min_epi16(__mmask16 __M, __m128i __V) {
  __V = _mm_mask_mov_epi16(_mm_set1_epi16(32767), __M, __V);
  __m128i __t1 = (__m128i)__builtin_shufflevector((__v8hi)__V, (__v8hi)__V, 4, 5, 6, 7, 4, 5, 6, 7); __m128i __t2 = _mm_min_epi16(__V, __t1); __m128i __t3 = (__m128i)__builtin_shufflevector((__v8hi)__t2, (__v8hi)__t2, 2, 3, 2, 3, 4, 5, 6, 7); __m128i __t4 = _mm_min_epi16(__t2, __t3); __m128i __t5 = (__m128i)__builtin_shufflevector((__v8hi)__t4, (__v8hi)__t4, 1, 1, 2, 3, 4, 5, 6, 7); __v8hi __t6 = (__v8hi)_mm_min_epi16(__t4, __t5); return __t6[0];
}

static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_reduce_min_epu16(__mmask16 __M, __m128i __V) {
  __V = _mm_mask_mov_epi16(_mm_set1_epi16(-1), __M, __V);
  __m128i __t1 = (__m128i)__builtin_shufflevector((__v8hi)__V, (__v8hi)__V, 4, 5, 6, 7, 4, 5, 6, 7); __m128i __t2 = _mm_min_epu16(__V, __t1); __m128i __t3 = (__m128i)__builtin_shufflevector((__v8hi)__t2, (__v8hi)__t2, 2, 3, 2, 3, 4, 5, 6, 7); __m128i __t4 = _mm_min_epu16(__t2, __t3); __m128i __t5 = (__m128i)__builtin_shufflevector((__v8hi)__t4, (__v8hi)__t4, 1, 1, 2, 3, 4, 5, 6, 7); __v8hi __t6 = (__v8hi)_mm_min_epu16(__t4, __t5); return __t6[0];
}
# 2950 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbwintrin.h" 3
static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_reduce_add_epi16(__m256i __W) {
  __v8hu __t1 = (__v8hu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(0))); __v8hu __t2 = (__v8hu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(1))); __v8hu __t3 = __t1 + __t2; __v8hu __t4 = __builtin_shufflevector(__t3, __t3, 4, 5, 6, 7, 4, 5, 6, 7); __v8hu __t5 = __t3 + __t4; __v8hu __t6 = __builtin_shufflevector(__t5, __t5, 2, 3, 2, 3, 4, 5, 6, 7); __v8hu __t7 = __t5 + __t6; __v8hu __t8 = __builtin_shufflevector(__t7, __t7, 1, 1, 2, 3, 4, 5, 6, 7); __v8hu __t9 = __t7 + __t8; return __t9[0];
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_reduce_mul_epi16(__m256i __W) {
  __v8hu __t1 = (__v8hu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(0))); __v8hu __t2 = (__v8hu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(1))); __v8hu __t3 = __t1 * __t2; __v8hu __t4 = __builtin_shufflevector(__t3, __t3, 4, 5, 6, 7, 4, 5, 6, 7); __v8hu __t5 = __t3 * __t4; __v8hu __t6 = __builtin_shufflevector(__t5, __t5, 2, 3, 2, 3, 4, 5, 6, 7); __v8hu __t7 = __t5 * __t6; __v8hu __t8 = __builtin_shufflevector(__t7, __t7, 1, 1, 2, 3, 4, 5, 6, 7); __v8hu __t9 = __t7 * __t8; return __t9[0];
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_reduce_and_epi16(__m256i __W) {
  __v8hu __t1 = (__v8hu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(0))); __v8hu __t2 = (__v8hu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(1))); __v8hu __t3 = __t1 & __t2; __v8hu __t4 = __builtin_shufflevector(__t3, __t3, 4, 5, 6, 7, 4, 5, 6, 7); __v8hu __t5 = __t3 & __t4; __v8hu __t6 = __builtin_shufflevector(__t5, __t5, 2, 3, 2, 3, 4, 5, 6, 7); __v8hu __t7 = __t5 & __t6; __v8hu __t8 = __builtin_shufflevector(__t7, __t7, 1, 1, 2, 3, 4, 5, 6, 7); __v8hu __t9 = __t7 & __t8; return __t9[0];
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_reduce_or_epi16(__m256i __W) {
  __v8hu __t1 = (__v8hu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(0))); __v8hu __t2 = (__v8hu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(1))); __v8hu __t3 = __t1 | __t2; __v8hu __t4 = __builtin_shufflevector(__t3, __t3, 4, 5, 6, 7, 4, 5, 6, 7); __v8hu __t5 = __t3 | __t4; __v8hu __t6 = __builtin_shufflevector(__t5, __t5, 2, 3, 2, 3, 4, 5, 6, 7); __v8hu __t7 = __t5 | __t6; __v8hu __t8 = __builtin_shufflevector(__t7, __t7, 1, 1, 2, 3, 4, 5, 6, 7); __v8hu __t9 = __t7 | __t8; return __t9[0];
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_reduce_add_epi16( __mmask16 __M, __m256i __W) {
  __W = _mm256_maskz_mov_epi16(__M, __W);
  __v8hu __t1 = (__v8hu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(0))); __v8hu __t2 = (__v8hu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(1))); __v8hu __t3 = __t1 + __t2; __v8hu __t4 = __builtin_shufflevector(__t3, __t3, 4, 5, 6, 7, 4, 5, 6, 7); __v8hu __t5 = __t3 + __t4; __v8hu __t6 = __builtin_shufflevector(__t5, __t5, 2, 3, 2, 3, 4, 5, 6, 7); __v8hu __t7 = __t5 + __t6; __v8hu __t8 = __builtin_shufflevector(__t7, __t7, 1, 1, 2, 3, 4, 5, 6, 7); __v8hu __t9 = __t7 + __t8; return __t9[0];
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_reduce_mul_epi16( __mmask16 __M, __m256i __W) {
  __W = _mm256_mask_mov_epi16(_mm256_set1_epi16(1), __M, __W);
  __v8hu __t1 = (__v8hu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(0))); __v8hu __t2 = (__v8hu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(1))); __v8hu __t3 = __t1 * __t2; __v8hu __t4 = __builtin_shufflevector(__t3, __t3, 4, 5, 6, 7, 4, 5, 6, 7); __v8hu __t5 = __t3 * __t4; __v8hu __t6 = __builtin_shufflevector(__t5, __t5, 2, 3, 2, 3, 4, 5, 6, 7); __v8hu __t7 = __t5 * __t6; __v8hu __t8 = __builtin_shufflevector(__t7, __t7, 1, 1, 2, 3, 4, 5, 6, 7); __v8hu __t9 = __t7 * __t8; return __t9[0];
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_reduce_and_epi16( __mmask16 __M, __m256i __W) {
  __W = _mm256_mask_mov_epi16(_mm256_set1_epi16(-1), __M, __W);
  __v8hu __t1 = (__v8hu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(0))); __v8hu __t2 = (__v8hu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(1))); __v8hu __t3 = __t1 & __t2; __v8hu __t4 = __builtin_shufflevector(__t3, __t3, 4, 5, 6, 7, 4, 5, 6, 7); __v8hu __t5 = __t3 & __t4; __v8hu __t6 = __builtin_shufflevector(__t5, __t5, 2, 3, 2, 3, 4, 5, 6, 7); __v8hu __t7 = __t5 & __t6; __v8hu __t8 = __builtin_shufflevector(__t7, __t7, 1, 1, 2, 3, 4, 5, 6, 7); __v8hu __t9 = __t7 & __t8; return __t9[0];
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_reduce_or_epi16(__mmask16 __M, __m256i __W) {
  __W = _mm256_maskz_mov_epi16(__M, __W);
  __v8hu __t1 = (__v8hu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(0))); __v8hu __t2 = (__v8hu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(1))); __v8hu __t3 = __t1 | __t2; __v8hu __t4 = __builtin_shufflevector(__t3, __t3, 4, 5, 6, 7, 4, 5, 6, 7); __v8hu __t5 = __t3 | __t4; __v8hu __t6 = __builtin_shufflevector(__t5, __t5, 2, 3, 2, 3, 4, 5, 6, 7); __v8hu __t7 = __t5 | __t6; __v8hu __t8 = __builtin_shufflevector(__t7, __t7, 1, 1, 2, 3, 4, 5, 6, 7); __v8hu __t9 = __t7 | __t8; return __t9[0];
}
# 3007 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbwintrin.h" 3
static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_reduce_max_epi16(__m256i __V) {
  __m128i __t1 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(0))); __m128i __t2 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(1))); __m128i __t3 = _mm_max_epi16(__t1, __t2); __m128i __t4 = (__m128i)__builtin_shufflevector((__v8hi)__t3, (__v8hi)__t3, 4, 5, 6, 7, 4, 5, 6, 7); __m128i __t5 = _mm_max_epi16(__t3, __t4); __m128i __t6 = (__m128i)__builtin_shufflevector((__v8hi)__t5, (__v8hi)__t5, 2, 3, 2, 3, 4, 5, 6, 7); __m128i __t7 = _mm_max_epi16(__t5, __t6); __m128i __t8 = (__m128i)__builtin_shufflevector((__v8hi)__t7, (__v8hi)__t7, 1, 1, 2, 3, 4, 5, 6, 7); __v8hi __t9 = (__v8hi)_mm_max_epi16(__t7, __t8); return __t9[0];
}

static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_reduce_max_epu16(__m256i __V) {
  __m128i __t1 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(0))); __m128i __t2 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(1))); __m128i __t3 = _mm_max_epu16(__t1, __t2); __m128i __t4 = (__m128i)__builtin_shufflevector((__v8hi)__t3, (__v8hi)__t3, 4, 5, 6, 7, 4, 5, 6, 7); __m128i __t5 = _mm_max_epu16(__t3, __t4); __m128i __t6 = (__m128i)__builtin_shufflevector((__v8hi)__t5, (__v8hi)__t5, 2, 3, 2, 3, 4, 5, 6, 7); __m128i __t7 = _mm_max_epu16(__t5, __t6); __m128i __t8 = (__m128i)__builtin_shufflevector((__v8hi)__t7, (__v8hi)__t7, 1, 1, 2, 3, 4, 5, 6, 7); __v8hi __t9 = (__v8hi)_mm_max_epu16(__t7, __t8); return __t9[0];
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_reduce_min_epi16(__m256i __V) {
  __m128i __t1 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(0))); __m128i __t2 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(1))); __m128i __t3 = _mm_min_epi16(__t1, __t2); __m128i __t4 = (__m128i)__builtin_shufflevector((__v8hi)__t3, (__v8hi)__t3, 4, 5, 6, 7, 4, 5, 6, 7); __m128i __t5 = _mm_min_epi16(__t3, __t4); __m128i __t6 = (__m128i)__builtin_shufflevector((__v8hi)__t5, (__v8hi)__t5, 2, 3, 2, 3, 4, 5, 6, 7); __m128i __t7 = _mm_min_epi16(__t5, __t6); __m128i __t8 = (__m128i)__builtin_shufflevector((__v8hi)__t7, (__v8hi)__t7, 1, 1, 2, 3, 4, 5, 6, 7); __v8hi __t9 = (__v8hi)_mm_min_epi16(__t7, __t8); return __t9[0];
}

static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_reduce_min_epu16(__m256i __V) {
  __m128i __t1 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(0))); __m128i __t2 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(1))); __m128i __t3 = _mm_min_epu16(__t1, __t2); __m128i __t4 = (__m128i)__builtin_shufflevector((__v8hi)__t3, (__v8hi)__t3, 4, 5, 6, 7, 4, 5, 6, 7); __m128i __t5 = _mm_min_epu16(__t3, __t4); __m128i __t6 = (__m128i)__builtin_shufflevector((__v8hi)__t5, (__v8hi)__t5, 2, 3, 2, 3, 4, 5, 6, 7); __m128i __t7 = _mm_min_epu16(__t5, __t6); __m128i __t8 = (__m128i)__builtin_shufflevector((__v8hi)__t7, (__v8hi)__t7, 1, 1, 2, 3, 4, 5, 6, 7); __v8hi __t9 = (__v8hi)_mm_min_epu16(__t7, __t8); return __t9[0];
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_reduce_max_epi16(__mmask16 __M, __m256i __V) {
  __V = _mm256_mask_mov_epi16(_mm256_set1_epi16(-32767-1), __M, __V);
  __m128i __t1 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(0))); __m128i __t2 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(1))); __m128i __t3 = _mm_max_epi16(__t1, __t2); __m128i __t4 = (__m128i)__builtin_shufflevector((__v8hi)__t3, (__v8hi)__t3, 4, 5, 6, 7, 4, 5, 6, 7); __m128i __t5 = _mm_max_epi16(__t3, __t4); __m128i __t6 = (__m128i)__builtin_shufflevector((__v8hi)__t5, (__v8hi)__t5, 2, 3, 2, 3, 4, 5, 6, 7); __m128i __t7 = _mm_max_epi16(__t5, __t6); __m128i __t8 = (__m128i)__builtin_shufflevector((__v8hi)__t7, (__v8hi)__t7, 1, 1, 2, 3, 4, 5, 6, 7); __v8hi __t9 = (__v8hi)_mm_max_epi16(__t7, __t8); return __t9[0];
}

static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_reduce_max_epu16(__mmask16 __M, __m256i __V) {
  __V = _mm256_maskz_mov_epi16(__M, __V);
  __m128i __t1 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(0))); __m128i __t2 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(1))); __m128i __t3 = _mm_max_epu16(__t1, __t2); __m128i __t4 = (__m128i)__builtin_shufflevector((__v8hi)__t3, (__v8hi)__t3, 4, 5, 6, 7, 4, 5, 6, 7); __m128i __t5 = _mm_max_epu16(__t3, __t4); __m128i __t6 = (__m128i)__builtin_shufflevector((__v8hi)__t5, (__v8hi)__t5, 2, 3, 2, 3, 4, 5, 6, 7); __m128i __t7 = _mm_max_epu16(__t5, __t6); __m128i __t8 = (__m128i)__builtin_shufflevector((__v8hi)__t7, (__v8hi)__t7, 1, 1, 2, 3, 4, 5, 6, 7); __v8hi __t9 = (__v8hi)_mm_max_epu16(__t7, __t8); return __t9[0];
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_reduce_min_epi16(__mmask16 __M, __m256i __V) {
  __V = _mm256_mask_mov_epi16(_mm256_set1_epi16(32767), __M, __V);
  __m128i __t1 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(0))); __m128i __t2 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(1))); __m128i __t3 = _mm_min_epi16(__t1, __t2); __m128i __t4 = (__m128i)__builtin_shufflevector((__v8hi)__t3, (__v8hi)__t3, 4, 5, 6, 7, 4, 5, 6, 7); __m128i __t5 = _mm_min_epi16(__t3, __t4); __m128i __t6 = (__m128i)__builtin_shufflevector((__v8hi)__t5, (__v8hi)__t5, 2, 3, 2, 3, 4, 5, 6, 7); __m128i __t7 = _mm_min_epi16(__t5, __t6); __m128i __t8 = (__m128i)__builtin_shufflevector((__v8hi)__t7, (__v8hi)__t7, 1, 1, 2, 3, 4, 5, 6, 7); __v8hi __t9 = (__v8hi)_mm_min_epi16(__t7, __t8); return __t9[0];
}

static __inline__ unsigned short __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_reduce_min_epu16(__mmask16 __M, __m256i __V) {
  __V = _mm256_mask_mov_epi16(_mm256_set1_epi16(-1), __M, __V);
  __m128i __t1 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(0))); __m128i __t2 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(1))); __m128i __t3 = _mm_min_epu16(__t1, __t2); __m128i __t4 = (__m128i)__builtin_shufflevector((__v8hi)__t3, (__v8hi)__t3, 4, 5, 6, 7, 4, 5, 6, 7); __m128i __t5 = _mm_min_epu16(__t3, __t4); __m128i __t6 = (__m128i)__builtin_shufflevector((__v8hi)__t5, (__v8hi)__t5, 2, 3, 2, 3, 4, 5, 6, 7); __m128i __t7 = _mm_min_epu16(__t5, __t6); __m128i __t8 = (__m128i)__builtin_shufflevector((__v8hi)__t7, (__v8hi)__t7, 1, 1, 2, 3, 4, 5, 6, 7); __v8hi __t9 = (__v8hi)_mm_min_epu16(__t7, __t8); return __t9[0];
}
# 3064 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbwintrin.h" 3
static __inline__ char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_reduce_add_epi8(__m128i __W) {
  __v16qu __t1 = (__v16qu)__W; __v16qu __t2 = __builtin_shufflevector(__t1, __t1, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t3 = __t1 + __t2; __v16qu __t4 = __builtin_shufflevector(__t3, __t3, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t5 = __t3 + __t4; __v16qu __t6 = __builtin_shufflevector(__t5, __t5, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t7 = __t5 + __t6; __v16qu __t8 = __builtin_shufflevector(__t7, __t7, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t9 = __t7 + __t8; return __t9[0];
}

static __inline__ char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_reduce_mul_epi8(__m128i __W) {
  __v16qu __t1 = (__v16qu)__W; __v16qu __t2 = __builtin_shufflevector(__t1, __t1, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t3 = __t1 * __t2; __v16qu __t4 = __builtin_shufflevector(__t3, __t3, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t5 = __t3 * __t4; __v16qu __t6 = __builtin_shufflevector(__t5, __t5, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t7 = __t5 * __t6; __v16qu __t8 = __builtin_shufflevector(__t7, __t7, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t9 = __t7 * __t8; return __t9[0];
}

static __inline__ char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_reduce_and_epi8(__m128i __W) {
  __v16qu __t1 = (__v16qu)__W; __v16qu __t2 = __builtin_shufflevector(__t1, __t1, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t3 = __t1 & __t2; __v16qu __t4 = __builtin_shufflevector(__t3, __t3, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t5 = __t3 & __t4; __v16qu __t6 = __builtin_shufflevector(__t5, __t5, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t7 = __t5 & __t6; __v16qu __t8 = __builtin_shufflevector(__t7, __t7, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t9 = __t7 & __t8; return __t9[0];
}

static __inline__ char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_reduce_or_epi8(__m128i __W) {
  __v16qu __t1 = (__v16qu)__W; __v16qu __t2 = __builtin_shufflevector(__t1, __t1, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t3 = __t1 | __t2; __v16qu __t4 = __builtin_shufflevector(__t3, __t3, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t5 = __t3 | __t4; __v16qu __t6 = __builtin_shufflevector(__t5, __t5, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t7 = __t5 | __t6; __v16qu __t8 = __builtin_shufflevector(__t7, __t7, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t9 = __t7 | __t8; return __t9[0];
}

static __inline__ char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_reduce_add_epi8( __mmask16 __M, __m128i __W) {
  __W = _mm_maskz_mov_epi8(__M, __W);
  __v16qu __t1 = (__v16qu)__W; __v16qu __t2 = __builtin_shufflevector(__t1, __t1, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t3 = __t1 + __t2; __v16qu __t4 = __builtin_shufflevector(__t3, __t3, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t5 = __t3 + __t4; __v16qu __t6 = __builtin_shufflevector(__t5, __t5, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t7 = __t5 + __t6; __v16qu __t8 = __builtin_shufflevector(__t7, __t7, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t9 = __t7 + __t8; return __t9[0];
}

static __inline__ char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_reduce_mul_epi8( __mmask16 __M, __m128i __W) {
  __W = _mm_mask_mov_epi8(_mm_set1_epi8(1), __M, __W);
  __v16qu __t1 = (__v16qu)__W; __v16qu __t2 = __builtin_shufflevector(__t1, __t1, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t3 = __t1 * __t2; __v16qu __t4 = __builtin_shufflevector(__t3, __t3, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t5 = __t3 * __t4; __v16qu __t6 = __builtin_shufflevector(__t5, __t5, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t7 = __t5 * __t6; __v16qu __t8 = __builtin_shufflevector(__t7, __t7, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t9 = __t7 * __t8; return __t9[0];
}

static __inline__ char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_reduce_and_epi8( __mmask16 __M, __m128i __W) {
  __W = _mm_mask_mov_epi8(_mm_set1_epi8(-1), __M, __W);
  __v16qu __t1 = (__v16qu)__W; __v16qu __t2 = __builtin_shufflevector(__t1, __t1, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t3 = __t1 & __t2; __v16qu __t4 = __builtin_shufflevector(__t3, __t3, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t5 = __t3 & __t4; __v16qu __t6 = __builtin_shufflevector(__t5, __t5, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t7 = __t5 & __t6; __v16qu __t8 = __builtin_shufflevector(__t7, __t7, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t9 = __t7 & __t8; return __t9[0];
}

static __inline__ char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_reduce_or_epi8(__mmask16 __M, __m128i __W) {
  __W = _mm_maskz_mov_epi8(__M, __W);
  __v16qu __t1 = (__v16qu)__W; __v16qu __t2 = __builtin_shufflevector(__t1, __t1, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t3 = __t1 | __t2; __v16qu __t4 = __builtin_shufflevector(__t3, __t3, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t5 = __t3 | __t4; __v16qu __t6 = __builtin_shufflevector(__t5, __t5, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t7 = __t5 | __t6; __v16qu __t8 = __builtin_shufflevector(__t7, __t7, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t9 = __t7 | __t8; return __t9[0];
}
# 3120 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbwintrin.h" 3
static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_reduce_max_epi8(__m128i __V) {
  __m128i __t1 = (__m128i)__builtin_shufflevector((__v16qi)__V, (__v16qi)__V, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t2 = _mm_max_epi8(__V, __t1); __m128i __t3 = (__m128i)__builtin_shufflevector((__v16qi)__t2, (__v16qi)__t2, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t4 = _mm_max_epi8(__t2, __t3); __m128i __t5 = (__m128i)__builtin_shufflevector((__v16qi)__t4, (__v16qi)__t4, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t6 = _mm_max_epi8(__t4, __t5); __m128i __t7 = (__m128i)__builtin_shufflevector((__v16qi)__t6, (__v16qi)__t6, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qi __t8 = (__v16qi)_mm_max_epi8(__t6, __t7); return __t8[0];
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_reduce_max_epu8(__m128i __V) {
  __m128i __t1 = (__m128i)__builtin_shufflevector((__v16qi)__V, (__v16qi)__V, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t2 = _mm_max_epu8(__V, __t1); __m128i __t3 = (__m128i)__builtin_shufflevector((__v16qi)__t2, (__v16qi)__t2, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t4 = _mm_max_epu8(__t2, __t3); __m128i __t5 = (__m128i)__builtin_shufflevector((__v16qi)__t4, (__v16qi)__t4, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t6 = _mm_max_epu8(__t4, __t5); __m128i __t7 = (__m128i)__builtin_shufflevector((__v16qi)__t6, (__v16qi)__t6, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qi __t8 = (__v16qi)_mm_max_epu8(__t6, __t7); return __t8[0];
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_reduce_min_epi8(__m128i __V) {
  __m128i __t1 = (__m128i)__builtin_shufflevector((__v16qi)__V, (__v16qi)__V, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t2 = _mm_min_epi8(__V, __t1); __m128i __t3 = (__m128i)__builtin_shufflevector((__v16qi)__t2, (__v16qi)__t2, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t4 = _mm_min_epi8(__t2, __t3); __m128i __t5 = (__m128i)__builtin_shufflevector((__v16qi)__t4, (__v16qi)__t4, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t6 = _mm_min_epi8(__t4, __t5); __m128i __t7 = (__m128i)__builtin_shufflevector((__v16qi)__t6, (__v16qi)__t6, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qi __t8 = (__v16qi)_mm_min_epi8(__t6, __t7); return __t8[0];
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_reduce_min_epu8(__m128i __V) {
  __m128i __t1 = (__m128i)__builtin_shufflevector((__v16qi)__V, (__v16qi)__V, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t2 = _mm_min_epu8(__V, __t1); __m128i __t3 = (__m128i)__builtin_shufflevector((__v16qi)__t2, (__v16qi)__t2, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t4 = _mm_min_epu8(__t2, __t3); __m128i __t5 = (__m128i)__builtin_shufflevector((__v16qi)__t4, (__v16qi)__t4, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t6 = _mm_min_epu8(__t4, __t5); __m128i __t7 = (__m128i)__builtin_shufflevector((__v16qi)__t6, (__v16qi)__t6, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qi __t8 = (__v16qi)_mm_min_epu8(__t6, __t7); return __t8[0];
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_reduce_max_epi8(__mmask16 __M, __m128i __V) {
  __V = _mm_mask_mov_epi8(_mm_set1_epi8(-127-1), __M, __V);
  __m128i __t1 = (__m128i)__builtin_shufflevector((__v16qi)__V, (__v16qi)__V, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t2 = _mm_max_epi8(__V, __t1); __m128i __t3 = (__m128i)__builtin_shufflevector((__v16qi)__t2, (__v16qi)__t2, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t4 = _mm_max_epi8(__t2, __t3); __m128i __t5 = (__m128i)__builtin_shufflevector((__v16qi)__t4, (__v16qi)__t4, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t6 = _mm_max_epi8(__t4, __t5); __m128i __t7 = (__m128i)__builtin_shufflevector((__v16qi)__t6, (__v16qi)__t6, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qi __t8 = (__v16qi)_mm_max_epi8(__t6, __t7); return __t8[0];
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_reduce_max_epu8(__mmask16 __M, __m128i __V) {
  __V = _mm_maskz_mov_epi8(__M, __V);
  __m128i __t1 = (__m128i)__builtin_shufflevector((__v16qi)__V, (__v16qi)__V, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t2 = _mm_max_epu8(__V, __t1); __m128i __t3 = (__m128i)__builtin_shufflevector((__v16qi)__t2, (__v16qi)__t2, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t4 = _mm_max_epu8(__t2, __t3); __m128i __t5 = (__m128i)__builtin_shufflevector((__v16qi)__t4, (__v16qi)__t4, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t6 = _mm_max_epu8(__t4, __t5); __m128i __t7 = (__m128i)__builtin_shufflevector((__v16qi)__t6, (__v16qi)__t6, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qi __t8 = (__v16qi)_mm_max_epu8(__t6, __t7); return __t8[0];
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_reduce_min_epi8(__mmask16 __M, __m128i __V) {
  __V = _mm_mask_mov_epi8(_mm_set1_epi8(127), __M, __V);
  __m128i __t1 = (__m128i)__builtin_shufflevector((__v16qi)__V, (__v16qi)__V, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t2 = _mm_min_epi8(__V, __t1); __m128i __t3 = (__m128i)__builtin_shufflevector((__v16qi)__t2, (__v16qi)__t2, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t4 = _mm_min_epi8(__t2, __t3); __m128i __t5 = (__m128i)__builtin_shufflevector((__v16qi)__t4, (__v16qi)__t4, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t6 = _mm_min_epi8(__t4, __t5); __m128i __t7 = (__m128i)__builtin_shufflevector((__v16qi)__t6, (__v16qi)__t6, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qi __t8 = (__v16qi)_mm_min_epi8(__t6, __t7); return __t8[0];
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(128)))
_mm_mask_reduce_min_epu8(__mmask16 __M, __m128i __V) {
  __V = _mm_mask_mov_epi8(_mm_set1_epi8(-1), __M, __V);
  __m128i __t1 = (__m128i)__builtin_shufflevector((__v16qi)__V, (__v16qi)__V, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t2 = _mm_min_epu8(__V, __t1); __m128i __t3 = (__m128i)__builtin_shufflevector((__v16qi)__t2, (__v16qi)__t2, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t4 = _mm_min_epu8(__t2, __t3); __m128i __t5 = (__m128i)__builtin_shufflevector((__v16qi)__t4, (__v16qi)__t4, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t6 = _mm_min_epu8(__t4, __t5); __m128i __t7 = (__m128i)__builtin_shufflevector((__v16qi)__t6, (__v16qi)__t6, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qi __t8 = (__v16qi)_mm_min_epu8(__t6, __t7); return __t8[0];
}
# 3179 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbwintrin.h" 3
static __inline__ char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_reduce_add_epi8(__m256i __W) {
  __v16qu __t1 = (__v16qu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(0))); __v16qu __t2 = (__v16qu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(1))); __v16qu __t3 = __t1 + __t2; __v16qu __t4 = __builtin_shufflevector(__t3, __t3, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t5 = __t3 + __t4; __v16qu __t6 = __builtin_shufflevector(__t5, __t5, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t7 = __t5 + __t6; __v16qu __t8 = __builtin_shufflevector(__t7, __t7, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t9 = __t7 + __t8; __v16qu __t10 = __builtin_shufflevector(__t9, __t9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t11 = __t9 + __t10; return __t11[0];
}

static __inline__ char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_reduce_mul_epi8(__m256i __W) {
  __v16qu __t1 = (__v16qu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(0))); __v16qu __t2 = (__v16qu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(1))); __v16qu __t3 = __t1 * __t2; __v16qu __t4 = __builtin_shufflevector(__t3, __t3, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t5 = __t3 * __t4; __v16qu __t6 = __builtin_shufflevector(__t5, __t5, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t7 = __t5 * __t6; __v16qu __t8 = __builtin_shufflevector(__t7, __t7, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t9 = __t7 * __t8; __v16qu __t10 = __builtin_shufflevector(__t9, __t9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t11 = __t9 * __t10; return __t11[0];
}

static __inline__ char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_reduce_and_epi8(__m256i __W) {
  __v16qu __t1 = (__v16qu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(0))); __v16qu __t2 = (__v16qu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(1))); __v16qu __t3 = __t1 & __t2; __v16qu __t4 = __builtin_shufflevector(__t3, __t3, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t5 = __t3 & __t4; __v16qu __t6 = __builtin_shufflevector(__t5, __t5, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t7 = __t5 & __t6; __v16qu __t8 = __builtin_shufflevector(__t7, __t7, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t9 = __t7 & __t8; __v16qu __t10 = __builtin_shufflevector(__t9, __t9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t11 = __t9 & __t10; return __t11[0];
}

static __inline__ char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_reduce_or_epi8(__m256i __W) {
  __v16qu __t1 = (__v16qu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(0))); __v16qu __t2 = (__v16qu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(1))); __v16qu __t3 = __t1 | __t2; __v16qu __t4 = __builtin_shufflevector(__t3, __t3, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t5 = __t3 | __t4; __v16qu __t6 = __builtin_shufflevector(__t5, __t5, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t7 = __t5 | __t6; __v16qu __t8 = __builtin_shufflevector(__t7, __t7, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t9 = __t7 | __t8; __v16qu __t10 = __builtin_shufflevector(__t9, __t9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t11 = __t9 | __t10; return __t11[0];
}

static __inline__ char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_reduce_add_epi8( __mmask32 __M, __m256i __W) {
  __W = _mm256_maskz_mov_epi8(__M, __W);
  __v16qu __t1 = (__v16qu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(0))); __v16qu __t2 = (__v16qu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(1))); __v16qu __t3 = __t1 + __t2; __v16qu __t4 = __builtin_shufflevector(__t3, __t3, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t5 = __t3 + __t4; __v16qu __t6 = __builtin_shufflevector(__t5, __t5, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t7 = __t5 + __t6; __v16qu __t8 = __builtin_shufflevector(__t7, __t7, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t9 = __t7 + __t8; __v16qu __t10 = __builtin_shufflevector(__t9, __t9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t11 = __t9 + __t10; return __t11[0];
}

static __inline__ char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_reduce_mul_epi8( __mmask32 __M, __m256i __W) {
  __W = _mm256_mask_mov_epi8(_mm256_set1_epi8(1), __M, __W);
  __v16qu __t1 = (__v16qu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(0))); __v16qu __t2 = (__v16qu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(1))); __v16qu __t3 = __t1 * __t2; __v16qu __t4 = __builtin_shufflevector(__t3, __t3, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t5 = __t3 * __t4; __v16qu __t6 = __builtin_shufflevector(__t5, __t5, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t7 = __t5 * __t6; __v16qu __t8 = __builtin_shufflevector(__t7, __t7, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t9 = __t7 * __t8; __v16qu __t10 = __builtin_shufflevector(__t9, __t9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t11 = __t9 * __t10; return __t11[0];
}

static __inline__ char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_reduce_and_epi8( __mmask32 __M, __m256i __W) {
  __W = _mm256_mask_mov_epi8(_mm256_set1_epi8(-1), __M, __W);
  __v16qu __t1 = (__v16qu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(0))); __v16qu __t2 = (__v16qu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(1))); __v16qu __t3 = __t1 & __t2; __v16qu __t4 = __builtin_shufflevector(__t3, __t3, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t5 = __t3 & __t4; __v16qu __t6 = __builtin_shufflevector(__t5, __t5, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t7 = __t5 & __t6; __v16qu __t8 = __builtin_shufflevector(__t7, __t7, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t9 = __t7 & __t8; __v16qu __t10 = __builtin_shufflevector(__t9, __t9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t11 = __t9 & __t10; return __t11[0];
}

static __inline__ char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_reduce_or_epi8(__mmask32 __M, __m256i __W) {
  __W = _mm256_maskz_mov_epi8(__M, __W);
  __v16qu __t1 = (__v16qu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(0))); __v16qu __t2 = (__v16qu)((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__W), (int)(1))); __v16qu __t3 = __t1 | __t2; __v16qu __t4 = __builtin_shufflevector(__t3, __t3, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t5 = __t3 | __t4; __v16qu __t6 = __builtin_shufflevector(__t5, __t5, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t7 = __t5 | __t6; __v16qu __t8 = __builtin_shufflevector(__t7, __t7, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t9 = __t7 | __t8; __v16qu __t10 = __builtin_shufflevector(__t9, __t9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qu __t11 = __t9 | __t10; return __t11[0];
}
# 3238 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbwintrin.h" 3
static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_reduce_max_epi8(__m256i __V) {
  __m128i __t1 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(0))); __m128i __t2 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(1))); __m128i __t3 = _mm_max_epi8(__t1, __t2); __m128i __t4 = (__m128i)__builtin_shufflevector((__v16qi)__t3, (__v16qi)__t3, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t5 = _mm_max_epi8(__t3, __t4); __m128i __t6 = (__m128i)__builtin_shufflevector((__v16qi)__t5, (__v16qi)__t5, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t7 = _mm_max_epi8(__t5, __t6); __m128i __t8 = (__m128i)__builtin_shufflevector((__v16qi)__t7, (__v16qi)__t5, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t9 = _mm_max_epi8(__t7, __t8); __m128i __t10 = (__m128i)__builtin_shufflevector((__v16qi)__t9, (__v16qi)__t9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qi __t11 = (__v16qi)_mm_max_epi8(__t9, __t10); return __t11[0];
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_reduce_max_epu8(__m256i __V) {
  __m128i __t1 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(0))); __m128i __t2 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(1))); __m128i __t3 = _mm_max_epu8(__t1, __t2); __m128i __t4 = (__m128i)__builtin_shufflevector((__v16qi)__t3, (__v16qi)__t3, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t5 = _mm_max_epu8(__t3, __t4); __m128i __t6 = (__m128i)__builtin_shufflevector((__v16qi)__t5, (__v16qi)__t5, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t7 = _mm_max_epu8(__t5, __t6); __m128i __t8 = (__m128i)__builtin_shufflevector((__v16qi)__t7, (__v16qi)__t5, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t9 = _mm_max_epu8(__t7, __t8); __m128i __t10 = (__m128i)__builtin_shufflevector((__v16qi)__t9, (__v16qi)__t9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qi __t11 = (__v16qi)_mm_max_epu8(__t9, __t10); return __t11[0];
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_reduce_min_epi8(__m256i __V) {
  __m128i __t1 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(0))); __m128i __t2 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(1))); __m128i __t3 = _mm_min_epi8(__t1, __t2); __m128i __t4 = (__m128i)__builtin_shufflevector((__v16qi)__t3, (__v16qi)__t3, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t5 = _mm_min_epi8(__t3, __t4); __m128i __t6 = (__m128i)__builtin_shufflevector((__v16qi)__t5, (__v16qi)__t5, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t7 = _mm_min_epi8(__t5, __t6); __m128i __t8 = (__m128i)__builtin_shufflevector((__v16qi)__t7, (__v16qi)__t5, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t9 = _mm_min_epi8(__t7, __t8); __m128i __t10 = (__m128i)__builtin_shufflevector((__v16qi)__t9, (__v16qi)__t9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qi __t11 = (__v16qi)_mm_min_epi8(__t9, __t10); return __t11[0];
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_reduce_min_epu8(__m256i __V) {
  __m128i __t1 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(0))); __m128i __t2 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(1))); __m128i __t3 = _mm_min_epu8(__t1, __t2); __m128i __t4 = (__m128i)__builtin_shufflevector((__v16qi)__t3, (__v16qi)__t3, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t5 = _mm_min_epu8(__t3, __t4); __m128i __t6 = (__m128i)__builtin_shufflevector((__v16qi)__t5, (__v16qi)__t5, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t7 = _mm_min_epu8(__t5, __t6); __m128i __t8 = (__m128i)__builtin_shufflevector((__v16qi)__t7, (__v16qi)__t5, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t9 = _mm_min_epu8(__t7, __t8); __m128i __t10 = (__m128i)__builtin_shufflevector((__v16qi)__t9, (__v16qi)__t9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qi __t11 = (__v16qi)_mm_min_epu8(__t9, __t10); return __t11[0];
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_reduce_max_epi8(__mmask32 __M, __m256i __V) {
  __V = _mm256_mask_mov_epi8(_mm256_set1_epi8(-127-1), __M, __V);
  __m128i __t1 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(0))); __m128i __t2 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(1))); __m128i __t3 = _mm_max_epi8(__t1, __t2); __m128i __t4 = (__m128i)__builtin_shufflevector((__v16qi)__t3, (__v16qi)__t3, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t5 = _mm_max_epi8(__t3, __t4); __m128i __t6 = (__m128i)__builtin_shufflevector((__v16qi)__t5, (__v16qi)__t5, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t7 = _mm_max_epi8(__t5, __t6); __m128i __t8 = (__m128i)__builtin_shufflevector((__v16qi)__t7, (__v16qi)__t5, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t9 = _mm_max_epi8(__t7, __t8); __m128i __t10 = (__m128i)__builtin_shufflevector((__v16qi)__t9, (__v16qi)__t9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qi __t11 = (__v16qi)_mm_max_epi8(__t9, __t10); return __t11[0];
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_reduce_max_epu8(__mmask32 __M, __m256i __V) {
  __V = _mm256_maskz_mov_epi8(__M, __V);
  __m128i __t1 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(0))); __m128i __t2 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(1))); __m128i __t3 = _mm_max_epu8(__t1, __t2); __m128i __t4 = (__m128i)__builtin_shufflevector((__v16qi)__t3, (__v16qi)__t3, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t5 = _mm_max_epu8(__t3, __t4); __m128i __t6 = (__m128i)__builtin_shufflevector((__v16qi)__t5, (__v16qi)__t5, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t7 = _mm_max_epu8(__t5, __t6); __m128i __t8 = (__m128i)__builtin_shufflevector((__v16qi)__t7, (__v16qi)__t5, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t9 = _mm_max_epu8(__t7, __t8); __m128i __t10 = (__m128i)__builtin_shufflevector((__v16qi)__t9, (__v16qi)__t9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qi __t11 = (__v16qi)_mm_max_epu8(__t9, __t10); return __t11[0];
}

static __inline__ signed char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_reduce_min_epi8(__mmask32 __M, __m256i __V) {
  __V = _mm256_mask_mov_epi8(_mm256_set1_epi8(127), __M, __V);
  __m128i __t1 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(0))); __m128i __t2 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(1))); __m128i __t3 = _mm_min_epi8(__t1, __t2); __m128i __t4 = (__m128i)__builtin_shufflevector((__v16qi)__t3, (__v16qi)__t3, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t5 = _mm_min_epi8(__t3, __t4); __m128i __t6 = (__m128i)__builtin_shufflevector((__v16qi)__t5, (__v16qi)__t5, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t7 = _mm_min_epi8(__t5, __t6); __m128i __t8 = (__m128i)__builtin_shufflevector((__v16qi)__t7, (__v16qi)__t5, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t9 = _mm_min_epi8(__t7, __t8); __m128i __t10 = (__m128i)__builtin_shufflevector((__v16qi)__t9, (__v16qi)__t9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qi __t11 = (__v16qi)_mm_min_epi8(__t9, __t10); return __t11[0];
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512bw"), __min_vector_width__(256)))
_mm256_mask_reduce_min_epu8(__mmask32 __M, __m256i __V) {
  __V = _mm256_mask_mov_epi8(_mm256_set1_epi8(-1), __M, __V);
  __m128i __t1 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(0))); __m128i __t2 = ((__m128i)__builtin_ia32_extract128i256((__v4di)(__m256i)(__V), (int)(1))); __m128i __t3 = _mm_min_epu8(__t1, __t2); __m128i __t4 = (__m128i)__builtin_shufflevector((__v16qi)__t3, (__v16qi)__t3, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t5 = _mm_min_epu8(__t3, __t4); __m128i __t6 = (__m128i)__builtin_shufflevector((__v16qi)__t5, (__v16qi)__t5, 4, 5, 6, 7, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t7 = _mm_min_epu8(__t5, __t6); __m128i __t8 = (__m128i)__builtin_shufflevector((__v16qi)__t7, (__v16qi)__t5, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __m128i __t9 = _mm_min_epu8(__t7, __t8); __m128i __t10 = (__m128i)__builtin_shufflevector((__v16qi)__t9, (__v16qi)__t9, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15); __v16qi __t11 = (__v16qi)_mm_min_epu8(__t9, __t10); return __t11[0];
}
# 208 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3





# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlcdintrin.h" 1 3
# 35 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlcdintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(128)))
_mm_broadcastmb_epi64 (__mmask8 __A)
{
  return (__m128i) _mm_set1_epi64x((long long) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(256)))
_mm256_broadcastmb_epi64 (__mmask8 __A)
{
  return (__m256i) _mm256_set1_epi64x((long long)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(128)))
_mm_broadcastmw_epi32 (__mmask16 __A)
{
  return (__m128i) _mm_set1_epi32((int)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(256)))
_mm256_broadcastmw_epi32 (__mmask16 __A)
{
  return (__m256i) _mm256_set1_epi32((int)__A);
}


static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(128)))
_mm_conflict_epi64 (__m128i __A)
{
  return (__m128i) __builtin_ia32_vpconflictdi_128 ((__v2di) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(128)))
_mm_mask_conflict_epi64 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_conflict_epi64(__A),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(128)))
_mm_maskz_conflict_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_conflict_epi64(__A),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(256)))
_mm256_conflict_epi64 (__m256i __A)
{
  return (__m256i) __builtin_ia32_vpconflictdi_256 ((__v4di) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(256)))
_mm256_mask_conflict_epi64 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_conflict_epi64(__A),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(256)))
_mm256_maskz_conflict_epi64 (__mmask8 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_conflict_epi64(__A),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(128)))
_mm_conflict_epi32 (__m128i __A)
{
  return (__m128i) __builtin_ia32_vpconflictsi_128 ((__v4si) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(128)))
_mm_mask_conflict_epi32 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_conflict_epi32(__A),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(128)))
_mm_maskz_conflict_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_conflict_epi32(__A),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(256)))
_mm256_conflict_epi32 (__m256i __A)
{
  return (__m256i) __builtin_ia32_vpconflictsi_256 ((__v8si) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(256)))
_mm256_mask_conflict_epi32 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_conflict_epi32(__A),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(256)))
_mm256_maskz_conflict_epi32 (__mmask8 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_conflict_epi32(__A),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(128)))
_mm_lzcnt_epi32 (__m128i __A)
{
  return (__m128i) __builtin_ia32_vplzcntd_128 ((__v4si) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(128)))
_mm_mask_lzcnt_epi32 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_lzcnt_epi32(__A),
                                             (__v4si)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(128)))
_mm_maskz_lzcnt_epi32 (__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__U,
                                             (__v4si)_mm_lzcnt_epi32(__A),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(256)))
_mm256_lzcnt_epi32 (__m256i __A)
{
  return (__m256i) __builtin_ia32_vplzcntd_256 ((__v8si) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(256)))
_mm256_mask_lzcnt_epi32 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_lzcnt_epi32(__A),
                                             (__v8si)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(256)))
_mm256_maskz_lzcnt_epi32 (__mmask8 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__U,
                                             (__v8si)_mm256_lzcnt_epi32(__A),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(128)))
_mm_lzcnt_epi64 (__m128i __A)
{
  return (__m128i) __builtin_ia32_vplzcntq_128 ((__v2di) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(128)))
_mm_mask_lzcnt_epi64 (__m128i __W, __mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_lzcnt_epi64(__A),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(128)))
_mm_maskz_lzcnt_epi64 (__mmask8 __U, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_lzcnt_epi64(__A),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(256)))
_mm256_lzcnt_epi64 (__m256i __A)
{
  return (__m256i) __builtin_ia32_vplzcntq_256 ((__v4di) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(256)))
_mm256_mask_lzcnt_epi64 (__m256i __W, __mmask8 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_lzcnt_epi64(__A),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512cd"), __min_vector_width__(256)))
_mm256_maskz_lzcnt_epi64 (__mmask8 __U, __m256i __A)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_lzcnt_epi64(__A),
                                             (__v4di)_mm256_setzero_si256());
}
# 214 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3





# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vldqintrin.h" 1 3
# 36 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vldqintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mullo_epi64 (__m256i __A, __m256i __B) {
  return (__m256i) ((__v4du) __A * (__v4du) __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_mullo_epi64(__m256i __W, __mmask8 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_mullo_epi64(__A, __B),
                                             (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_mullo_epi64(__mmask8 __U, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__U,
                                             (__v4di)_mm256_mullo_epi64(__A, __B),
                                             (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_mullo_epi64 (__m128i __A, __m128i __B) {
  return (__m128i) ((__v2du) __A * (__v2du) __B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_mask_mullo_epi64(__m128i __W, __mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_mullo_epi64(__A, __B),
                                             (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_maskz_mullo_epi64(__mmask8 __U, __m128i __A, __m128i __B) {
  return (__m128i)__builtin_ia32_selectq_128((__mmask8)__U,
                                             (__v2di)_mm_mullo_epi64(__A, __B),
                                             (__v2di)_mm_setzero_si128());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_andnot_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_andnot_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_andnot_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_andnot_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_mask_andnot_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_andnot_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_maskz_andnot_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_andnot_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_andnot_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_andnot_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_andnot_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_andnot_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_mask_andnot_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_andnot_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_maskz_andnot_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_andnot_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_and_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_and_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_and_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_and_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_mask_and_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_and_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_maskz_and_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_and_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_and_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_and_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_and_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_and_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_mask_and_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_and_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_maskz_and_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_and_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_xor_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_xor_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_xor_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_xor_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_mask_xor_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_xor_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_maskz_xor_pd (__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_xor_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_xor_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_xor_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_xor_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_xor_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_mask_xor_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_xor_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_maskz_xor_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_xor_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_or_pd(__m256d __W, __mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_or_pd(__A, __B),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_or_pd(__mmask8 __U, __m256d __A, __m256d __B) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_or_pd(__A, __B),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_mask_or_pd(__m128d __W, __mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_or_pd(__A, __B),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_maskz_or_pd(__mmask8 __U, __m128d __A, __m128d __B) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_or_pd(__A, __B),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_or_ps(__m256 __W, __mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_or_ps(__A, __B),
                                             (__v8sf)__W);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_or_ps(__mmask8 __U, __m256 __A, __m256 __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                             (__v8sf)_mm256_or_ps(__A, __B),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_mask_or_ps(__m128 __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_or_ps(__A, __B),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_maskz_or_ps(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm_or_ps(__A, __B),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_cvtpd_epi64 (__m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2qq128_mask ((__v2df) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_mask_cvtpd_epi64 (__m128i __W, __mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2qq128_mask ((__v2df) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_maskz_cvtpd_epi64 (__mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2qq128_mask ((__v2df) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_cvtpd_epi64 (__m256d __A) {
  return (__m256i) __builtin_ia32_cvtpd2qq256_mask ((__v4df) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_cvtpd_epi64 (__m256i __W, __mmask8 __U, __m256d __A) {
  return (__m256i) __builtin_ia32_cvtpd2qq256_mask ((__v4df) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_cvtpd_epi64 (__mmask8 __U, __m256d __A) {
  return (__m256i) __builtin_ia32_cvtpd2qq256_mask ((__v4df) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_cvtpd_epu64 (__m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2uqq128_mask ((__v2df) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_mask_cvtpd_epu64 (__m128i __W, __mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2uqq128_mask ((__v2df) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_maskz_cvtpd_epu64 (__mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvtpd2uqq128_mask ((__v2df) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_cvtpd_epu64 (__m256d __A) {
  return (__m256i) __builtin_ia32_cvtpd2uqq256_mask ((__v4df) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_cvtpd_epu64 (__m256i __W, __mmask8 __U, __m256d __A) {
  return (__m256i) __builtin_ia32_cvtpd2uqq256_mask ((__v4df) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_cvtpd_epu64 (__mmask8 __U, __m256d __A) {
  return (__m256i) __builtin_ia32_cvtpd2uqq256_mask ((__v4df) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_cvtps_epi64 (__m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2qq128_mask ((__v4sf) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_mask_cvtps_epi64 (__m128i __W, __mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2qq128_mask ((__v4sf) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_maskz_cvtps_epi64 (__mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2qq128_mask ((__v4sf) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_cvtps_epi64 (__m128 __A) {
  return (__m256i) __builtin_ia32_cvtps2qq256_mask ((__v4sf) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_cvtps_epi64 (__m256i __W, __mmask8 __U, __m128 __A) {
  return (__m256i) __builtin_ia32_cvtps2qq256_mask ((__v4sf) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_cvtps_epi64 (__mmask8 __U, __m128 __A) {
  return (__m256i) __builtin_ia32_cvtps2qq256_mask ((__v4sf) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_cvtps_epu64 (__m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2uqq128_mask ((__v4sf) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_mask_cvtps_epu64 (__m128i __W, __mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2uqq128_mask ((__v4sf) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_maskz_cvtps_epu64 (__mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvtps2uqq128_mask ((__v4sf) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_cvtps_epu64 (__m128 __A) {
  return (__m256i) __builtin_ia32_cvtps2uqq256_mask ((__v4sf) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_cvtps_epu64 (__m256i __W, __mmask8 __U, __m128 __A) {
  return (__m256i) __builtin_ia32_cvtps2uqq256_mask ((__v4sf) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_cvtps_epu64 (__mmask8 __U, __m128 __A) {
  return (__m256i) __builtin_ia32_cvtps2uqq256_mask ((__v4sf) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_cvtepi64_pd (__m128i __A) {
  return (__m128d)__builtin_convertvector((__v2di)__A, __v2df);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_mask_cvtepi64_pd (__m128d __W, __mmask8 __U, __m128i __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_cvtepi64_pd(__A),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_maskz_cvtepi64_pd (__mmask8 __U, __m128i __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_cvtepi64_pd(__A),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_cvtepi64_pd (__m256i __A) {
  return (__m256d)__builtin_convertvector((__v4di)__A, __v4df);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_cvtepi64_pd (__m256d __W, __mmask8 __U, __m256i __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_cvtepi64_pd(__A),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_cvtepi64_pd (__mmask8 __U, __m256i __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_cvtepi64_pd(__A),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_cvtepi64_ps (__m128i __A) {
  return (__m128) __builtin_ia32_cvtqq2ps128_mask ((__v2di) __A,
                (__v4sf) _mm_setzero_ps(),
                (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_mask_cvtepi64_ps (__m128 __W, __mmask8 __U, __m128i __A) {
  return (__m128) __builtin_ia32_cvtqq2ps128_mask ((__v2di) __A,
                (__v4sf) __W,
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_maskz_cvtepi64_ps (__mmask8 __U, __m128i __A) {
  return (__m128) __builtin_ia32_cvtqq2ps128_mask ((__v2di) __A,
                (__v4sf) _mm_setzero_ps(),
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_cvtepi64_ps (__m256i __A) {
  return (__m128)__builtin_convertvector((__v4di)__A, __v4sf);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_cvtepi64_ps (__m128 __W, __mmask8 __U, __m256i __A) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm256_cvtepi64_ps(__A),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_cvtepi64_ps (__mmask8 __U, __m256i __A) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm256_cvtepi64_ps(__A),
                                             (__v4sf)_mm_setzero_ps());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_cvttpd_epi64 (__m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2qq128_mask ((__v2df) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_mask_cvttpd_epi64 (__m128i __W, __mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2qq128_mask ((__v2df) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_maskz_cvttpd_epi64 (__mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2qq128_mask ((__v2df) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_cvttpd_epi64 (__m256d __A) {
  return (__m256i) __builtin_ia32_cvttpd2qq256_mask ((__v4df) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_cvttpd_epi64 (__m256i __W, __mmask8 __U, __m256d __A) {
  return (__m256i) __builtin_ia32_cvttpd2qq256_mask ((__v4df) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_cvttpd_epi64 (__mmask8 __U, __m256d __A) {
  return (__m256i) __builtin_ia32_cvttpd2qq256_mask ((__v4df) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_cvttpd_epu64 (__m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2uqq128_mask ((__v2df) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_mask_cvttpd_epu64 (__m128i __W, __mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2uqq128_mask ((__v2df) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_maskz_cvttpd_epu64 (__mmask8 __U, __m128d __A) {
  return (__m128i) __builtin_ia32_cvttpd2uqq128_mask ((__v2df) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_cvttpd_epu64 (__m256d __A) {
  return (__m256i) __builtin_ia32_cvttpd2uqq256_mask ((__v4df) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_cvttpd_epu64 (__m256i __W, __mmask8 __U, __m256d __A) {
  return (__m256i) __builtin_ia32_cvttpd2uqq256_mask ((__v4df) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_cvttpd_epu64 (__mmask8 __U, __m256d __A) {
  return (__m256i) __builtin_ia32_cvttpd2uqq256_mask ((__v4df) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_cvttps_epi64 (__m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2qq128_mask ((__v4sf) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_mask_cvttps_epi64 (__m128i __W, __mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2qq128_mask ((__v4sf) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_maskz_cvttps_epi64 (__mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2qq128_mask ((__v4sf) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_cvttps_epi64 (__m128 __A) {
  return (__m256i) __builtin_ia32_cvttps2qq256_mask ((__v4sf) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_cvttps_epi64 (__m256i __W, __mmask8 __U, __m128 __A) {
  return (__m256i) __builtin_ia32_cvttps2qq256_mask ((__v4sf) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_cvttps_epi64 (__mmask8 __U, __m128 __A) {
  return (__m256i) __builtin_ia32_cvttps2qq256_mask ((__v4sf) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_cvttps_epu64 (__m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2uqq128_mask ((__v4sf) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) -1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_mask_cvttps_epu64 (__m128i __W, __mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2uqq128_mask ((__v4sf) __A,
                (__v2di) __W,
                (__mmask8) __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_maskz_cvttps_epu64 (__mmask8 __U, __m128 __A) {
  return (__m128i) __builtin_ia32_cvttps2uqq128_mask ((__v4sf) __A,
                (__v2di) _mm_setzero_si128(),
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_cvttps_epu64 (__m128 __A) {
  return (__m256i) __builtin_ia32_cvttps2uqq256_mask ((__v4sf) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) -1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_cvttps_epu64 (__m256i __W, __mmask8 __U, __m128 __A) {
  return (__m256i) __builtin_ia32_cvttps2uqq256_mask ((__v4sf) __A,
                (__v4di) __W,
                (__mmask8) __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_cvttps_epu64 (__mmask8 __U, __m128 __A) {
  return (__m256i) __builtin_ia32_cvttps2uqq256_mask ((__v4sf) __A,
                (__v4di) _mm256_setzero_si256(),
                (__mmask8) __U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_cvtepu64_pd (__m128i __A) {
  return (__m128d)__builtin_convertvector((__v2du)__A, __v2df);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_mask_cvtepu64_pd (__m128d __W, __mmask8 __U, __m128i __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_cvtepu64_pd(__A),
                                              (__v2df)__W);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_maskz_cvtepu64_pd (__mmask8 __U, __m128i __A) {
  return (__m128d)__builtin_ia32_selectpd_128((__mmask8)__U,
                                              (__v2df)_mm_cvtepu64_pd(__A),
                                              (__v2df)_mm_setzero_pd());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_cvtepu64_pd (__m256i __A) {
  return (__m256d)__builtin_convertvector((__v4du)__A, __v4df);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_cvtepu64_pd (__m256d __W, __mmask8 __U, __m256i __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_cvtepu64_pd(__A),
                                              (__v4df)__W);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_cvtepu64_pd (__mmask8 __U, __m256i __A) {
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__U,
                                              (__v4df)_mm256_cvtepu64_pd(__A),
                                              (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_cvtepu64_ps (__m128i __A) {
  return (__m128) __builtin_ia32_cvtuqq2ps128_mask ((__v2di) __A,
                (__v4sf) _mm_setzero_ps(),
                (__mmask8) -1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_mask_cvtepu64_ps (__m128 __W, __mmask8 __U, __m128i __A) {
  return (__m128) __builtin_ia32_cvtuqq2ps128_mask ((__v2di) __A,
                (__v4sf) __W,
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_maskz_cvtepu64_ps (__mmask8 __U, __m128i __A) {
  return (__m128) __builtin_ia32_cvtuqq2ps128_mask ((__v2di) __A,
                (__v4sf) _mm_setzero_ps(),
                (__mmask8) __U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_cvtepu64_ps (__m256i __A) {
  return (__m128)__builtin_convertvector((__v4du)__A, __v4sf);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_cvtepu64_ps (__m128 __W, __mmask8 __U, __m256i __A) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm256_cvtepu64_ps(__A),
                                             (__v4sf)__W);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_cvtepu64_ps (__mmask8 __U, __m256i __A) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                             (__v4sf)_mm256_cvtepu64_ps(__A),
                                             (__v4sf)_mm_setzero_ps());
}
# 920 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vldqintrin.h" 3
static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_movepi32_mask (__m128i __A)
{
  return (__mmask8) __builtin_ia32_cvtd2mask128 ((__v4si) __A);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_movepi32_mask (__m256i __A)
{
  return (__mmask8) __builtin_ia32_cvtd2mask256 ((__v8si) __A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_movm_epi32 (__mmask8 __A)
{
  return (__m128i) __builtin_ia32_cvtmask2d128 (__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_movm_epi32 (__mmask8 __A)
{
  return (__m256i) __builtin_ia32_cvtmask2d256 (__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_movm_epi64 (__mmask8 __A)
{
  return (__m128i) __builtin_ia32_cvtmask2q128 (__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_movm_epi64 (__mmask8 __A)
{
  return (__m256i) __builtin_ia32_cvtmask2q256 (__A);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_movepi64_mask (__m128i __A)
{
  return (__mmask8) __builtin_ia32_cvtq2mask128 ((__v2di) __A);
}

static __inline__ __mmask8 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_movepi64_mask (__m256i __A)
{
  return (__mmask8) __builtin_ia32_cvtq2mask256 ((__v4di) __A);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_broadcast_f32x2 (__m128 __A)
{
  return (__m256)__builtin_shufflevector((__v4sf)__A, (__v4sf)__A,
                                         0, 1, 0, 1, 0, 1, 0, 1);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_broadcast_f32x2 (__m256 __O, __mmask8 __M, __m128 __A)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__M,
                                             (__v8sf)_mm256_broadcast_f32x2(__A),
                                             (__v8sf)__O);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_broadcast_f32x2 (__mmask8 __M, __m128 __A)
{
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__M,
                                             (__v8sf)_mm256_broadcast_f32x2(__A),
                                             (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_broadcast_f64x2(__m128d __A)
{
  return (__m256d)__builtin_shufflevector((__v2df)__A, (__v2df)__A,
                                          0, 1, 0, 1);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_broadcast_f64x2(__m256d __O, __mmask8 __M, __m128d __A)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__M,
                                            (__v4df)_mm256_broadcast_f64x2(__A),
                                            (__v4df)__O);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_broadcast_f64x2 (__mmask8 __M, __m128d __A)
{
  return (__m256d)__builtin_ia32_selectpd_256((__mmask8)__M,
                                            (__v4df)_mm256_broadcast_f64x2(__A),
                                            (__v4df)_mm256_setzero_pd());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_broadcast_i32x2 (__m128i __A)
{
  return (__m128i)__builtin_shufflevector((__v4si)__A, (__v4si)__A,
                                          0, 1, 0, 1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_mask_broadcast_i32x2 (__m128i __O, __mmask8 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_broadcast_i32x2(__A),
                                             (__v4si)__O);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(128)))
_mm_maskz_broadcast_i32x2 (__mmask8 __M, __m128i __A)
{
  return (__m128i)__builtin_ia32_selectd_128((__mmask8)__M,
                                             (__v4si)_mm_broadcast_i32x2(__A),
                                             (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_broadcast_i32x2 (__m128i __A)
{
  return (__m256i)__builtin_shufflevector((__v4si)__A, (__v4si)__A,
                                          0, 1, 0, 1, 0, 1, 0, 1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_broadcast_i32x2 (__m256i __O, __mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_broadcast_i32x2(__A),
                                             (__v8si)__O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_broadcast_i32x2 (__mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectd_256((__mmask8)__M,
                                             (__v8si)_mm256_broadcast_i32x2(__A),
                                             (__v8si)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_broadcast_i64x2(__m128i __A)
{
  return (__m256i)__builtin_shufflevector((__v2di)__A, (__v2di)__A,
                                          0, 1, 0, 1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_mask_broadcast_i64x2(__m256i __O, __mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                            (__v4di)_mm256_broadcast_i64x2(__A),
                                            (__v4di)__O);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512dq"), __min_vector_width__(256)))
_mm256_maskz_broadcast_i64x2 (__mmask8 __M, __m128i __A)
{
  return (__m256i)__builtin_ia32_selectq_256((__mmask8)__M,
                                            (__v4di)_mm256_broadcast_i64x2(__A),
                                            (__v4di)_mm256_setzero_si256());
}
# 220 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512erintrin.h" 1 3
# 225 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512ifmaintrin.h" 1 3
# 20 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512ifmaintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma"), __min_vector_width__(512)))
_mm512_madd52hi_epu64 (__m512i __X, __m512i __Y, __m512i __Z)
{
  return (__m512i)__builtin_ia32_vpmadd52huq512((__v8di) __X, (__v8di) __Y,
                                                (__v8di) __Z);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma"), __min_vector_width__(512)))
_mm512_mask_madd52hi_epu64 (__m512i __W, __mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512(__M,
                                   (__v8di)_mm512_madd52hi_epu64(__W, __X, __Y),
                                   (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma"), __min_vector_width__(512)))
_mm512_maskz_madd52hi_epu64 (__mmask8 __M, __m512i __X, __m512i __Y, __m512i __Z)
{
  return (__m512i)__builtin_ia32_selectq_512(__M,
                                   (__v8di)_mm512_madd52hi_epu64(__X, __Y, __Z),
                                   (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma"), __min_vector_width__(512)))
_mm512_madd52lo_epu64 (__m512i __X, __m512i __Y, __m512i __Z)
{
  return (__m512i)__builtin_ia32_vpmadd52luq512((__v8di) __X, (__v8di) __Y,
                                                (__v8di) __Z);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma"), __min_vector_width__(512)))
_mm512_mask_madd52lo_epu64 (__m512i __W, __mmask8 __M, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectq_512(__M,
                                   (__v8di)_mm512_madd52lo_epu64(__W, __X, __Y),
                                   (__v8di)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma"), __min_vector_width__(512)))
_mm512_maskz_madd52lo_epu64 (__mmask8 __M, __m512i __X, __m512i __Y, __m512i __Z)
{
  return (__m512i)__builtin_ia32_selectq_512(__M,
                                   (__v8di)_mm512_madd52lo_epu64(__X, __Y, __Z),
                                   (__v8di)_mm512_setzero_si512());
}
# 230 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3





# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512ifmavlintrin.h" 1 3
# 52 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512ifmavlintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl"), __min_vector_width__(128)))
_mm_mask_madd52hi_epu64 (__m128i __W, __mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128(__M,
                                      (__v2di)((__m128i)__builtin_ia32_vpmadd52huq128((__v2di)(__W), (__v2di)(__X), (__v2di)(__Y))),
                                      (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl"), __min_vector_width__(128)))
_mm_maskz_madd52hi_epu64 (__mmask8 __M, __m128i __X, __m128i __Y, __m128i __Z)
{
  return (__m128i)__builtin_ia32_selectq_128(__M,
                                      (__v2di)((__m128i)__builtin_ia32_vpmadd52huq128((__v2di)(__X), (__v2di)(__Y), (__v2di)(__Z))),
                                      (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl"), __min_vector_width__(256)))
_mm256_mask_madd52hi_epu64 (__m256i __W, __mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256(__M,
                                   (__v4di)((__m256i)__builtin_ia32_vpmadd52huq256((__v4di)(__W), (__v4di)(__X), (__v4di)(__Y))),
                                   (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl"), __min_vector_width__(256)))
_mm256_maskz_madd52hi_epu64 (__mmask8 __M, __m256i __X, __m256i __Y, __m256i __Z)
{
  return (__m256i)__builtin_ia32_selectq_256(__M,
                                   (__v4di)((__m256i)__builtin_ia32_vpmadd52huq256((__v4di)(__X), (__v4di)(__Y), (__v4di)(__Z))),
                                   (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl"), __min_vector_width__(128)))
_mm_mask_madd52lo_epu64 (__m128i __W, __mmask8 __M, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectq_128(__M,
                                      (__v2di)((__m128i)__builtin_ia32_vpmadd52luq128((__v2di)(__W), (__v2di)(__X), (__v2di)(__Y))),
                                      (__v2di)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl"), __min_vector_width__(128)))
_mm_maskz_madd52lo_epu64 (__mmask8 __M, __m128i __X, __m128i __Y, __m128i __Z)
{
  return (__m128i)__builtin_ia32_selectq_128(__M,
                                      (__v2di)((__m128i)__builtin_ia32_vpmadd52luq128((__v2di)(__X), (__v2di)(__Y), (__v2di)(__Z))),
                                      (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl"), __min_vector_width__(256)))
_mm256_mask_madd52lo_epu64 (__m256i __W, __mmask8 __M, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectq_256(__M,
                                   (__v4di)((__m256i)__builtin_ia32_vpmadd52luq256((__v4di)(__W), (__v4di)(__X), (__v4di)(__Y))),
                                   (__v4di)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512ifma,avx512vl"), __min_vector_width__(256)))
_mm256_maskz_madd52lo_epu64 (__mmask8 __M, __m256i __X, __m256i __Y, __m256i __Z)
{
  return (__m256i)__builtin_ia32_selectq_256(__M,
                                   (__v4di)((__m256i)__builtin_ia32_vpmadd52luq256((__v4di)(__X), (__v4di)(__Y), (__v4di)(__Z))),
                                   (__v4di)_mm256_setzero_si256());
}
# 236 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vbmiintrin.h" 1 3
# 21 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vbmiintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi"), __min_vector_width__(512)))
_mm512_permutex2var_epi8(__m512i __A, __m512i __I, __m512i __B)
{
  return (__m512i)__builtin_ia32_vpermi2varqi512((__v64qi)__A, (__v64qi)__I,
                                                 (__v64qi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi"), __min_vector_width__(512)))
_mm512_mask_permutex2var_epi8(__m512i __A, __mmask64 __U, __m512i __I,
                              __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512(__U,
                               (__v64qi)_mm512_permutex2var_epi8(__A, __I, __B),
                               (__v64qi)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi"), __min_vector_width__(512)))
_mm512_mask2_permutex2var_epi8(__m512i __A, __m512i __I, __mmask64 __U,
                               __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512(__U,
                               (__v64qi)_mm512_permutex2var_epi8(__A, __I, __B),
                               (__v64qi)__I);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi"), __min_vector_width__(512)))
_mm512_maskz_permutex2var_epi8(__mmask64 __U, __m512i __A, __m512i __I,
                               __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512(__U,
                               (__v64qi)_mm512_permutex2var_epi8(__A, __I, __B),
                               (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi"), __min_vector_width__(512)))
_mm512_permutexvar_epi8 (__m512i __A, __m512i __B)
{
  return (__m512i)__builtin_ia32_permvarqi512((__v64qi) __B, (__v64qi) __A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi"), __min_vector_width__(512)))
_mm512_maskz_permutexvar_epi8 (__mmask64 __M, __m512i __A,
        __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                     (__v64qi)_mm512_permutexvar_epi8(__A, __B),
                                     (__v64qi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi"), __min_vector_width__(512)))
_mm512_mask_permutexvar_epi8 (__m512i __W, __mmask64 __M, __m512i __A,
             __m512i __B)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                     (__v64qi)_mm512_permutexvar_epi8(__A, __B),
                                     (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi"), __min_vector_width__(512)))
_mm512_multishift_epi64_epi8(__m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_vpmultishiftqb512((__v64qi)__X, (__v64qi) __Y);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi"), __min_vector_width__(512)))
_mm512_mask_multishift_epi64_epi8(__m512i __W, __mmask64 __M, __m512i __X,
                                  __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                (__v64qi)_mm512_multishift_epi64_epi8(__X, __Y),
                                (__v64qi)__W);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi"), __min_vector_width__(512)))
_mm512_maskz_multishift_epi64_epi8(__mmask64 __M, __m512i __X, __m512i __Y)
{
  return (__m512i)__builtin_ia32_selectb_512((__mmask64)__M,
                                (__v64qi)_mm512_multishift_epi64_epi8(__X, __Y),
                                (__v64qi)_mm512_setzero_si512());
}
# 241 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxifmaintrin.h" 1 3
# 58 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxifmaintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxifma"), __min_vector_width__(128)))
_mm_madd52hi_avx_epu64(__m128i __X, __m128i __Y, __m128i __Z) {
  return (__m128i)__builtin_ia32_vpmadd52huq128((__v2di)__X, (__v2di)__Y,
                                                (__v2di)__Z);
}
# 95 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxifmaintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxifma"), __min_vector_width__(256)))
_mm256_madd52hi_avx_epu64(__m256i __X, __m256i __Y, __m256i __Z) {
  return (__m256i)__builtin_ia32_vpmadd52huq256((__v4di)__X, (__v4di)__Y,
                                                (__v4di)__Z);
}
# 132 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxifmaintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxifma"), __min_vector_width__(128)))
_mm_madd52lo_avx_epu64(__m128i __X, __m128i __Y, __m128i __Z) {
  return (__m128i)__builtin_ia32_vpmadd52luq128((__v2di)__X, (__v2di)__Y,
                                                (__v2di)__Z);
}
# 169 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxifmaintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxifma"), __min_vector_width__(256)))
_mm256_madd52lo_avx_epu64(__m256i __X, __m256i __Y, __m256i __Z) {
  return (__m256i)__builtin_ia32_vpmadd52luq256((__v4di)__X, (__v4di)__Y,
                                                (__v4di)__Z);
}
# 246 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxneconvertintrin.h" 1 3
# 56 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxneconvertintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(128)))
_mm_bcstnebf16_ps(const void *__A) {
  return (__m128)__builtin_ia32_vbcstnebf162ps128((const __bf16 *)__A);
}
# 89 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxneconvertintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(256)))
_mm256_bcstnebf16_ps(const void *__A) {
  return (__m256)__builtin_ia32_vbcstnebf162ps256((const __bf16 *)__A);
}
# 122 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxneconvertintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(128)))
_mm_bcstnesh_ps(const void *__A) {
  return (__m128)__builtin_ia32_vbcstnesh2ps128((const _Float16 *)__A);
}
# 155 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxneconvertintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(256)))
_mm256_bcstnesh_ps(const void *__A) {
  return (__m256)__builtin_ia32_vbcstnesh2ps256((const _Float16 *)__A);
}
# 188 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxneconvertintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(128)))
_mm_cvtneebf16_ps(const __m128bh *__A) {
  return (__m128)__builtin_ia32_vcvtneebf162ps128((const __v8bf *)__A);
}
# 221 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxneconvertintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(256)))
_mm256_cvtneebf16_ps(const __m256bh *__A) {
  return (__m256)__builtin_ia32_vcvtneebf162ps256((const __v16bf *)__A);
}
# 254 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxneconvertintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(128)))
_mm_cvtneeph_ps(const __m128h *__A) {
  return (__m128)__builtin_ia32_vcvtneeph2ps128((const __v8hf *)__A);
}
# 287 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxneconvertintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(256)))
_mm256_cvtneeph_ps(const __m256h *__A) {
  return (__m256)__builtin_ia32_vcvtneeph2ps256((const __v16hf *)__A);
}
# 320 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxneconvertintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(128)))
_mm_cvtneobf16_ps(const __m128bh *__A) {
  return (__m128)__builtin_ia32_vcvtneobf162ps128((const __v8bf *)__A);
}
# 353 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxneconvertintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(256)))
_mm256_cvtneobf16_ps(const __m256bh *__A) {
  return (__m256)__builtin_ia32_vcvtneobf162ps256((const __v16bf *)__A);
}
# 386 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxneconvertintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(128)))
_mm_cvtneoph_ps(const __m128h *__A) {
  return (__m128)__builtin_ia32_vcvtneoph2ps128((const __v8hf *)__A);
}
# 419 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxneconvertintrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(256)))
_mm256_cvtneoph_ps(const __m256h *__A) {
  return (__m256)__builtin_ia32_vcvtneoph2ps256((const __v16hf *)__A);
}
# 447 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxneconvertintrin.h" 3
static __inline__ __m128bh __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(128)))
_mm_cvtneps_avx_pbh(__m128 __A) {
  return (__m128bh)__builtin_ia32_vcvtneps2bf16128((__v4sf)__A);
}
# 475 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxneconvertintrin.h" 3
static __inline__ __m128bh __attribute__((__always_inline__, __nodebug__, __target__("avxneconvert"), __min_vector_width__(256)))
_mm256_cvtneps_avx_pbh(__m256 __A) {
  return (__m128bh)__builtin_ia32_vcvtneps2bf16256((__v8sf)__A);
}
# 251 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3





# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vbmivlintrin.h" 1 3
# 36 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vbmivlintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl"), __min_vector_width__(128)))
_mm_permutex2var_epi8(__m128i __A, __m128i __I, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpermi2varqi128((__v16qi)__A,
                                                 (__v16qi)__I,
                                                 (__v16qi)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl"), __min_vector_width__(128)))
_mm_mask_permutex2var_epi8(__m128i __A, __mmask16 __U, __m128i __I,
                           __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128(__U,
                                  (__v16qi)_mm_permutex2var_epi8(__A, __I, __B),
                                  (__v16qi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl"), __min_vector_width__(128)))
_mm_mask2_permutex2var_epi8(__m128i __A, __m128i __I, __mmask16 __U,
                            __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128(__U,
                                  (__v16qi)_mm_permutex2var_epi8(__A, __I, __B),
                                  (__v16qi)__I);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl"), __min_vector_width__(128)))
_mm_maskz_permutex2var_epi8(__mmask16 __U, __m128i __A, __m128i __I,
                            __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128(__U,
                                  (__v16qi)_mm_permutex2var_epi8(__A, __I, __B),
                                  (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl"), __min_vector_width__(256)))
_mm256_permutex2var_epi8(__m256i __A, __m256i __I, __m256i __B)
{
  return (__m256i)__builtin_ia32_vpermi2varqi256((__v32qi)__A, (__v32qi)__I,
                                                 (__v32qi)__B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl"), __min_vector_width__(256)))
_mm256_mask_permutex2var_epi8(__m256i __A, __mmask32 __U, __m256i __I,
                              __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256(__U,
                               (__v32qi)_mm256_permutex2var_epi8(__A, __I, __B),
                               (__v32qi)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl"), __min_vector_width__(256)))
_mm256_mask2_permutex2var_epi8(__m256i __A, __m256i __I, __mmask32 __U,
                               __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256(__U,
                               (__v32qi)_mm256_permutex2var_epi8(__A, __I, __B),
                               (__v32qi)__I);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl"), __min_vector_width__(256)))
_mm256_maskz_permutex2var_epi8(__mmask32 __U, __m256i __A, __m256i __I,
                               __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256(__U,
                               (__v32qi)_mm256_permutex2var_epi8(__A, __I, __B),
                               (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl"), __min_vector_width__(128)))
_mm_permutexvar_epi8 (__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_permvarqi128((__v16qi)__B, (__v16qi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl"), __min_vector_width__(128)))
_mm_maskz_permutexvar_epi8 (__mmask16 __M, __m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                        (__v16qi)_mm_permutexvar_epi8(__A, __B),
                                        (__v16qi)_mm_setzero_si128());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl"), __min_vector_width__(128)))
_mm_mask_permutexvar_epi8 (__m128i __W, __mmask16 __M, __m128i __A,
          __m128i __B)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                        (__v16qi)_mm_permutexvar_epi8(__A, __B),
                                        (__v16qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl"), __min_vector_width__(256)))
_mm256_permutexvar_epi8 (__m256i __A, __m256i __B)
{
  return (__m256i)__builtin_ia32_permvarqi256((__v32qi) __B, (__v32qi) __A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl"), __min_vector_width__(256)))
_mm256_maskz_permutexvar_epi8 (__mmask32 __M, __m256i __A,
        __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                     (__v32qi)_mm256_permutexvar_epi8(__A, __B),
                                     (__v32qi)_mm256_setzero_si256());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl"), __min_vector_width__(256)))
_mm256_mask_permutexvar_epi8 (__m256i __W, __mmask32 __M, __m256i __A,
             __m256i __B)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                     (__v32qi)_mm256_permutexvar_epi8(__A, __B),
                                     (__v32qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl"), __min_vector_width__(128)))
_mm_multishift_epi64_epi8(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_vpmultishiftqb128((__v16qi)__X, (__v16qi)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl"), __min_vector_width__(128)))
_mm_mask_multishift_epi64_epi8(__m128i __W, __mmask16 __M, __m128i __X,
                               __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                   (__v16qi)_mm_multishift_epi64_epi8(__X, __Y),
                                   (__v16qi)__W);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl"), __min_vector_width__(128)))
_mm_maskz_multishift_epi64_epi8(__mmask16 __M, __m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_selectb_128((__mmask16)__M,
                                   (__v16qi)_mm_multishift_epi64_epi8(__X, __Y),
                                   (__v16qi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl"), __min_vector_width__(256)))
_mm256_multishift_epi64_epi8(__m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_vpmultishiftqb256((__v32qi)__X, (__v32qi)__Y);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl"), __min_vector_width__(256)))
_mm256_mask_multishift_epi64_epi8(__m256i __W, __mmask32 __M, __m256i __X,
                                  __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                (__v32qi)_mm256_multishift_epi64_epi8(__X, __Y),
                                (__v32qi)__W);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi,avx512vl"), __min_vector_width__(256)))
_mm256_maskz_multishift_epi64_epi8(__mmask32 __M, __m256i __X, __m256i __Y)
{
  return (__m256i)__builtin_ia32_selectb_256((__mmask32)__M,
                                (__v32qi)_mm256_multishift_epi64_epi8(__X, __Y),
                                (__v32qi)_mm256_setzero_si256());
}
# 257 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vbmi2intrin.h" 1 3
# 21 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vbmi2intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_mask_compress_epi16(__m512i __S, __mmask32 __U, __m512i __D)
{
  return (__m512i) __builtin_ia32_compresshi512_mask ((__v32hi) __D,
              (__v32hi) __S,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_maskz_compress_epi16(__mmask32 __U, __m512i __D)
{
  return (__m512i) __builtin_ia32_compresshi512_mask ((__v32hi) __D,
              (__v32hi) _mm512_setzero_si512(),
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_mask_compress_epi8(__m512i __S, __mmask64 __U, __m512i __D)
{
  return (__m512i) __builtin_ia32_compressqi512_mask ((__v64qi) __D,
              (__v64qi) __S,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_maskz_compress_epi8(__mmask64 __U, __m512i __D)
{
  return (__m512i) __builtin_ia32_compressqi512_mask ((__v64qi) __D,
              (__v64qi) _mm512_setzero_si512(),
              __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_mask_compressstoreu_epi16(void *__P, __mmask32 __U, __m512i __D)
{
  __builtin_ia32_compressstorehi512_mask ((__v32hi *) __P, (__v32hi) __D,
              __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_mask_compressstoreu_epi8(void *__P, __mmask64 __U, __m512i __D)
{
  __builtin_ia32_compressstoreqi512_mask ((__v64qi *) __P, (__v64qi) __D,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_mask_expand_epi16(__m512i __S, __mmask32 __U, __m512i __D)
{
  return (__m512i) __builtin_ia32_expandhi512_mask ((__v32hi) __D,
              (__v32hi) __S,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_maskz_expand_epi16(__mmask32 __U, __m512i __D)
{
  return (__m512i) __builtin_ia32_expandhi512_mask ((__v32hi) __D,
              (__v32hi) _mm512_setzero_si512(),
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_mask_expand_epi8(__m512i __S, __mmask64 __U, __m512i __D)
{
  return (__m512i) __builtin_ia32_expandqi512_mask ((__v64qi) __D,
              (__v64qi) __S,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_maskz_expand_epi8(__mmask64 __U, __m512i __D)
{
  return (__m512i) __builtin_ia32_expandqi512_mask ((__v64qi) __D,
              (__v64qi) _mm512_setzero_si512(),
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_mask_expandloadu_epi16(__m512i __S, __mmask32 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_expandloadhi512_mask ((const __v32hi *)__P,
              (__v32hi) __S,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_maskz_expandloadu_epi16(__mmask32 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_expandloadhi512_mask ((const __v32hi *)__P,
              (__v32hi) _mm512_setzero_si512(),
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_mask_expandloadu_epi8(__m512i __S, __mmask64 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_expandloadqi512_mask ((const __v64qi *)__P,
              (__v64qi) __S,
              __U);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_maskz_expandloadu_epi8(__mmask64 __U, void const *__P)
{
  return (__m512i) __builtin_ia32_expandloadqi512_mask ((const __v64qi *)__P,
              (__v64qi) _mm512_setzero_si512(),
              __U);
}
# 215 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vbmi2intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_shldv_epi64(__m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_vpshldvq512((__v8di)__A, (__v8di)__B,
                                             (__v8di)__C);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_mask_shldv_epi64(__m512i __A, __mmask8 __U, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_selectq_512(__U,
                                      (__v8di)_mm512_shldv_epi64(__A, __B, __C),
                                      (__v8di)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_maskz_shldv_epi64(__mmask8 __U, __m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_selectq_512(__U,
                                      (__v8di)_mm512_shldv_epi64(__A, __B, __C),
                                      (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_shldv_epi32(__m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_vpshldvd512((__v16si)__A, (__v16si)__B,
                                             (__v16si)__C);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_mask_shldv_epi32(__m512i __A, __mmask16 __U, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                     (__v16si)_mm512_shldv_epi32(__A, __B, __C),
                                     (__v16si)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_maskz_shldv_epi32(__mmask16 __U, __m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_selectd_512(__U,
                                     (__v16si)_mm512_shldv_epi32(__A, __B, __C),
                                     (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_shldv_epi16(__m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_vpshldvw512((__v32hi)__A, (__v32hi)__B,
                                             (__v32hi)__C);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_mask_shldv_epi16(__m512i __A, __mmask32 __U, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_selectw_512(__U,
                                     (__v32hi)_mm512_shldv_epi16(__A, __B, __C),
                                     (__v32hi)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_maskz_shldv_epi16(__mmask32 __U, __m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_selectw_512(__U,
                                     (__v32hi)_mm512_shldv_epi16(__A, __B, __C),
                                     (__v32hi)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_shrdv_epi64(__m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_vpshrdvq512((__v8di)__A, (__v8di)__B,
                                             (__v8di)__C);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_mask_shrdv_epi64(__m512i __A, __mmask8 __U, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_selectq_512(__U,
                                      (__v8di)_mm512_shrdv_epi64(__A, __B, __C),
                                      (__v8di)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_maskz_shrdv_epi64(__mmask8 __U, __m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_selectq_512(__U,
                                      (__v8di)_mm512_shrdv_epi64(__A, __B, __C),
                                      (__v8di)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_shrdv_epi32(__m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_vpshrdvd512((__v16si)__A, (__v16si)__B,
                                             (__v16si)__C);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_mask_shrdv_epi32(__m512i __A, __mmask16 __U, __m512i __B, __m512i __C)
{
  return (__m512i) __builtin_ia32_selectd_512(__U,
                                     (__v16si)_mm512_shrdv_epi32(__A, __B, __C),
                                     (__v16si)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_maskz_shrdv_epi32(__mmask16 __U, __m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i) __builtin_ia32_selectd_512(__U,
                                     (__v16si)_mm512_shrdv_epi32(__A, __B, __C),
                                     (__v16si)_mm512_setzero_si512());
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_shrdv_epi16(__m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_vpshrdvw512((__v32hi)__A, (__v32hi)__B,
                                             (__v32hi)__C);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_mask_shrdv_epi16(__m512i __A, __mmask32 __U, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_selectw_512(__U,
                                     (__v32hi)_mm512_shrdv_epi16(__A, __B, __C),
                                     (__v32hi)__A);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512vbmi2"), __min_vector_width__(512)))
_mm512_maskz_shrdv_epi16(__mmask32 __U, __m512i __A, __m512i __B, __m512i __C)
{
  return (__m512i)__builtin_ia32_selectw_512(__U,
                                     (__v32hi)_mm512_shrdv_epi16(__A, __B, __C),
                                     (__v32hi)_mm512_setzero_si512());
}
# 262 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3





# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlvbmi2intrin.h" 1 3
# 36 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlvbmi2intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_mask_compress_epi16(__m128i __S, __mmask8 __U, __m128i __D)
{
  return (__m128i) __builtin_ia32_compresshi128_mask ((__v8hi) __D,
              (__v8hi) __S,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_maskz_compress_epi16(__mmask8 __U, __m128i __D)
{
  return (__m128i) __builtin_ia32_compresshi128_mask ((__v8hi) __D,
              (__v8hi) _mm_setzero_si128(),
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_mask_compress_epi8(__m128i __S, __mmask16 __U, __m128i __D)
{
  return (__m128i) __builtin_ia32_compressqi128_mask ((__v16qi) __D,
              (__v16qi) __S,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_maskz_compress_epi8(__mmask16 __U, __m128i __D)
{
  return (__m128i) __builtin_ia32_compressqi128_mask ((__v16qi) __D,
              (__v16qi) _mm_setzero_si128(),
              __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_mask_compressstoreu_epi16(void *__P, __mmask8 __U, __m128i __D)
{
  __builtin_ia32_compressstorehi128_mask ((__v8hi *) __P, (__v8hi) __D,
              __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_mask_compressstoreu_epi8(void *__P, __mmask16 __U, __m128i __D)
{
  __builtin_ia32_compressstoreqi128_mask ((__v16qi *) __P, (__v16qi) __D,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_mask_expand_epi16(__m128i __S, __mmask8 __U, __m128i __D)
{
  return (__m128i) __builtin_ia32_expandhi128_mask ((__v8hi) __D,
              (__v8hi) __S,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_maskz_expand_epi16(__mmask8 __U, __m128i __D)
{
  return (__m128i) __builtin_ia32_expandhi128_mask ((__v8hi) __D,
              (__v8hi) _mm_setzero_si128(),
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_mask_expand_epi8(__m128i __S, __mmask16 __U, __m128i __D)
{
  return (__m128i) __builtin_ia32_expandqi128_mask ((__v16qi) __D,
              (__v16qi) __S,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_maskz_expand_epi8(__mmask16 __U, __m128i __D)
{
  return (__m128i) __builtin_ia32_expandqi128_mask ((__v16qi) __D,
              (__v16qi) _mm_setzero_si128(),
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_mask_expandloadu_epi16(__m128i __S, __mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_expandloadhi128_mask ((const __v8hi *)__P,
              (__v8hi) __S,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_maskz_expandloadu_epi16(__mmask8 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_expandloadhi128_mask ((const __v8hi *)__P,
              (__v8hi) _mm_setzero_si128(),
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_mask_expandloadu_epi8(__m128i __S, __mmask16 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_expandloadqi128_mask ((const __v16qi *)__P,
              (__v16qi) __S,
              __U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_maskz_expandloadu_epi8(__mmask16 __U, void const *__P)
{
  return (__m128i) __builtin_ia32_expandloadqi128_mask ((const __v16qi *)__P,
              (__v16qi) _mm_setzero_si128(),
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_mask_compress_epi16(__m256i __S, __mmask16 __U, __m256i __D)
{
  return (__m256i) __builtin_ia32_compresshi256_mask ((__v16hi) __D,
              (__v16hi) __S,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_maskz_compress_epi16(__mmask16 __U, __m256i __D)
{
  return (__m256i) __builtin_ia32_compresshi256_mask ((__v16hi) __D,
              (__v16hi) _mm256_setzero_si256(),
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_mask_compress_epi8(__m256i __S, __mmask32 __U, __m256i __D)
{
  return (__m256i) __builtin_ia32_compressqi256_mask ((__v32qi) __D,
              (__v32qi) __S,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_maskz_compress_epi8(__mmask32 __U, __m256i __D)
{
  return (__m256i) __builtin_ia32_compressqi256_mask ((__v32qi) __D,
              (__v32qi) _mm256_setzero_si256(),
              __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_mask_compressstoreu_epi16(void *__P, __mmask16 __U, __m256i __D)
{
  __builtin_ia32_compressstorehi256_mask ((__v16hi *) __P, (__v16hi) __D,
              __U);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_mask_compressstoreu_epi8(void *__P, __mmask32 __U, __m256i __D)
{
  __builtin_ia32_compressstoreqi256_mask ((__v32qi *) __P, (__v32qi) __D,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_mask_expand_epi16(__m256i __S, __mmask16 __U, __m256i __D)
{
  return (__m256i) __builtin_ia32_expandhi256_mask ((__v16hi) __D,
              (__v16hi) __S,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_maskz_expand_epi16(__mmask16 __U, __m256i __D)
{
  return (__m256i) __builtin_ia32_expandhi256_mask ((__v16hi) __D,
              (__v16hi) _mm256_setzero_si256(),
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_mask_expand_epi8(__m256i __S, __mmask32 __U, __m256i __D)
{
  return (__m256i) __builtin_ia32_expandqi256_mask ((__v32qi) __D,
              (__v32qi) __S,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_maskz_expand_epi8(__mmask32 __U, __m256i __D)
{
  return (__m256i) __builtin_ia32_expandqi256_mask ((__v32qi) __D,
              (__v32qi) _mm256_setzero_si256(),
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_mask_expandloadu_epi16(__m256i __S, __mmask16 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_expandloadhi256_mask ((const __v16hi *)__P,
              (__v16hi) __S,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_maskz_expandloadu_epi16(__mmask16 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_expandloadhi256_mask ((const __v16hi *)__P,
              (__v16hi) _mm256_setzero_si256(),
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_mask_expandloadu_epi8(__m256i __S, __mmask32 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_expandloadqi256_mask ((const __v32qi *)__P,
              (__v32qi) __S,
              __U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_maskz_expandloadu_epi8(__mmask32 __U, void const *__P)
{
  return (__m256i) __builtin_ia32_expandloadqi256_mask ((const __v32qi *)__P,
              (__v32qi) _mm256_setzero_si256(),
              __U);
}
# 424 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlvbmi2intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_shldv_epi64(__m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_vpshldvq256((__v4di)__A, (__v4di)__B,
                                             (__v4di)__C);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_mask_shldv_epi64(__m256i __A, __mmask8 __U, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_selectq_256(__U,
                                      (__v4di)_mm256_shldv_epi64(__A, __B, __C),
                                      (__v4di)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_maskz_shldv_epi64(__mmask8 __U, __m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_selectq_256(__U,
                                      (__v4di)_mm256_shldv_epi64(__A, __B, __C),
                                      (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_shldv_epi64(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpshldvq128((__v2di)__A, (__v2di)__B,
                                             (__v2di)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_mask_shldv_epi64(__m128i __A, __mmask8 __U, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_selectq_128(__U,
                                         (__v2di)_mm_shldv_epi64(__A, __B, __C),
                                         (__v2di)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_maskz_shldv_epi64(__mmask8 __U, __m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_selectq_128(__U,
                                         (__v2di)_mm_shldv_epi64(__A, __B, __C),
                                         (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_shldv_epi32(__m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_vpshldvd256((__v8si)__A, (__v8si)__B,
                                             (__v8si)__C);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_mask_shldv_epi32(__m256i __A, __mmask8 __U, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                      (__v8si)_mm256_shldv_epi32(__A, __B, __C),
                                      (__v8si)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_maskz_shldv_epi32(__mmask8 __U, __m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                      (__v8si)_mm256_shldv_epi32(__A, __B, __C),
                                      (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_shldv_epi32(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpshldvd128((__v4si)__A, (__v4si)__B,
                                             (__v4si)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_mask_shldv_epi32(__m128i __A, __mmask8 __U, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                         (__v4si)_mm_shldv_epi32(__A, __B, __C),
                                         (__v4si)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_maskz_shldv_epi32(__mmask8 __U, __m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                         (__v4si)_mm_shldv_epi32(__A, __B, __C),
                                         (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_shldv_epi16(__m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_vpshldvw256((__v16hi)__A, (__v16hi)__B,
                                             (__v16hi)__C);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_mask_shldv_epi16(__m256i __A, __mmask16 __U, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_selectw_256(__U,
                                      (__v16hi)_mm256_shldv_epi16(__A, __B, __C),
                                      (__v16hi)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_maskz_shldv_epi16(__mmask16 __U, __m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_selectw_256(__U,
                                      (__v16hi)_mm256_shldv_epi16(__A, __B, __C),
                                      (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_shldv_epi16(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpshldvw128((__v8hi)__A, (__v8hi)__B,
                                             (__v8hi)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_mask_shldv_epi16(__m128i __A, __mmask8 __U, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_selectw_128(__U,
                                         (__v8hi)_mm_shldv_epi16(__A, __B, __C),
                                         (__v8hi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_maskz_shldv_epi16(__mmask8 __U, __m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_selectw_128(__U,
                                         (__v8hi)_mm_shldv_epi16(__A, __B, __C),
                                         (__v8hi)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_shrdv_epi64(__m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_vpshrdvq256((__v4di)__A, (__v4di)__B,
                                             (__v4di)__C);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_mask_shrdv_epi64(__m256i __A, __mmask8 __U, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_selectq_256(__U,
                                      (__v4di)_mm256_shrdv_epi64(__A, __B, __C),
                                      (__v4di)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_maskz_shrdv_epi64(__mmask8 __U, __m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_selectq_256(__U,
                                      (__v4di)_mm256_shrdv_epi64(__A, __B, __C),
                                      (__v4di)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_shrdv_epi64(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpshrdvq128((__v2di)__A, (__v2di)__B,
                                             (__v2di)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_mask_shrdv_epi64(__m128i __A, __mmask8 __U, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_selectq_128(__U,
                                         (__v2di)_mm_shrdv_epi64(__A, __B, __C),
                                         (__v2di)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_maskz_shrdv_epi64(__mmask8 __U, __m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_selectq_128(__U,
                                         (__v2di)_mm_shrdv_epi64(__A, __B, __C),
                                         (__v2di)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_shrdv_epi32(__m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_vpshrdvd256((__v8si)__A, (__v8si)__B,
                                             (__v8si)__C);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_mask_shrdv_epi32(__m256i __A, __mmask8 __U, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                      (__v8si)_mm256_shrdv_epi32(__A, __B, __C),
                                      (__v8si)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_maskz_shrdv_epi32(__mmask8 __U, __m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_selectd_256(__U,
                                      (__v8si)_mm256_shrdv_epi32(__A, __B, __C),
                                      (__v8si)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_shrdv_epi32(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpshrdvd128((__v4si)__A, (__v4si)__B,
                                             (__v4si)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_mask_shrdv_epi32(__m128i __A, __mmask8 __U, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                         (__v4si)_mm_shrdv_epi32(__A, __B, __C),
                                         (__v4si)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_maskz_shrdv_epi32(__mmask8 __U, __m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_selectd_128(__U,
                                         (__v4si)_mm_shrdv_epi32(__A, __B, __C),
                                         (__v4si)_mm_setzero_si128());
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_shrdv_epi16(__m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_vpshrdvw256((__v16hi)__A, (__v16hi)__B,
                                             (__v16hi)__C);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_mask_shrdv_epi16(__m256i __A, __mmask16 __U, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_selectw_256(__U,
                                     (__v16hi)_mm256_shrdv_epi16(__A, __B, __C),
                                     (__v16hi)__A);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(256)))
_mm256_maskz_shrdv_epi16(__mmask16 __U, __m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)__builtin_ia32_selectw_256(__U,
                                     (__v16hi)_mm256_shrdv_epi16(__A, __B, __C),
                                     (__v16hi)_mm256_setzero_si256());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_shrdv_epi16(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpshrdvw128((__v8hi)__A, (__v8hi)__B,
                                             (__v8hi)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_mask_shrdv_epi16(__m128i __A, __mmask8 __U, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_selectw_128(__U,
                                         (__v8hi)_mm_shrdv_epi16(__A, __B, __C),
                                         (__v8hi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vbmi2"), __min_vector_width__(128)))
_mm_maskz_shrdv_epi16(__mmask8 __U, __m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_selectw_128(__U,
                                         (__v8hi)_mm_shrdv_epi16(__A, __B, __C),
                                         (__v8hi)_mm_setzero_si128());
}
# 268 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512pfintrin.h" 1 3
# 273 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 1 3
# 43 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_cvtsh_h(__m512h __a) {
  return __a[0];
}

static __inline __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_setzero_ph(void) {
  return (__m128h){0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0};
}

static __inline __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(256))) _mm256_setzero_ph(void) {
  return (__m256h){0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                   0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0};
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(256))) _mm256_undefined_ph(void) {
  return (__m256h)__builtin_ia32_undef256();
}

static __inline __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_setzero_ph(void) {
  return (__m512h){0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                   0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
                   0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0};
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_undefined_ph(void) {
  return (__m128h)__builtin_ia32_undef128();
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_undefined_ph(void) {
  return (__m512h)__builtin_ia32_undef512();
}

static __inline __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_set1_ph(_Float16 __h) {
  return (__m512h)(__v32hf){__h, __h, __h, __h, __h, __h, __h, __h,
                            __h, __h, __h, __h, __h, __h, __h, __h,
                            __h, __h, __h, __h, __h, __h, __h, __h,
                            __h, __h, __h, __h, __h, __h, __h, __h};
}

static __inline __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_set_ph(_Float16 __h1, _Float16 __h2, _Float16 __h3, _Float16 __h4,
              _Float16 __h5, _Float16 __h6, _Float16 __h7, _Float16 __h8,
              _Float16 __h9, _Float16 __h10, _Float16 __h11, _Float16 __h12,
              _Float16 __h13, _Float16 __h14, _Float16 __h15, _Float16 __h16,
              _Float16 __h17, _Float16 __h18, _Float16 __h19, _Float16 __h20,
              _Float16 __h21, _Float16 __h22, _Float16 __h23, _Float16 __h24,
              _Float16 __h25, _Float16 __h26, _Float16 __h27, _Float16 __h28,
              _Float16 __h29, _Float16 __h30, _Float16 __h31, _Float16 __h32) {
  return (__m512h)(__v32hf){__h32, __h31, __h30, __h29, __h28, __h27, __h26,
                            __h25, __h24, __h23, __h22, __h21, __h20, __h19,
                            __h18, __h17, __h16, __h15, __h14, __h13, __h12,
                            __h11, __h10, __h9, __h8, __h7, __h6, __h5,
                            __h4, __h3, __h2, __h1};
}
# 105 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_set1_pch(_Float16 _Complex h) {
  return (__m512h)_mm512_set1_ps(__builtin_bit_cast(float, h));
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_castph_ps(__m128h __a) {
  return (__m128)__a;
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(256))) _mm256_castph_ps(__m256h __a) {
  return (__m256)__a;
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_castph_ps(__m512h __a) {
  return (__m512)__a;
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_castph_pd(__m128h __a) {
  return (__m128d)__a;
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(256))) _mm256_castph_pd(__m256h __a) {
  return (__m256d)__a;
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_castph_pd(__m512h __a) {
  return (__m512d)__a;
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_castph_si128(__m128h __a) {
  return (__m128i)__a;
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(256)))
_mm256_castph_si256(__m256h __a) {
  return (__m256i)__a;
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_castph_si512(__m512h __a) {
  return (__m512i)__a;
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_castps_ph(__m128 __a) {
  return (__m128h)__a;
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(256))) _mm256_castps_ph(__m256 __a) {
  return (__m256h)__a;
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_castps_ph(__m512 __a) {
  return (__m512h)__a;
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_castpd_ph(__m128d __a) {
  return (__m128h)__a;
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(256))) _mm256_castpd_ph(__m256d __a) {
  return (__m256h)__a;
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_castpd_ph(__m512d __a) {
  return (__m512h)__a;
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_castsi128_ph(__m128i __a) {
  return (__m128h)__a;
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(256)))
_mm256_castsi256_ph(__m256i __a) {
  return (__m256h)__a;
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_castsi512_ph(__m512i __a) {
  return (__m512h)__a;
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(256)))
_mm256_castph256_ph128(__m256h __a) {
  return __builtin_shufflevector(__a, __a, 0, 1, 2, 3, 4, 5, 6, 7);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_castph512_ph128(__m512h __a) {
  return __builtin_shufflevector(__a, __a, 0, 1, 2, 3, 4, 5, 6, 7);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_castph512_ph256(__m512h __a) {
  return __builtin_shufflevector(__a, __a, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
                                 12, 13, 14, 15);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(256)))
_mm256_castph128_ph256(__m128h __a) {
  return __builtin_shufflevector(__a, __a, 0, 1, 2, 3, 4, 5, 6, 7, -1, -1, -1,
                                 -1, -1, -1, -1, -1);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_castph128_ph512(__m128h __a) {
  return __builtin_shufflevector(__a, __a, 0, 1, 2, 3, 4, 5, 6, 7, -1, -1, -1,
                                 -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
                                 -1, -1, -1, -1, -1, -1, -1, -1, -1);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_castph256_ph512(__m256h __a) {
  return __builtin_shufflevector(__a, __a, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
                                 12, 13, 14, 15, -1, -1, -1, -1, -1, -1, -1, -1,
                                 -1, -1, -1, -1, -1, -1, -1, -1);
}
# 235 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(256)))
_mm256_zextph128_ph256(__m128h __a) {
  return __builtin_shufflevector(__a, (__v8hf)_mm_setzero_ph(), 0, 1, 2, 3, 4,
                                 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);
}
# 254 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_zextph128_ph512(__m128h __a) {
  return __builtin_shufflevector(
      __a, (__v8hf)_mm_setzero_ph(), 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
      13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15, 8, 9, 10, 11, 12, 13, 14, 15);
}
# 274 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_zextph256_ph512(__m256h __a) {
  return __builtin_shufflevector(__a, (__v16hf)_mm256_setzero_ph(), 0, 1, 2, 3,
                                 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,
                                 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28,
                                 29, 30, 31);
}







static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_comieq_sh(__m128h A,
                                                          __m128h B) {
  return __builtin_ia32_vcomish((__v8hf)A, (__v8hf)B, 0x10,
                                0x04);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_comilt_sh(__m128h A,
                                                          __m128h B) {
  return __builtin_ia32_vcomish((__v8hf)A, (__v8hf)B, 0x01,
                                0x04);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_comile_sh(__m128h A,
                                                          __m128h B) {
  return __builtin_ia32_vcomish((__v8hf)A, (__v8hf)B, 0x02,
                                0x04);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_comigt_sh(__m128h A,
                                                          __m128h B) {
  return __builtin_ia32_vcomish((__v8hf)A, (__v8hf)B, 0x0e,
                                0x04);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_comige_sh(__m128h A,
                                                          __m128h B) {
  return __builtin_ia32_vcomish((__v8hf)A, (__v8hf)B, 0x0d,
                                0x04);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_comineq_sh(__m128h A,
                                                           __m128h B) {
  return __builtin_ia32_vcomish((__v8hf)A, (__v8hf)B, 0x14,
                                0x04);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_ucomieq_sh(__m128h A,
                                                           __m128h B) {
  return __builtin_ia32_vcomish((__v8hf)A, (__v8hf)B, 0x00,
                                0x04);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_ucomilt_sh(__m128h A,
                                                           __m128h B) {
  return __builtin_ia32_vcomish((__v8hf)A, (__v8hf)B, 0x11,
                                0x04);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_ucomile_sh(__m128h A,
                                                           __m128h B) {
  return __builtin_ia32_vcomish((__v8hf)A, (__v8hf)B, 0x12,
                                0x04);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_ucomigt_sh(__m128h A,
                                                           __m128h B) {
  return __builtin_ia32_vcomish((__v8hf)A, (__v8hf)B, 0x1e,
                                0x04);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_ucomige_sh(__m128h A,
                                                           __m128h B) {
  return __builtin_ia32_vcomish((__v8hf)A, (__v8hf)B, 0x1d,
                                0x04);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_ucomineq_sh(__m128h A,
                                                            __m128h B) {
  return __builtin_ia32_vcomish((__v8hf)A, (__v8hf)B, 0x04,
                                0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_add_ph(__m512h __A,
                                                              __m512h __B) {
  return (__m512h)((__v32hf)__A + (__v32hf)__B);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_add_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_selectph_512(
      (__mmask32)__U, (__v32hf)_mm512_add_ph(__A, __B), (__v32hf)__W);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_add_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_selectph_512((__mmask32)__U,
                                              (__v32hf)_mm512_add_ph(__A, __B),
                                              (__v32hf)_mm512_setzero_ph());
}
# 392 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_sub_ph(__m512h __A,
                                                              __m512h __B) {
  return (__m512h)((__v32hf)__A - (__v32hf)__B);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_sub_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_selectph_512(
      (__mmask32)__U, (__v32hf)_mm512_sub_ph(__A, __B), (__v32hf)__W);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_sub_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_selectph_512((__mmask32)__U,
                                              (__v32hf)_mm512_sub_ph(__A, __B),
                                              (__v32hf)_mm512_setzero_ph());
}
# 424 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_mul_ph(__m512h __A,
                                                              __m512h __B) {
  return (__m512h)((__v32hf)__A * (__v32hf)__B);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_mul_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_selectph_512(
      (__mmask32)__U, (__v32hf)_mm512_mul_ph(__A, __B), (__v32hf)__W);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_mul_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_selectph_512((__mmask32)__U,
                                              (__v32hf)_mm512_mul_ph(__A, __B),
                                              (__v32hf)_mm512_setzero_ph());
}
# 456 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_div_ph(__m512h __A,
                                                              __m512h __B) {
  return (__m512h)((__v32hf)__A / (__v32hf)__B);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_div_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_selectph_512(
      (__mmask32)__U, (__v32hf)_mm512_div_ph(__A, __B), (__v32hf)__W);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_div_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_selectph_512((__mmask32)__U,
                                              (__v32hf)_mm512_div_ph(__A, __B),
                                              (__v32hf)_mm512_setzero_ph());
}
# 488 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_min_ph(__m512h __A,
                                                              __m512h __B) {
  return (__m512h)__builtin_ia32_minph512((__v32hf)__A, (__v32hf)__B,
                                          0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_min_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_selectph_512(
      (__mmask32)__U, (__v32hf)_mm512_min_ph(__A, __B), (__v32hf)__W);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_min_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_selectph_512((__mmask32)__U,
                                              (__v32hf)_mm512_min_ph(__A, __B),
                                              (__v32hf)_mm512_setzero_ph());
}
# 521 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_max_ph(__m512h __A,
                                                              __m512h __B) {
  return (__m512h)__builtin_ia32_maxph512((__v32hf)__A, (__v32hf)__B,
                                          0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_max_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_selectph_512(
      (__mmask32)__U, (__v32hf)_mm512_max_ph(__A, __B), (__v32hf)__W);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_max_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_selectph_512((__mmask32)__U,
                                              (__v32hf)_mm512_max_ph(__A, __B),
                                              (__v32hf)_mm512_setzero_ph());
}
# 554 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_abs_ph(__m512h __A) {
  return (__m512h)_mm512_and_epi32(_mm512_set1_epi32(0x7FFF7FFF), (__m512i)__A);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_conj_pch(__m512h __A) {
  return (__m512h)_mm512_xor_ps((__m512)__A, _mm512_set1_ps(-0.0f));
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_conj_pch(__m512h __W, __mmask16 __U, __m512h __A) {
  return (__m512h)__builtin_ia32_selectps_512(
      (__mmask16)__U, (__v16sf)_mm512_conj_pch(__A), (__v16sf)__W);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_conj_pch(__mmask16 __U, __m512h __A) {
  return (__m512h)__builtin_ia32_selectps_512((__mmask16)__U,
                                              (__v16sf)_mm512_conj_pch(__A),
                                              (__v16sf)_mm512_setzero_ps());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_add_sh(__m128h __A,
                                                           __m128h __B) {
  __A[0] += __B[0];
  return __A;
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_mask_add_sh(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  __A = _mm_add_sh(__A, __B);
  return __builtin_ia32_selectsh_128(__U, __A, __W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_maskz_add_sh(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  __A = _mm_add_sh(__A, __B);
  return __builtin_ia32_selectsh_128(__U, __A, _mm_setzero_ph());
}
# 611 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_sub_sh(__m128h __A,
                                                           __m128h __B) {
  __A[0] -= __B[0];
  return __A;
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_mask_sub_sh(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  __A = _mm_sub_sh(__A, __B);
  return __builtin_ia32_selectsh_128(__U, __A, __W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_maskz_sub_sh(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  __A = _mm_sub_sh(__A, __B);
  return __builtin_ia32_selectsh_128(__U, __A, _mm_setzero_ph());
}
# 647 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_mul_sh(__m128h __A,
                                                           __m128h __B) {
  __A[0] *= __B[0];
  return __A;
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_mask_mul_sh(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  __A = _mm_mul_sh(__A, __B);
  return __builtin_ia32_selectsh_128(__U, __A, __W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_maskz_mul_sh(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  __A = _mm_mul_sh(__A, __B);
  return __builtin_ia32_selectsh_128(__U, __A, _mm_setzero_ph());
}
# 683 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_div_sh(__m128h __A,
                                                           __m128h __B) {
  __A[0] /= __B[0];
  return __A;
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_mask_div_sh(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  __A = _mm_div_sh(__A, __B);
  return __builtin_ia32_selectsh_128(__U, __A, __W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_maskz_div_sh(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  __A = _mm_div_sh(__A, __B);
  return __builtin_ia32_selectsh_128(__U, __A, _mm_setzero_ph());
}
# 719 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_min_sh(__m128h __A,
                                                           __m128h __B) {
  return (__m128h)__builtin_ia32_minsh_round_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_mask_min_sh(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  return (__m128h)__builtin_ia32_minsh_round_mask((__v8hf)__A, (__v8hf)__B,
                                                  (__v8hf)__W, (__mmask8)__U,
                                                  0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_maskz_min_sh(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  return (__m128h)__builtin_ia32_minsh_round_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)__U,
      0x04);
}
# 758 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_max_sh(__m128h __A,
                                                           __m128h __B) {
  return (__m128h)__builtin_ia32_maxsh_round_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_mask_max_sh(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  return (__m128h)__builtin_ia32_maxsh_round_mask((__v8hf)__A, (__v8hf)__B,
                                                  (__v8hf)__W, (__mmask8)__U,
                                                  0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_maskz_max_sh(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  return (__m128h)__builtin_ia32_maxsh_round_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)__U,
      0x04);
}
# 834 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_load_sh(void const *__dp) {
  struct __mm_load_sh_struct {
    _Float16 __u;
  } __attribute__((__packed__, __may_alias__));
  _Float16 __u = ((const struct __mm_load_sh_struct *)__dp)->__u;
  return (__m128h){__u, 0, 0, 0, 0, 0, 0, 0};
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_mask_load_sh(__m128h __W, __mmask8 __U, const void *__A) {
  __m128h src = (__v8hf)__builtin_shufflevector(
      (__v8hf)__W, (__v8hf)_mm_setzero_ph(), 0, 8, 8, 8, 8, 8, 8, 8);

  return (__m128h)__builtin_ia32_loadsh128_mask((const __v8hf *)__A, src, __U & 1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_maskz_load_sh(__mmask8 __U, const void *__A) {
  return (__m128h)__builtin_ia32_loadsh128_mask(
      (const __v8hf *)__A, (__v8hf)_mm_setzero_ph(), __U & 1);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_load_ph(void const *__p) {
  return *(const __m512h *)__p;
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(256)))
_mm256_load_ph(void const *__p) {
  return *(const __m256h *)__p;
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_load_ph(void const *__p) {
  return *(const __m128h *)__p;
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_loadu_ph(void const *__p) {
  struct __loadu_ph {
    __m512h_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_ph *)__p)->__v;
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(256)))
_mm256_loadu_ph(void const *__p) {
  struct __loadu_ph {
    __m256h_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_ph *)__p)->__v;
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_loadu_ph(void const *__p) {
  struct __loadu_ph {
    __m128h_u __v;
  } __attribute__((__packed__, __may_alias__));
  return ((const struct __loadu_ph *)__p)->__v;
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_store_sh(void *__dp,
                                                          __m128h __a) {
  struct __mm_store_sh_struct {
    _Float16 __u;
  } __attribute__((__packed__, __may_alias__));
  ((struct __mm_store_sh_struct *)__dp)->__u = __a[0];
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_mask_store_sh(void *__W,
                                                               __mmask8 __U,
                                                               __m128h __A) {
  __builtin_ia32_storesh128_mask((__v8hf *)__W, __A, __U & 1);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_store_ph(void *__P,
                                                             __m512h __A) {
  *(__m512h *)__P = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(256))) _mm256_store_ph(void *__P,
                                                             __m256h __A) {
  *(__m256h *)__P = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_store_ph(void *__P,
                                                          __m128h __A) {
  *(__m128h *)__P = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_storeu_ph(void *__P,
                                                              __m512h __A) {
  struct __storeu_ph {
    __m512h_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_ph *)__P)->__v = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(256))) _mm256_storeu_ph(void *__P,
                                                              __m256h __A) {
  struct __storeu_ph {
    __m256h_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_ph *)__P)->__v = __A;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_storeu_ph(void *__P,
                                                           __m128h __A) {
  struct __storeu_ph {
    __m128h_u __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_ph *)__P)->__v = __A;
}


static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_move_sh(__m128h __a,
                                                            __m128h __b) {
  __a[0] = __b[0];
  return __a;
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_mask_move_sh(__m128h __W,
                                                                 __mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  return __builtin_ia32_selectsh_128(__U, _mm_move_sh(__A, __B), __W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_maskz_move_sh(__mmask8 __U,
                                                                  __m128h __A,
                                                                  __m128h __B) {
  return __builtin_ia32_selectsh_128(__U, _mm_move_sh(__A, __B),
                                     _mm_setzero_ph());
}


static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_cvtsi16_si128(short __a) {
  return (__m128i)(__v8hi){__a, 0, 0, 0, 0, 0, 0, 0};
}

static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_cvtsi128_si16(__m128i __a) {
  __v8hi __b = (__v8hi)__a;
  return __b[0];
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_rcp_ph(__m512h __A) {
  return (__m512h)__builtin_ia32_rcpph512_mask(
      (__v32hf)__A, (__v32hf)_mm512_undefined_ph(), (__mmask32)-1);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_rcp_ph(__m512h __W, __mmask32 __U, __m512h __A) {
  return (__m512h)__builtin_ia32_rcpph512_mask((__v32hf)__A, (__v32hf)__W,
                                               (__mmask32)__U);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_rcp_ph(__mmask32 __U, __m512h __A) {
  return (__m512h)__builtin_ia32_rcpph512_mask(
      (__v32hf)__A, (__v32hf)_mm512_setzero_ph(), (__mmask32)__U);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_rsqrt_ph(__m512h __A) {
  return (__m512h)__builtin_ia32_rsqrtph512_mask(
      (__v32hf)__A, (__v32hf)_mm512_undefined_ph(), (__mmask32)-1);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_rsqrt_ph(__m512h __W, __mmask32 __U, __m512h __A) {
  return (__m512h)__builtin_ia32_rsqrtph512_mask((__v32hf)__A, (__v32hf)__W,
                                                 (__mmask32)__U);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_rsqrt_ph(__mmask32 __U, __m512h __A) {
  return (__m512h)__builtin_ia32_rsqrtph512_mask(
      (__v32hf)__A, (__v32hf)_mm512_setzero_ph(), (__mmask32)__U);
}
# 1043 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_getexp_ph(__m512h __A) {
  return (__m512h)__builtin_ia32_getexpph512_mask(
      (__v32hf)__A, (__v32hf)_mm512_undefined_ph(), (__mmask32)-1,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_getexp_ph(__m512h __W, __mmask32 __U, __m512h __A) {
  return (__m512h)__builtin_ia32_getexpph512_mask(
      (__v32hf)__A, (__v32hf)__W, (__mmask32)__U, 0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_getexp_ph(__mmask32 __U, __m512h __A) {
  return (__m512h)__builtin_ia32_getexpph512_mask(
      (__v32hf)__A, (__v32hf)_mm512_setzero_ph(), (__mmask32)__U,
      0x04);
}
# 1076 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_scalef_ph(__m512h __A,
                                                                 __m512h __B) {
  return (__m512h)__builtin_ia32_scalefph512_mask(
      (__v32hf)__A, (__v32hf)__B, (__v32hf)_mm512_undefined_ph(), (__mmask32)-1,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_scalef_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_scalefph512_mask((__v32hf)__A, (__v32hf)__B,
                                                  (__v32hf)__W, (__mmask32)__U,
                                                  0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_scalef_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_scalefph512_mask(
      (__v32hf)__A, (__v32hf)__B, (__v32hf)_mm512_setzero_ph(), (__mmask32)__U,
      0x04);
}
# 1172 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_rcp_sh(__m128h __A,
                                                           __m128h __B) {
  return (__m128h)__builtin_ia32_rcpsh_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_mask_rcp_sh(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  return (__m128h)__builtin_ia32_rcpsh_mask((__v8hf)__A, (__v8hf)__B,
                                            (__v8hf)__W, (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_maskz_rcp_sh(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  return (__m128h)__builtin_ia32_rcpsh_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_rsqrt_sh(__m128h __A,
                                                             __m128h __B) {
  return (__m128h)__builtin_ia32_rsqrtsh_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_mask_rsqrt_sh(__m128h __W,
                                                                  __mmask8 __U,
                                                                  __m128h __A,
                                                                  __m128h __B) {
  return (__m128h)__builtin_ia32_rsqrtsh_mask((__v8hf)__A, (__v8hf)__B,
                                              (__v8hf)__W, (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_maskz_rsqrt_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_rsqrtsh_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}
# 1248 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_getexp_sh(__m128h __A,
                                                              __m128h __B) {
  return (__m128h)__builtin_ia32_getexpsh128_round_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_mask_getexp_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_getexpsh128_round_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)__W, (__mmask8)__U,
      0x04);
}






static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_maskz_getexp_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_getexpsh128_round_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)__U,
      0x04);
}
# 1284 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_scalef_sh(__m128h __A,
                                                              __m128h __B) {
  return (__m128h)__builtin_ia32_scalefsh_round_mask(
      (__v8hf)__A, (__v8hf)(__B), (__v8hf)_mm_setzero_ph(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_mask_scalef_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_scalefsh_round_mask((__v8hf)__A, (__v8hf)__B,
                                                     (__v8hf)__W, (__mmask8)__U,
                                                     0x04);
}






static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_maskz_scalef_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_scalefsh_round_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)__U,
      0x04);
}
# 1388 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_sqrt_ph(__m512h __A) {
  return (__m512h)__builtin_ia32_sqrtph512((__v32hf)__A,
                                           0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_sqrt_ph(__m512h __W, __mmask32 __U, __m512h __A) {
  return (__m512h)__builtin_ia32_selectph_512(
      (__mmask32)(__U),
      (__v32hf)__builtin_ia32_sqrtph512((__A), (0x04)),
      (__v32hf)(__m512h)(__W));
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_sqrt_ph(__mmask32 __U, __m512h __A) {
  return (__m512h)__builtin_ia32_selectph_512(
      (__mmask32)(__U),
      (__v32hf)__builtin_ia32_sqrtph512((__A), (0x04)),
      (__v32hf)_mm512_setzero_ph());
}
# 1424 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_sqrt_sh(__m128h __A,
                                                            __m128h __B) {
  return (__m128h)__builtin_ia32_sqrtsh_round_mask(
      (__v8hf)(__m128h)(__A), (__v8hf)(__m128h)(__B), (__v8hf)_mm_setzero_ph(),
      (__mmask8)-1, 0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_mask_sqrt_sh(__m128h __W,
                                                                 __mmask32 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  return (__m128h)__builtin_ia32_sqrtsh_round_mask(
      (__v8hf)(__m128h)(__A), (__v8hf)(__m128h)(__B), (__v8hf)(__m128h)(__W),
      (__mmask8)(__U), 0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_maskz_sqrt_sh(__mmask32 __U,
                                                                  __m128h __A,
                                                                  __m128h __B) {
  return (__m128h)__builtin_ia32_sqrtsh_round_mask(
      (__v8hf)(__m128h)(__A), (__v8hf)(__m128h)(__B), (__v8hf)_mm_setzero_ph(),
      (__mmask8)(__U), 0x04);
}
# 1476 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_cvtpd_ph(__m512d __A) {
  return (__m128h)__builtin_ia32_vcvtpd2ph512_mask(
      (__v8df)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_cvtpd_ph(__m128h __W, __mmask8 __U, __m512d __A) {
  return (__m128h)__builtin_ia32_vcvtpd2ph512_mask(
      (__v8df)__A, (__v8hf)__W, (__mmask8)__U, 0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_cvtpd_ph(__mmask8 __U, __m512d __A) {
  return (__m128h)__builtin_ia32_vcvtpd2ph512_mask(
      (__v8df)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U,
      0x04);
}
# 1507 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_cvtph_pd(__m128h __A) {
  return (__m512d)__builtin_ia32_vcvtph2pd512_mask(
      (__v8hf)__A, (__v8df)_mm512_setzero_pd(), (__mmask8)-1,
      0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_cvtph_pd(__m512d __W, __mmask8 __U, __m128h __A) {
  return (__m512d)__builtin_ia32_vcvtph2pd512_mask(
      (__v8hf)__A, (__v8df)__W, (__mmask8)__U, 0x04);
}

static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_cvtph_pd(__mmask8 __U, __m128h __A) {
  return (__m512d)__builtin_ia32_vcvtph2pd512_mask(
      (__v8hf)__A, (__v8df)_mm512_setzero_pd(), (__mmask8)__U,
      0x04);
}
# 1540 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_cvtsh_ss(__m128 __A,
                                                            __m128h __B) {
  return (__m128)__builtin_ia32_vcvtsh2ss_round_mask(
      (__v4sf)__A, (__v8hf)__B, (__v4sf)_mm_undefined_ps(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_mask_cvtsh_ss(__m128 __W,
                                                                 __mmask8 __U,
                                                                 __m128 __A,
                                                                 __m128h __B) {
  return (__m128)__builtin_ia32_vcvtsh2ss_round_mask((__v4sf)__A, (__v8hf)__B,
                                                     (__v4sf)__W, (__mmask8)__U,
                                                     0x04);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_maskz_cvtsh_ss(__mmask8 __U,
                                                                  __m128 __A,
                                                                  __m128h __B) {
  return (__m128)__builtin_ia32_vcvtsh2ss_round_mask(
      (__v4sf)__A, (__v8hf)__B, (__v4sf)_mm_setzero_ps(), (__mmask8)__U,
      0x04);
}
# 1578 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_cvtss_sh(__m128h __A,
                                                             __m128 __B) {
  return (__m128h)__builtin_ia32_vcvtss2sh_round_mask(
      (__v8hf)__A, (__v4sf)__B, (__v8hf)_mm_undefined_ph(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_mask_cvtss_sh(__m128h __W,
                                                                  __mmask8 __U,
                                                                  __m128h __A,
                                                                  __m128 __B) {
  return (__m128h)__builtin_ia32_vcvtss2sh_round_mask(
      (__v8hf)__A, (__v4sf)__B, (__v8hf)__W, (__mmask8)__U,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_maskz_cvtss_sh(__mmask8 __U,
                                                                   __m128h __A,
                                                                   __m128 __B) {
  return (__m128h)__builtin_ia32_vcvtss2sh_round_mask(
      (__v8hf)__A, (__v4sf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)__U,
      0x04);
}
# 1616 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_cvtsd_sh(__m128h __A,
                                                             __m128d __B) {
  return (__m128h)__builtin_ia32_vcvtsd2sh_round_mask(
      (__v8hf)__A, (__v2df)__B, (__v8hf)_mm_undefined_ph(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_mask_cvtsd_sh(__m128h __W,
                                                                  __mmask8 __U,
                                                                  __m128h __A,
                                                                  __m128d __B) {
  return (__m128h)__builtin_ia32_vcvtsd2sh_round_mask(
      (__v8hf)__A, (__v2df)__B, (__v8hf)__W, (__mmask8)__U,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_maskz_cvtsd_sh(__mmask8 __U, __m128h __A, __m128d __B) {
  return (__m128h)__builtin_ia32_vcvtsd2sh_round_mask(
      (__v8hf)__A, (__v2df)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)__U,
      0x04);
}
# 1653 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_cvtsh_sd(__m128d __A,
                                                             __m128h __B) {
  return (__m128d)__builtin_ia32_vcvtsh2sd_round_mask(
      (__v2df)__A, (__v8hf)__B, (__v2df)_mm_undefined_pd(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_mask_cvtsh_sd(__m128d __W,
                                                                  __mmask8 __U,
                                                                  __m128d __A,
                                                                  __m128h __B) {
  return (__m128d)__builtin_ia32_vcvtsh2sd_round_mask(
      (__v2df)__A, (__v8hf)__B, (__v2df)__W, (__mmask8)__U,
      0x04);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_maskz_cvtsh_sd(__mmask8 __U, __m128d __A, __m128h __B) {
  return (__m128d)__builtin_ia32_vcvtsh2sd_round_mask(
      (__v2df)__A, (__v8hf)__B, (__v2df)_mm_setzero_pd(), (__mmask8)__U,
      0x04);
}
# 1690 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_cvtph_epi16(__m512h __A) {
  return (__m512i)__builtin_ia32_vcvtph2w512_mask(
      (__v32hf)__A, (__v32hi)_mm512_setzero_si512(), (__mmask32)-1,
      0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_cvtph_epi16(__m512i __W, __mmask32 __U, __m512h __A) {
  return (__m512i)__builtin_ia32_vcvtph2w512_mask(
      (__v32hf)__A, (__v32hi)__W, (__mmask32)__U, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_cvtph_epi16(__mmask32 __U, __m512h __A) {
  return (__m512i)__builtin_ia32_vcvtph2w512_mask(
      (__v32hf)__A, (__v32hi)_mm512_setzero_si512(), (__mmask32)__U,
      0x04);
}
# 1724 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_cvttph_epi16(__m512h __A) {
  return (__m512i)__builtin_ia32_vcvttph2w512_mask(
      (__v32hf)__A, (__v32hi)_mm512_setzero_si512(), (__mmask32)-1,
      0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_cvttph_epi16(__m512i __W, __mmask32 __U, __m512h __A) {
  return (__m512i)__builtin_ia32_vcvttph2w512_mask(
      (__v32hf)__A, (__v32hi)__W, (__mmask32)__U, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_cvttph_epi16(__mmask32 __U, __m512h __A) {
  return (__m512i)__builtin_ia32_vcvttph2w512_mask(
      (__v32hf)__A, (__v32hi)_mm512_setzero_si512(), (__mmask32)__U,
      0x04);
}
# 1757 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_cvtepi16_ph(__m512i __A) {
  return (__m512h)__builtin_ia32_vcvtw2ph512_mask(
      (__v32hi)__A, (__v32hf)_mm512_setzero_ph(), (__mmask32)-1,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_cvtepi16_ph(__m512h __W, __mmask32 __U, __m512i __A) {
  return (__m512h)__builtin_ia32_vcvtw2ph512_mask(
      (__v32hi)__A, (__v32hf)__W, (__mmask32)__U, 0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_cvtepi16_ph(__mmask32 __U, __m512i __A) {
  return (__m512h)__builtin_ia32_vcvtw2ph512_mask(
      (__v32hi)__A, (__v32hf)_mm512_setzero_ph(), (__mmask32)__U,
      0x04);
}
# 1791 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_cvtph_epu16(__m512h __A) {
  return (__m512i)__builtin_ia32_vcvtph2uw512_mask(
      (__v32hf)__A, (__v32hu)_mm512_setzero_si512(), (__mmask32)-1,
      0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_cvtph_epu16(__m512i __W, __mmask32 __U, __m512h __A) {
  return (__m512i)__builtin_ia32_vcvtph2uw512_mask(
      (__v32hf)__A, (__v32hu)__W, (__mmask32)__U, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_cvtph_epu16(__mmask32 __U, __m512h __A) {
  return (__m512i)__builtin_ia32_vcvtph2uw512_mask(
      (__v32hf)__A, (__v32hu)_mm512_setzero_si512(), (__mmask32)__U,
      0x04);
}
# 1825 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_cvttph_epu16(__m512h __A) {
  return (__m512i)__builtin_ia32_vcvttph2uw512_mask(
      (__v32hf)__A, (__v32hu)_mm512_setzero_si512(), (__mmask32)-1,
      0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_cvttph_epu16(__m512i __W, __mmask32 __U, __m512h __A) {
  return (__m512i)__builtin_ia32_vcvttph2uw512_mask(
      (__v32hf)__A, (__v32hu)__W, (__mmask32)__U, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_cvttph_epu16(__mmask32 __U, __m512h __A) {
  return (__m512i)__builtin_ia32_vcvttph2uw512_mask(
      (__v32hf)__A, (__v32hu)_mm512_setzero_si512(), (__mmask32)__U,
      0x04);
}
# 1858 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_cvtepu16_ph(__m512i __A) {
  return (__m512h)__builtin_ia32_vcvtuw2ph512_mask(
      (__v32hu)__A, (__v32hf)_mm512_setzero_ph(), (__mmask32)-1,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_cvtepu16_ph(__m512h __W, __mmask32 __U, __m512i __A) {
  return (__m512h)__builtin_ia32_vcvtuw2ph512_mask(
      (__v32hu)__A, (__v32hf)__W, (__mmask32)__U, 0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_cvtepu16_ph(__mmask32 __U, __m512i __A) {
  return (__m512h)__builtin_ia32_vcvtuw2ph512_mask(
      (__v32hu)__A, (__v32hf)_mm512_setzero_ph(), (__mmask32)__U,
      0x04);
}
# 1892 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_cvtph_epi32(__m256h __A) {
  return (__m512i)__builtin_ia32_vcvtph2dq512_mask(
      (__v16hf)__A, (__v16si)_mm512_setzero_si512(), (__mmask16)-1,
      0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_cvtph_epi32(__m512i __W, __mmask16 __U, __m256h __A) {
  return (__m512i)__builtin_ia32_vcvtph2dq512_mask(
      (__v16hf)__A, (__v16si)__W, (__mmask16)__U, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_cvtph_epi32(__mmask16 __U, __m256h __A) {
  return (__m512i)__builtin_ia32_vcvtph2dq512_mask(
      (__v16hf)__A, (__v16si)_mm512_setzero_si512(), (__mmask16)__U,
      0x04);
}
# 1926 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_cvtph_epu32(__m256h __A) {
  return (__m512i)__builtin_ia32_vcvtph2udq512_mask(
      (__v16hf)__A, (__v16su)_mm512_setzero_si512(), (__mmask16)-1,
      0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_cvtph_epu32(__m512i __W, __mmask16 __U, __m256h __A) {
  return (__m512i)__builtin_ia32_vcvtph2udq512_mask(
      (__v16hf)__A, (__v16su)__W, (__mmask16)__U, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_cvtph_epu32(__mmask16 __U, __m256h __A) {
  return (__m512i)__builtin_ia32_vcvtph2udq512_mask(
      (__v16hf)__A, (__v16su)_mm512_setzero_si512(), (__mmask16)__U,
      0x04);
}
# 1959 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_cvtepi32_ph(__m512i __A) {
  return (__m256h)__builtin_ia32_vcvtdq2ph512_mask(
      (__v16si)__A, (__v16hf)_mm256_setzero_ph(), (__mmask16)-1,
      0x04);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_cvtepi32_ph(__m256h __W, __mmask16 __U, __m512i __A) {
  return (__m256h)__builtin_ia32_vcvtdq2ph512_mask(
      (__v16si)__A, (__v16hf)__W, (__mmask16)__U, 0x04);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_cvtepi32_ph(__mmask16 __U, __m512i __A) {
  return (__m256h)__builtin_ia32_vcvtdq2ph512_mask(
      (__v16si)__A, (__v16hf)_mm256_setzero_ph(), (__mmask16)__U,
      0x04);
}
# 1992 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_cvtepu32_ph(__m512i __A) {
  return (__m256h)__builtin_ia32_vcvtudq2ph512_mask(
      (__v16su)__A, (__v16hf)_mm256_setzero_ph(), (__mmask16)-1,
      0x04);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_cvtepu32_ph(__m256h __W, __mmask16 __U, __m512i __A) {
  return (__m256h)__builtin_ia32_vcvtudq2ph512_mask(
      (__v16su)__A, (__v16hf)__W, (__mmask16)__U, 0x04);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_cvtepu32_ph(__mmask16 __U, __m512i __A) {
  return (__m256h)__builtin_ia32_vcvtudq2ph512_mask(
      (__v16su)__A, (__v16hf)_mm256_setzero_ph(), (__mmask16)__U,
      0x04);
}
# 2026 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_cvttph_epi32(__m256h __A) {
  return (__m512i)__builtin_ia32_vcvttph2dq512_mask(
      (__v16hf)__A, (__v16si)_mm512_setzero_si512(), (__mmask16)-1,
      0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_cvttph_epi32(__m512i __W, __mmask16 __U, __m256h __A) {
  return (__m512i)__builtin_ia32_vcvttph2dq512_mask(
      (__v16hf)__A, (__v16si)__W, (__mmask16)__U, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_cvttph_epi32(__mmask16 __U, __m256h __A) {
  return (__m512i)__builtin_ia32_vcvttph2dq512_mask(
      (__v16hf)__A, (__v16si)_mm512_setzero_si512(), (__mmask16)__U,
      0x04);
}
# 2060 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_cvttph_epu32(__m256h __A) {
  return (__m512i)__builtin_ia32_vcvttph2udq512_mask(
      (__v16hf)__A, (__v16su)_mm512_setzero_si512(), (__mmask16)-1,
      0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_cvttph_epu32(__m512i __W, __mmask16 __U, __m256h __A) {
  return (__m512i)__builtin_ia32_vcvttph2udq512_mask(
      (__v16hf)__A, (__v16su)__W, (__mmask16)__U, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_cvttph_epu32(__mmask16 __U, __m256h __A) {
  return (__m512i)__builtin_ia32_vcvttph2udq512_mask(
      (__v16hf)__A, (__v16su)_mm512_setzero_si512(), (__mmask16)__U,
      0x04);
}
# 2092 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_cvtepi64_ph(__m512i __A) {
  return (__m128h)__builtin_ia32_vcvtqq2ph512_mask(
      (__v8di)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_cvtepi64_ph(__m128h __W, __mmask8 __U, __m512i __A) {
  return (__m128h)__builtin_ia32_vcvtqq2ph512_mask(
      (__v8di)__A, (__v8hf)__W, (__mmask8)__U, 0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_cvtepi64_ph(__mmask8 __U, __m512i __A) {
  return (__m128h)__builtin_ia32_vcvtqq2ph512_mask(
      (__v8di)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U,
      0x04);
}
# 2125 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_cvtph_epi64(__m128h __A) {
  return (__m512i)__builtin_ia32_vcvtph2qq512_mask(
      (__v8hf)__A, (__v8di)_mm512_setzero_si512(), (__mmask8)-1,
      0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_cvtph_epi64(__m512i __W, __mmask8 __U, __m128h __A) {
  return (__m512i)__builtin_ia32_vcvtph2qq512_mask(
      (__v8hf)__A, (__v8di)__W, (__mmask8)__U, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_cvtph_epi64(__mmask8 __U, __m128h __A) {
  return (__m512i)__builtin_ia32_vcvtph2qq512_mask(
      (__v8hf)__A, (__v8di)_mm512_setzero_si512(), (__mmask8)__U,
      0x04);
}
# 2157 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_cvtepu64_ph(__m512i __A) {
  return (__m128h)__builtin_ia32_vcvtuqq2ph512_mask(
      (__v8du)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_cvtepu64_ph(__m128h __W, __mmask8 __U, __m512i __A) {
  return (__m128h)__builtin_ia32_vcvtuqq2ph512_mask(
      (__v8du)__A, (__v8hf)__W, (__mmask8)__U, 0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_cvtepu64_ph(__mmask8 __U, __m512i __A) {
  return (__m128h)__builtin_ia32_vcvtuqq2ph512_mask(
      (__v8du)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U,
      0x04);
}
# 2190 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_cvtph_epu64(__m128h __A) {
  return (__m512i)__builtin_ia32_vcvtph2uqq512_mask(
      (__v8hf)__A, (__v8du)_mm512_setzero_si512(), (__mmask8)-1,
      0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_cvtph_epu64(__m512i __W, __mmask8 __U, __m128h __A) {
  return (__m512i)__builtin_ia32_vcvtph2uqq512_mask(
      (__v8hf)__A, (__v8du)__W, (__mmask8)__U, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_cvtph_epu64(__mmask8 __U, __m128h __A) {
  return (__m512i)__builtin_ia32_vcvtph2uqq512_mask(
      (__v8hf)__A, (__v8du)_mm512_setzero_si512(), (__mmask8)__U,
      0x04);
}
# 2223 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_cvttph_epi64(__m128h __A) {
  return (__m512i)__builtin_ia32_vcvttph2qq512_mask(
      (__v8hf)__A, (__v8di)_mm512_setzero_si512(), (__mmask8)-1,
      0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_cvttph_epi64(__m512i __W, __mmask8 __U, __m128h __A) {
  return (__m512i)__builtin_ia32_vcvttph2qq512_mask(
      (__v8hf)__A, (__v8di)__W, (__mmask8)__U, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_cvttph_epi64(__mmask8 __U, __m128h __A) {
  return (__m512i)__builtin_ia32_vcvttph2qq512_mask(
      (__v8hf)__A, (__v8di)_mm512_setzero_si512(), (__mmask8)__U,
      0x04);
}
# 2256 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_cvttph_epu64(__m128h __A) {
  return (__m512i)__builtin_ia32_vcvttph2uqq512_mask(
      (__v8hf)__A, (__v8du)_mm512_setzero_si512(), (__mmask8)-1,
      0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_cvttph_epu64(__m512i __W, __mmask8 __U, __m128h __A) {
  return (__m512i)__builtin_ia32_vcvttph2uqq512_mask(
      (__v8hf)__A, (__v8du)__W, (__mmask8)__U, 0x04);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_cvttph_epu64(__mmask8 __U, __m128h __A) {
  return (__m512i)__builtin_ia32_vcvttph2uqq512_mask(
      (__v8hf)__A, (__v8du)_mm512_setzero_si512(), (__mmask8)__U,
      0x04);
}




static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_cvtsh_i32(__m128h __A) {
  return (int)__builtin_ia32_vcvtsh2si32((__v8hf)__A, 0x04);
}




static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_cvtsh_u32(__m128h __A) {
  return (unsigned int)__builtin_ia32_vcvtsh2usi32((__v8hf)__A,
                                                   0x04);
}





static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_cvtsh_i64(__m128h __A) {
  return (long long)__builtin_ia32_vcvtsh2si64((__v8hf)__A,
                                               0x04);
}




static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_cvtsh_u64(__m128h __A) {
  return (unsigned long long)__builtin_ia32_vcvtsh2usi64(
      (__v8hf)__A, 0x04);
}





static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_cvtu32_sh(__m128h __A, unsigned int __B) {
  __A[0] = __B;
  return __A;
}






static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_cvtu64_sh(__m128h __A, unsigned long long __B) {
  __A[0] = __B;
  return __A;
}





static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_cvti32_sh(__m128h __A,
                                                              int __B) {
  __A[0] = __B;
  return __A;
}





static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_cvti64_sh(__m128h __A,
                                                              long long __B) {
  __A[0] = __B;
  return __A;
}





static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_cvttsh_i32(__m128h __A) {
  return (int)__builtin_ia32_vcvttsh2si32((__v8hf)__A,
                                          0x04);
}





static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_cvttsh_i64(__m128h __A) {
  return (long long)__builtin_ia32_vcvttsh2si64((__v8hf)__A,
                                                0x04);
}





static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_cvttsh_u32(__m128h __A) {
  return (unsigned int)__builtin_ia32_vcvttsh2usi32((__v8hf)__A,
                                                    0x04);
}





static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_cvttsh_u64(__m128h __A) {
  return (unsigned long long)__builtin_ia32_vcvttsh2usi64(
      (__v8hf)__A, 0x04);
}
# 2403 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_cvtxph_ps(__m256h __A) {
  return (__m512)__builtin_ia32_vcvtph2psx512_mask(
      (__v16hf)__A, (__v16sf)_mm512_setzero_ps(), (__mmask16)-1,
      0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_cvtxph_ps(__m512 __W, __mmask16 __U, __m256h __A) {
  return (__m512)__builtin_ia32_vcvtph2psx512_mask(
      (__v16hf)__A, (__v16sf)__W, (__mmask16)__U, 0x04);
}

static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_cvtxph_ps(__mmask16 __U, __m256h __A) {
  return (__m512)__builtin_ia32_vcvtph2psx512_mask(
      (__v16hf)__A, (__v16sf)_mm512_setzero_ps(), (__mmask16)__U,
      0x04);
}
# 2435 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_cvtxps_ph(__m512 __A) {
  return (__m256h)__builtin_ia32_vcvtps2phx512_mask(
      (__v16sf)__A, (__v16hf)_mm256_setzero_ph(), (__mmask16)-1,
      0x04);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_cvtxps_ph(__m256h __W, __mmask16 __U, __m512 __A) {
  return (__m256h)__builtin_ia32_vcvtps2phx512_mask(
      (__v16sf)__A, (__v16hf)__W, (__mmask16)__U, 0x04);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_cvtxps_ph(__mmask16 __U, __m512 __A) {
  return (__m256h)__builtin_ia32_vcvtps2phx512_mask(
      (__v16sf)__A, (__v16hf)_mm256_setzero_ph(), (__mmask16)__U,
      0x04);
}
# 2514 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_fmadd_ph(__m512h __A,
                                                                __m512h __B,
                                                                __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddph512_mask((__v32hf)__A, (__v32hf)__B,
                                                  (__v32hf)__C, (__mmask32)-1,
                                                  0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_fmadd_ph(__m512h __A, __mmask32 __U, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddph512_mask((__v32hf)__A, (__v32hf)__B,
                                                  (__v32hf)__C, (__mmask32)__U,
                                                  0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask3_fmadd_ph(__m512h __A, __m512h __B, __m512h __C, __mmask32 __U) {
  return (__m512h)__builtin_ia32_vfmaddph512_mask3((__v32hf)__A, (__v32hf)__B,
                                                   (__v32hf)__C, (__mmask32)__U,
                                                   0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_fmadd_ph(__mmask32 __U, __m512h __A, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddph512_maskz((__v32hf)__A, (__v32hf)__B,
                                                   (__v32hf)__C, (__mmask32)__U,
                                                   0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_fmsub_ph(__m512h __A,
                                                                __m512h __B,
                                                                __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddph512_mask((__v32hf)__A, (__v32hf)__B,
                                                  -(__v32hf)__C, (__mmask32)-1,
                                                  0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_fmsub_ph(__m512h __A, __mmask32 __U, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddph512_mask((__v32hf)__A, (__v32hf)__B,
                                                  -(__v32hf)__C, (__mmask32)__U,
                                                  0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_fmsub_ph(__mmask32 __U, __m512h __A, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddph512_maskz(
      (__v32hf)__A, (__v32hf)__B, -(__v32hf)__C, (__mmask32)__U,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_fnmadd_ph(__m512h __A,
                                                                 __m512h __B,
                                                                 __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddph512_mask((__v32hf)__A, -(__v32hf)__B,
                                                  (__v32hf)__C, (__mmask32)-1,
                                                  0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask3_fnmadd_ph(__m512h __A, __m512h __B, __m512h __C, __mmask32 __U) {
  return (__m512h)__builtin_ia32_vfmaddph512_mask3(-(__v32hf)__A, (__v32hf)__B,
                                                   (__v32hf)__C, (__mmask32)__U,
                                                   0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_fnmadd_ph(__mmask32 __U, __m512h __A, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddph512_maskz(-(__v32hf)__A, (__v32hf)__B,
                                                   (__v32hf)__C, (__mmask32)__U,
                                                   0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_fnmsub_ph(__m512h __A,
                                                                 __m512h __B,
                                                                 __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddph512_mask((__v32hf)__A, -(__v32hf)__B,
                                                  -(__v32hf)__C, (__mmask32)-1,
                                                  0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_fnmsub_ph(__mmask32 __U, __m512h __A, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddph512_maskz(
      -(__v32hf)__A, (__v32hf)__B, -(__v32hf)__C, (__mmask32)__U,
      0x04);
}
# 2637 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_fmaddsub_ph(__m512h __A, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddsubph512_mask(
      (__v32hf)__A, (__v32hf)__B, (__v32hf)__C, (__mmask32)-1,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_fmaddsub_ph(__m512h __A, __mmask32 __U, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddsubph512_mask(
      (__v32hf)__A, (__v32hf)__B, (__v32hf)__C, (__mmask32)__U,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask3_fmaddsub_ph(__m512h __A, __m512h __B, __m512h __C, __mmask32 __U) {
  return (__m512h)__builtin_ia32_vfmaddsubph512_mask3(
      (__v32hf)__A, (__v32hf)__B, (__v32hf)__C, (__mmask32)__U,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_fmaddsub_ph(__mmask32 __U, __m512h __A, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddsubph512_maskz(
      (__v32hf)__A, (__v32hf)__B, (__v32hf)__C, (__mmask32)__U,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_fmsubadd_ph(__m512h __A, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddsubph512_mask(
      (__v32hf)__A, (__v32hf)__B, -(__v32hf)__C, (__mmask32)-1,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_fmsubadd_ph(__m512h __A, __mmask32 __U, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddsubph512_mask(
      (__v32hf)__A, (__v32hf)__B, -(__v32hf)__C, (__mmask32)__U,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_fmsubadd_ph(__mmask32 __U, __m512h __A, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddsubph512_maskz(
      (__v32hf)__A, (__v32hf)__B, -(__v32hf)__C, (__mmask32)__U,
      0x04);
}






static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask3_fmsub_ph(__m512h __A, __m512h __B, __m512h __C, __mmask32 __U) {
  return (__m512h)__builtin_ia32_vfmsubph512_mask3((__v32hf)__A, (__v32hf)__B,
                                                   (__v32hf)__C, (__mmask32)__U,
                                                   0x04);
}






static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask3_fmsubadd_ph(__m512h __A, __m512h __B, __m512h __C, __mmask32 __U) {
  return (__m512h)__builtin_ia32_vfmsubaddph512_mask3(
      (__v32hf)__A, (__v32hf)__B, (__v32hf)__C, (__mmask32)__U,
      0x04);
}






static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_fnmadd_ph(__m512h __A, __mmask32 __U, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddph512_mask((__v32hf)__A, -(__v32hf)__B,
                                                  (__v32hf)__C, (__mmask32)__U,
                                                  0x04);
}
# 2732 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_fnmsub_ph(__m512h __A, __mmask32 __U, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddph512_mask((__v32hf)__A, -(__v32hf)__B,
                                                  -(__v32hf)__C, (__mmask32)__U,
                                                  0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask3_fnmsub_ph(__m512h __A, __m512h __B, __m512h __C, __mmask32 __U) {
  return (__m512h)__builtin_ia32_vfmsubph512_mask3(-(__v32hf)__A, (__v32hf)__B,
                                                   (__v32hf)__C, (__mmask32)__U,
                                                   0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_fmadd_sh(__m128h __W,
                                                             __m128h __A,
                                                             __m128h __B) {
  return __builtin_ia32_vfmaddsh3_mask((__v8hf)__W, (__v8hf)__A, (__v8hf)__B,
                                       (__mmask8)-1, 0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_mask_fmadd_sh(__m128h __W,
                                                                  __mmask8 __U,
                                                                  __m128h __A,
                                                                  __m128h __B) {
  return __builtin_ia32_vfmaddsh3_mask((__v8hf)__W, (__v8hf)__A, (__v8hf)__B,
                                       (__mmask8)__U, 0x04);
}
# 2771 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_maskz_fmadd_sh(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return __builtin_ia32_vfmaddsh3_maskz((__v8hf)__A, (__v8hf)__B, (__v8hf)__C,
                                        (__mmask8)__U,
                                        0x04);
}






static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_mask3_fmadd_sh(__m128h __W, __m128h __X, __m128h __Y, __mmask8 __U) {
  return __builtin_ia32_vfmaddsh3_mask3((__v8hf)__W, (__v8hf)__X, (__v8hf)__Y,
                                        (__mmask8)__U,
                                        0x04);
}






static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_fmsub_sh(__m128h __W,
                                                             __m128h __A,
                                                             __m128h __B) {
  return (__m128h)__builtin_ia32_vfmaddsh3_mask((__v8hf)__W, (__v8hf)__A,
                                                -(__v8hf)__B, (__mmask8)-1,
                                                0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_mask_fmsub_sh(__m128h __W,
                                                                  __mmask8 __U,
                                                                  __m128h __A,
                                                                  __m128h __B) {
  return (__m128h)__builtin_ia32_vfmaddsh3_mask((__v8hf)__W, (__v8hf)__A,
                                                -(__v8hf)__B, (__mmask8)__U,
                                                0x04);
}
# 2822 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_maskz_fmsub_sh(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_vfmaddsh3_maskz((__v8hf)__A, (__v8hf)__B,
                                                 -(__v8hf)__C, (__mmask8)__U,
                                                 0x04);
}






static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_mask3_fmsub_sh(__m128h __W, __m128h __X, __m128h __Y, __mmask8 __U) {
  return __builtin_ia32_vfmsubsh3_mask3((__v8hf)__W, (__v8hf)__X, (__v8hf)__Y,
                                        (__mmask8)__U,
                                        0x04);
}






static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_fnmadd_sh(__m128h __W,
                                                              __m128h __A,
                                                              __m128h __B) {
  return __builtin_ia32_vfmaddsh3_mask((__v8hf)__W, -(__v8hf)__A, (__v8hf)__B,
                                       (__mmask8)-1, 0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_mask_fnmadd_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return __builtin_ia32_vfmaddsh3_mask((__v8hf)__W, -(__v8hf)__A, (__v8hf)__B,
                                       (__mmask8)__U, 0x04);
}
# 2869 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_maskz_fnmadd_sh(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return __builtin_ia32_vfmaddsh3_maskz((__v8hf)__A, -(__v8hf)__B, (__v8hf)__C,
                                        (__mmask8)__U,
                                        0x04);
}






static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_mask3_fnmadd_sh(__m128h __W, __m128h __X, __m128h __Y, __mmask8 __U) {
  return __builtin_ia32_vfmaddsh3_mask3((__v8hf)__W, -(__v8hf)__X, (__v8hf)__Y,
                                        (__mmask8)__U,
                                        0x04);
}






static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_fnmsub_sh(__m128h __W,
                                                              __m128h __A,
                                                              __m128h __B) {
  return __builtin_ia32_vfmaddsh3_mask((__v8hf)__W, -(__v8hf)__A, -(__v8hf)__B,
                                       (__mmask8)-1, 0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_mask_fnmsub_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return __builtin_ia32_vfmaddsh3_mask((__v8hf)__W, -(__v8hf)__A, -(__v8hf)__B,
                                       (__mmask8)__U, 0x04);
}
# 2916 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_maskz_fnmsub_sh(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return __builtin_ia32_vfmaddsh3_maskz((__v8hf)__A, -(__v8hf)__B, -(__v8hf)__C,
                                        (__mmask8)__U,
                                        0x04);
}






static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_mask3_fnmsub_sh(__m128h __W, __m128h __X, __m128h __Y, __mmask8 __U) {
  return __builtin_ia32_vfmsubsh3_mask3((__v8hf)__W, -(__v8hf)__X, (__v8hf)__Y,
                                        (__mmask8)__U,
                                        0x04);
}






static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_fcmadd_sch(__m128h __A,
                                                               __m128h __B,
                                                               __m128h __C) {
  return (__m128h)__builtin_ia32_vfcmaddcsh_mask((__v4sf)__A, (__v4sf)__B,
                                                 (__v4sf)__C, (__mmask8)-1,
                                                 0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_mask_fcmadd_sch(__m128h __A, __mmask8 __U, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_vfcmaddcsh_round_mask(
      (__v4sf)__A, (__v4sf)(__B), (__v4sf)(__C), __U, 0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_maskz_fcmadd_sch(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_vfcmaddcsh_maskz((__v4sf)__A, (__v4sf)__B,
                                                  (__v4sf)__C, (__mmask8)__U,
                                                  0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_mask3_fcmadd_sch(__m128h __A, __m128h __B, __m128h __C, __mmask8 __U) {
  return (__m128h)__builtin_ia32_vfcmaddcsh_round_mask3(
      (__v4sf)__A, (__v4sf)__B, (__v4sf)__C, __U, 0x04);
}
# 2987 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_fmadd_sch(__m128h __A,
                                                              __m128h __B,
                                                              __m128h __C) {
  return (__m128h)__builtin_ia32_vfmaddcsh_mask((__v4sf)__A, (__v4sf)__B,
                                                (__v4sf)__C, (__mmask8)-1,
                                                0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_mask_fmadd_sch(__m128h __A, __mmask8 __U, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_vfmaddcsh_round_mask(
      (__v4sf)__A, (__v4sf)(__B), (__v4sf)(__C), __U, 0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_maskz_fmadd_sch(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_vfmaddcsh_maskz((__v4sf)__A, (__v4sf)__B,
                                                 (__v4sf)__C, (__mmask8)__U,
                                                 0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_mask3_fmadd_sch(__m128h __A, __m128h __B, __m128h __C, __mmask8 __U) {
  return (__m128h)__builtin_ia32_vfmaddcsh_round_mask3(
      (__v4sf)__A, (__v4sf)__B, (__v4sf)__C, __U, 0x04);
}
# 3034 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_fcmul_sch(__m128h __A,
                                                              __m128h __B) {
  return (__m128h)__builtin_ia32_vfcmulcsh_mask(
      (__v4sf)__A, (__v4sf)__B, (__v4sf)_mm_undefined_ph(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_mask_fcmul_sch(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_vfcmulcsh_mask((__v4sf)__A, (__v4sf)__B,
                                                (__v4sf)__W, (__mmask8)__U,
                                                0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_maskz_fcmul_sch(__mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_vfcmulcsh_mask(
      (__v4sf)__A, (__v4sf)__B, (__v4sf)_mm_setzero_ph(), (__mmask8)__U,
      0x04);
}
# 3070 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_fmul_sch(__m128h __A,
                                                             __m128h __B) {
  return (__m128h)__builtin_ia32_vfmulcsh_mask(
      (__v4sf)__A, (__v4sf)__B, (__v4sf)_mm_undefined_ph(), (__mmask8)-1,
      0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128))) _mm_mask_fmul_sch(__m128h __W,
                                                                  __mmask8 __U,
                                                                  __m128h __A,
                                                                  __m128h __B) {
  return (__m128h)__builtin_ia32_vfmulcsh_mask((__v4sf)__A, (__v4sf)__B,
                                               (__v4sf)__W, (__mmask8)__U,
                                               0x04);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_maskz_fmul_sch(__mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_vfmulcsh_mask(
      (__v4sf)__A, (__v4sf)__B, (__v4sf)_mm_setzero_ph(), (__mmask8)__U,
      0x04);
}
# 3108 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_fcmul_pch(__m512h __A,
                                                                 __m512h __B) {
  return (__m512h)__builtin_ia32_vfcmulcph512_mask(
      (__v16sf)__A, (__v16sf)__B, (__v16sf)_mm512_undefined_ph(), (__mmask16)-1,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_fcmul_pch(__m512h __W, __mmask16 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_vfcmulcph512_mask((__v16sf)__A, (__v16sf)__B,
                                                   (__v16sf)__W, (__mmask16)__U,
                                                   0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_fcmul_pch(__mmask16 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_vfcmulcph512_mask(
      (__v16sf)__A, (__v16sf)__B, (__v16sf)_mm512_setzero_ph(), (__mmask16)__U,
      0x04);
}
# 3144 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_fmul_pch(__m512h __A,
                                                                __m512h __B) {
  return (__m512h)__builtin_ia32_vfmulcph512_mask(
      (__v16sf)__A, (__v16sf)__B, (__v16sf)_mm512_undefined_ph(), (__mmask16)-1,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_fmul_pch(__m512h __W, __mmask16 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_vfmulcph512_mask((__v16sf)__A, (__v16sf)__B,
                                                  (__v16sf)__W, (__mmask16)__U,
                                                  0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_fmul_pch(__mmask16 __U, __m512h __A, __m512h __B) {
  return (__m512h)__builtin_ia32_vfmulcph512_mask(
      (__v16sf)__A, (__v16sf)__B, (__v16sf)_mm512_setzero_ph(), (__mmask16)__U,
      0x04);
}
# 3180 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_fcmadd_pch(__m512h __A,
                                                                  __m512h __B,
                                                                  __m512h __C) {
  return (__m512h)__builtin_ia32_vfcmaddcph512_mask3(
      (__v16sf)__A, (__v16sf)__B, (__v16sf)__C, (__mmask16)-1,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_fcmadd_pch(__m512h __A, __mmask16 __U, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfcmaddcph512_mask(
      (__v16sf)__A, (__v16sf)__B, (__v16sf)__C, (__mmask16)__U,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask3_fcmadd_pch(__m512h __A, __m512h __B, __m512h __C, __mmask16 __U) {
  return (__m512h)__builtin_ia32_vfcmaddcph512_mask3(
      (__v16sf)__A, (__v16sf)__B, (__v16sf)__C, (__mmask16)__U,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_fcmadd_pch(__mmask16 __U, __m512h __A, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfcmaddcph512_maskz(
      (__v16sf)__A, (__v16sf)__B, (__v16sf)__C, (__mmask16)__U,
      0x04);
}
# 3229 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512))) _mm512_fmadd_pch(__m512h __A,
                                                                 __m512h __B,
                                                                 __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddcph512_mask3((__v16sf)__A, (__v16sf)__B,
                                                    (__v16sf)__C, (__mmask16)-1,
                                                    0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_fmadd_pch(__m512h __A, __mmask16 __U, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddcph512_mask((__v16sf)__A, (__v16sf)__B,
                                                   (__v16sf)__C, (__mmask16)__U,
                                                   0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask3_fmadd_pch(__m512h __A, __m512h __B, __m512h __C, __mmask16 __U) {
  return (__m512h)__builtin_ia32_vfmaddcph512_mask3(
      (__v16sf)__A, (__v16sf)__B, (__v16sf)__C, (__mmask16)__U,
      0x04);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_maskz_fmadd_pch(__mmask16 __U, __m512h __A, __m512h __B, __m512h __C) {
  return (__m512h)__builtin_ia32_vfmaddcph512_maskz(
      (__v16sf)__A, (__v16sf)__B, (__v16sf)__C, (__mmask16)__U,
      0x04);
}
# 3278 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512fp16intrin.h" 3
static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_reduce_add_ph(__m512h __W) {
  return __builtin_ia32_reduce_fadd_ph512(-0.0f16, __W);
}

static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_reduce_mul_ph(__m512h __W) {
  return __builtin_ia32_reduce_fmul_ph512(1.0f16, __W);
}

static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_reduce_max_ph(__m512h __V) {
  return __builtin_ia32_reduce_fmax_ph512(__V);
}

static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_reduce_min_ph(__m512h __V) {
  return __builtin_ia32_reduce_fmin_ph512(__V);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_blend_ph(__mmask32 __U, __m512h __A, __m512h __W) {
  return (__m512h)__builtin_ia32_selectph_512((__mmask32)__U, (__v32hf)__W,
                                              (__v32hf)__A);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_permutex2var_ph(__m512h __A, __m512i __I, __m512h __B) {
  return (__m512h)__builtin_ia32_vpermi2varhi512((__v32hi)__A, (__v32hi)__I,
                                                 (__v32hi)__B);
}

static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_permutexvar_ph(__m512i __A, __m512h __B) {
  return (__m512h)__builtin_ia32_permvarhi512((__v32hi)__B, (__v32hi)__A);
}
# 278 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3





# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlfp16intrin.h" 1 3
# 44 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlfp16intrin.h" 3
static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_cvtsh_h(__m128h __a) {
  return __a[0];
}

static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_cvtsh_h(__m256h __a) {
  return __a[0];
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_set_sh(_Float16 __h) {
  return __extension__(__m128h){__h, 0, 0, 0, 0, 0, 0, 0};
}

static __inline __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_set1_ph(_Float16 __h) {
  return (__m128h)(__v8hf){__h, __h, __h, __h, __h, __h, __h, __h};
}

static __inline __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_set1_ph(_Float16 __h) {
  return (__m256h)(__v16hf){__h, __h, __h, __h, __h, __h, __h, __h,
                            __h, __h, __h, __h, __h, __h, __h, __h};
}

static __inline __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_set_ph(_Float16 __h1, _Float16 __h2, _Float16 __h3, _Float16 __h4,
           _Float16 __h5, _Float16 __h6, _Float16 __h7, _Float16 __h8) {
  return (__m128h)(__v8hf){__h8, __h7, __h6, __h5, __h4, __h3, __h2, __h1};
}

static __inline __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_set1_pch(_Float16 _Complex h) {
  return (__m256h)_mm256_set1_ps(__builtin_bit_cast(float, h));
}

static __inline __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_set1_pch(_Float16 _Complex h) {
  return (__m128h)_mm_set1_ps(__builtin_bit_cast(float, h));
}

static __inline __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_set_ph(_Float16 __h1, _Float16 __h2, _Float16 __h3, _Float16 __h4,
              _Float16 __h5, _Float16 __h6, _Float16 __h7, _Float16 __h8,
              _Float16 __h9, _Float16 __h10, _Float16 __h11, _Float16 __h12,
              _Float16 __h13, _Float16 __h14, _Float16 __h15, _Float16 __h16) {
  return (__m256h)(__v16hf){__h16, __h15, __h14, __h13, __h12, __h11,
                            __h10, __h9, __h8, __h7, __h6, __h5,
                            __h4, __h3, __h2, __h1};
}
# 99 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlfp16intrin.h" 3
static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_add_ph(__m256h __A,
                                                              __m256h __B) {
  return (__m256h)((__v16hf)__A + (__v16hf)__B);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_add_ph(__m256h __W, __mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_selectph_256(
      __U, (__v16hf)_mm256_add_ph(__A, __B), (__v16hf)__W);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_add_ph(__mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_selectph_256(
      __U, (__v16hf)_mm256_add_ph(__A, __B), (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_add_ph(__m128h __A,
                                                           __m128h __B) {
  return (__m128h)((__v8hf)__A + (__v8hf)__B);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_mask_add_ph(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  return (__m128h)__builtin_ia32_selectph_128(__U, (__v8hf)_mm_add_ph(__A, __B),
                                              (__v8hf)__W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_maskz_add_ph(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  return (__m128h)__builtin_ia32_selectph_128(__U, (__v8hf)_mm_add_ph(__A, __B),
                                              (__v8hf)_mm_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_sub_ph(__m256h __A,
                                                              __m256h __B) {
  return (__m256h)((__v16hf)__A - (__v16hf)__B);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_sub_ph(__m256h __W, __mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_selectph_256(
      __U, (__v16hf)_mm256_sub_ph(__A, __B), (__v16hf)__W);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_sub_ph(__mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_selectph_256(
      __U, (__v16hf)_mm256_sub_ph(__A, __B), (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_sub_ph(__m128h __A,
                                                           __m128h __B) {
  return (__m128h)((__v8hf)__A - (__v8hf)__B);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_mask_sub_ph(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  return (__m128h)__builtin_ia32_selectph_128(__U, (__v8hf)_mm_sub_ph(__A, __B),
                                              (__v8hf)__W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_maskz_sub_ph(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  return (__m128h)__builtin_ia32_selectph_128(__U, (__v8hf)_mm_sub_ph(__A, __B),
                                              (__v8hf)_mm_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_mul_ph(__m256h __A,
                                                              __m256h __B) {
  return (__m256h)((__v16hf)__A * (__v16hf)__B);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_mul_ph(__m256h __W, __mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_selectph_256(
      __U, (__v16hf)_mm256_mul_ph(__A, __B), (__v16hf)__W);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_mul_ph(__mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_selectph_256(
      __U, (__v16hf)_mm256_mul_ph(__A, __B), (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_mul_ph(__m128h __A,
                                                           __m128h __B) {
  return (__m128h)((__v8hf)__A * (__v8hf)__B);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_mask_mul_ph(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  return (__m128h)__builtin_ia32_selectph_128(__U, (__v8hf)_mm_mul_ph(__A, __B),
                                              (__v8hf)__W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_maskz_mul_ph(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  return (__m128h)__builtin_ia32_selectph_128(__U, (__v8hf)_mm_mul_ph(__A, __B),
                                              (__v8hf)_mm_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_div_ph(__m256h __A,
                                                              __m256h __B) {
  return (__m256h)((__v16hf)__A / (__v16hf)__B);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_div_ph(__m256h __W, __mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_selectph_256(
      __U, (__v16hf)_mm256_div_ph(__A, __B), (__v16hf)__W);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_div_ph(__mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_selectph_256(
      __U, (__v16hf)_mm256_div_ph(__A, __B), (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_div_ph(__m128h __A,
                                                           __m128h __B) {
  return (__m128h)((__v8hf)__A / (__v8hf)__B);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_mask_div_ph(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  return (__m128h)__builtin_ia32_selectph_128(__U, (__v8hf)_mm_div_ph(__A, __B),
                                              (__v8hf)__W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_maskz_div_ph(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  return (__m128h)__builtin_ia32_selectph_128(__U, (__v8hf)_mm_div_ph(__A, __B),
                                              (__v8hf)_mm_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_min_ph(__m256h __A,
                                                              __m256h __B) {
  return (__m256h)__builtin_ia32_minph256((__v16hf)__A, (__v16hf)__B);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_min_ph(__m256h __W, __mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      (__v16hf)__builtin_ia32_minph256((__v16hf)__A, (__v16hf)__B),
      (__v16hf)__W);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_min_ph(__mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      (__v16hf)__builtin_ia32_minph256((__v16hf)__A, (__v16hf)__B),
      (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_min_ph(__m128h __A,
                                                           __m128h __B) {
  return (__m128h)__builtin_ia32_minph128((__v8hf)__A, (__v8hf)__B);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_mask_min_ph(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)__builtin_ia32_minph128((__v8hf)__A, (__v8hf)__B),
      (__v8hf)__W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_maskz_min_ph(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)__builtin_ia32_minph128((__v8hf)__A, (__v8hf)__B),
      (__v8hf)_mm_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_max_ph(__m256h __A,
                                                              __m256h __B) {
  return (__m256h)__builtin_ia32_maxph256((__v16hf)__A, (__v16hf)__B);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_max_ph(__m256h __W, __mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      (__v16hf)__builtin_ia32_maxph256((__v16hf)__A, (__v16hf)__B),
      (__v16hf)__W);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_max_ph(__mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      (__v16hf)__builtin_ia32_maxph256((__v16hf)__A, (__v16hf)__B),
      (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_max_ph(__m128h __A,
                                                           __m128h __B) {
  return (__m128h)__builtin_ia32_maxph128((__v8hf)__A, (__v8hf)__B);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_mask_max_ph(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A,
                                                                __m128h __B) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)__builtin_ia32_maxph128((__v8hf)__A, (__v8hf)__B),
      (__v8hf)__W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_maskz_max_ph(__mmask8 __U,
                                                                 __m128h __A,
                                                                 __m128h __B) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)__builtin_ia32_maxph128((__v8hf)__A, (__v8hf)__B),
      (__v8hf)_mm_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_abs_ph(__m256h __A) {
  return (__m256h)_mm256_and_epi32(_mm256_set1_epi32(0x7FFF7FFF), (__m256i)__A);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_abs_ph(__m128h __A) {
  return (__m128h)_mm_and_epi32(_mm_set1_epi32(0x7FFF7FFF), (__m128i)__A);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_conj_pch(__m256h __A) {
  return (__m256h)_mm256_xor_ps((__m256)__A, _mm256_set1_ps(-0.0f));
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_conj_pch(__m256h __W, __mmask8 __U, __m256h __A) {
  return (__m256h)__builtin_ia32_selectps_256(
      (__mmask8)__U, (__v8sf)_mm256_conj_pch(__A), (__v8sf)__W);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_conj_pch(__mmask8 __U, __m256h __A) {
  return (__m256h)__builtin_ia32_selectps_256(
      (__mmask8)__U, (__v8sf)_mm256_conj_pch(__A), (__v8sf)_mm256_setzero_ps());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_conj_pch(__m128h __A) {
  return (__m128h)_mm_xor_ps((__m128)__A, _mm_set1_ps(-0.0f));
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_mask_conj_pch(__m128h __W,
                                                                  __mmask8 __U,
                                                                  __m128h __A) {
  return (__m128h)__builtin_ia32_selectps_128(
      (__mmask8)__U, (__v4sf)_mm_conj_pch(__A), (__v4sf)__W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_conj_pch(__mmask8 __U, __m128h __A) {
  return (__m128h)__builtin_ia32_selectps_128(
      (__mmask8)__U, (__v4sf)_mm_conj_pch(__A), (__v4sf)_mm_setzero_ps());
}
# 390 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlfp16intrin.h" 3
static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_rcp_ph(__m256h __A) {
  return (__m256h)__builtin_ia32_rcpph256_mask(
      (__v16hf)__A, (__v16hf)_mm256_undefined_ph(), (__mmask16)-1);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_rcp_ph(__m256h __W, __mmask16 __U, __m256h __A) {
  return (__m256h)__builtin_ia32_rcpph256_mask((__v16hf)__A, (__v16hf)__W,
                                               (__mmask16)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_rcp_ph(__mmask16 __U, __m256h __A) {
  return (__m256h)__builtin_ia32_rcpph256_mask(
      (__v16hf)__A, (__v16hf)_mm256_setzero_ph(), (__mmask16)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_rcp_ph(__m128h __A) {
  return (__m128h)__builtin_ia32_rcpph128_mask(
      (__v8hf)__A, (__v8hf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_mask_rcp_ph(__m128h __W,
                                                                __mmask8 __U,
                                                                __m128h __A) {
  return (__m128h)__builtin_ia32_rcpph128_mask((__v8hf)__A, (__v8hf)__W,
                                               (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_maskz_rcp_ph(__mmask8 __U,
                                                                 __m128h __A) {
  return (__m128h)__builtin_ia32_rcpph128_mask(
      (__v8hf)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_rsqrt_ph(__m256h __A) {
  return (__m256h)__builtin_ia32_rsqrtph256_mask(
      (__v16hf)__A, (__v16hf)_mm256_undefined_ph(), (__mmask16)-1);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_rsqrt_ph(__m256h __W, __mmask16 __U, __m256h __A) {
  return (__m256h)__builtin_ia32_rsqrtph256_mask((__v16hf)__A, (__v16hf)__W,
                                                 (__mmask16)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_rsqrt_ph(__mmask16 __U, __m256h __A) {
  return (__m256h)__builtin_ia32_rsqrtph256_mask(
      (__v16hf)__A, (__v16hf)_mm256_setzero_ph(), (__mmask16)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_rsqrt_ph(__m128h __A) {
  return (__m128h)__builtin_ia32_rsqrtph128_mask(
      (__v8hf)__A, (__v8hf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_mask_rsqrt_ph(__m128h __W,
                                                                  __mmask8 __U,
                                                                  __m128h __A) {
  return (__m128h)__builtin_ia32_rsqrtph128_mask((__v8hf)__A, (__v8hf)__W,
                                                 (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_rsqrt_ph(__mmask8 __U, __m128h __A) {
  return (__m128h)__builtin_ia32_rsqrtph128_mask(
      (__v8hf)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_getexp_ph(__m128h __A) {
  return (__m128h)__builtin_ia32_getexpph128_mask(
      (__v8hf)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_getexp_ph(__m128h __W, __mmask8 __U, __m128h __A) {
  return (__m128h)__builtin_ia32_getexpph128_mask((__v8hf)__A, (__v8hf)__W,
                                                  (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_getexp_ph(__mmask8 __U, __m128h __A) {
  return (__m128h)__builtin_ia32_getexpph128_mask(
      (__v8hf)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_getexp_ph(__m256h __A) {
  return (__m256h)__builtin_ia32_getexpph256_mask(
      (__v16hf)__A, (__v16hf)_mm256_setzero_ph(), (__mmask16)-1);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_getexp_ph(__m256h __W, __mmask16 __U, __m256h __A) {
  return (__m256h)__builtin_ia32_getexpph256_mask((__v16hf)__A, (__v16hf)__W,
                                                  (__mmask16)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_getexp_ph(__mmask16 __U, __m256h __A) {
  return (__m256h)__builtin_ia32_getexpph256_mask(
      (__v16hf)__A, (__v16hf)_mm256_setzero_ph(), (__mmask16)__U);
}
# 524 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlfp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_scalef_ph(__m128h __A,
                                                              __m128h __B) {
  return (__m128h)__builtin_ia32_scalefph128_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_scalef_ph(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_scalefph128_mask((__v8hf)__A, (__v8hf)__B,
                                                  (__v8hf)__W, (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_scalef_ph(__mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_scalefph128_mask(
      (__v8hf)__A, (__v8hf)__B, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_scalef_ph(__m256h __A,
                                                                 __m256h __B) {
  return (__m256h)__builtin_ia32_scalefph256_mask(
      (__v16hf)__A, (__v16hf)__B, (__v16hf)_mm256_setzero_ph(), (__mmask16)-1);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_scalef_ph(__m256h __W, __mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_scalefph256_mask((__v16hf)__A, (__v16hf)__B,
                                                  (__v16hf)__W, (__mmask16)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_scalef_ph(__mmask16 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_scalefph256_mask(
      (__v16hf)__A, (__v16hf)__B, (__v16hf)_mm256_setzero_ph(), (__mmask16)__U);
}
# 618 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlfp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_sqrt_ph(__m128h __a) {
  return __builtin_ia32_sqrtph((__v8hf)__a);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_mask_sqrt_ph(__m128h __W,
                                                                 __mmask8 __U,
                                                                 __m128h __A) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)_mm_sqrt_ph(__A), (__v8hf)__W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_maskz_sqrt_ph(__mmask8 __U,
                                                                  __m128h __A) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)_mm_sqrt_ph(__A), (__v8hf)_mm_setzero_ph());
}

static __inline __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_sqrt_ph(__m256h __a) {
  return (__m256h)__builtin_ia32_sqrtph256((__v16hf)__a);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_sqrt_ph(__m256h __W, __mmask16 __U, __m256h __A) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U, (__v16hf)_mm256_sqrt_ph(__A), (__v16hf)__W);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_sqrt_ph(__mmask16 __U, __m256h __A) {
  return (__m256h)__builtin_ia32_selectph_256((__mmask16)__U,
                                              (__v16hf)_mm256_sqrt_ph(__A),
                                              (__v16hf)_mm256_setzero_ph());
}
# 668 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlfp16intrin.h" 3
static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_cvtpd_ph(__m128d __A) {
  return (__m128h)__builtin_ia32_vcvtpd2ph128_mask(
      (__v2df)__A, (__v8hf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_mask_cvtpd_ph(__m128h __W,
                                                                  __mmask8 __U,
                                                                  __m128d __A) {
  return (__m128h)__builtin_ia32_vcvtpd2ph128_mask((__v2df)__A, (__v8hf)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtpd_ph(__mmask8 __U, __m128d __A) {
  return (__m128h)__builtin_ia32_vcvtpd2ph128_mask(
      (__v2df)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_cvtpd_ph(__m256d __A) {
  return (__m128h)__builtin_ia32_vcvtpd2ph256_mask(
      (__v4df)__A, (__v8hf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtpd_ph(__m128h __W, __mmask8 __U, __m256d __A) {
  return (__m128h)__builtin_ia32_vcvtpd2ph256_mask((__v4df)__A, (__v8hf)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtpd_ph(__mmask8 __U, __m256d __A) {
  return (__m128h)__builtin_ia32_vcvtpd2ph256_mask(
      (__v4df)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_cvtph_pd(__m128h __A) {
  return (__m128d)__builtin_ia32_vcvtph2pd128_mask(
      (__v8hf)__A, (__v2df)_mm_undefined_pd(), (__mmask8)-1);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_mask_cvtph_pd(__m128d __W,
                                                                  __mmask8 __U,
                                                                  __m128h __A) {
  return (__m128d)__builtin_ia32_vcvtph2pd128_mask((__v8hf)__A, (__v2df)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtph_pd(__mmask8 __U, __m128h __A) {
  return (__m128d)__builtin_ia32_vcvtph2pd128_mask(
      (__v8hf)__A, (__v2df)_mm_setzero_pd(), (__mmask8)__U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_cvtph_pd(__m128h __A) {
  return (__m256d)__builtin_ia32_vcvtph2pd256_mask(
      (__v8hf)__A, (__v4df)_mm256_undefined_pd(), (__mmask8)-1);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtph_pd(__m256d __W, __mmask8 __U, __m128h __A) {
  return (__m256d)__builtin_ia32_vcvtph2pd256_mask((__v8hf)__A, (__v4df)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtph_pd(__mmask8 __U, __m128h __A) {
  return (__m256d)__builtin_ia32_vcvtph2pd256_mask(
      (__v8hf)__A, (__v4df)_mm256_setzero_pd(), (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_cvtph_epi16(__m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2w128_mask(
      (__v8hf)__A, (__v8hi)_mm_undefined_si128(), (__mmask8)-1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtph_epi16(__m128i __W, __mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2w128_mask((__v8hf)__A, (__v8hi)__W,
                                                  (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtph_epi16(__mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2w128_mask(
      (__v8hf)__A, (__v8hi)_mm_setzero_si128(), (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_cvtph_epi16(__m256h __A) {
  return (__m256i)__builtin_ia32_vcvtph2w256_mask(
      (__v16hf)__A, (__v16hi)_mm256_undefined_si256(), (__mmask16)-1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtph_epi16(__m256i __W, __mmask16 __U, __m256h __A) {
  return (__m256i)__builtin_ia32_vcvtph2w256_mask((__v16hf)__A, (__v16hi)__W,
                                                  (__mmask16)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtph_epi16(__mmask16 __U, __m256h __A) {
  return (__m256i)__builtin_ia32_vcvtph2w256_mask(
      (__v16hf)__A, (__v16hi)_mm256_setzero_si256(), (__mmask16)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_cvttph_epi16(__m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2w128_mask(
      (__v8hf)__A, (__v8hi)_mm_undefined_si128(), (__mmask8)-1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_cvttph_epi16(__m128i __W, __mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2w128_mask((__v8hf)__A, (__v8hi)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvttph_epi16(__mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2w128_mask(
      (__v8hf)__A, (__v8hi)_mm_setzero_si128(), (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_cvttph_epi16(__m256h __A) {
  return (__m256i)__builtin_ia32_vcvttph2w256_mask(
      (__v16hf)__A, (__v16hi)_mm256_undefined_si256(), (__mmask16)-1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvttph_epi16(__m256i __W, __mmask16 __U, __m256h __A) {
  return (__m256i)__builtin_ia32_vcvttph2w256_mask((__v16hf)__A, (__v16hi)__W,
                                                   (__mmask16)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvttph_epi16(__mmask16 __U, __m256h __A) {
  return (__m256i)__builtin_ia32_vcvttph2w256_mask(
      (__v16hf)__A, (__v16hi)_mm256_setzero_si256(), (__mmask16)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_cvtepi16_ph(__m128i __A) {
  return (__m128h) __builtin_convertvector((__v8hi)__A, __v8hf);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtepi16_ph(__m128h __W, __mmask8 __U, __m128i __A) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)_mm_cvtepi16_ph(__A), (__v8hf)__W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtepi16_ph(__mmask8 __U, __m128i __A) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)_mm_cvtepi16_ph(__A), (__v8hf)_mm_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_cvtepi16_ph(__m256i __A) {
  return (__m256h) __builtin_convertvector((__v16hi)__A, __v16hf);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtepi16_ph(__m256h __W, __mmask16 __U, __m256i __A) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U, (__v16hf)_mm256_cvtepi16_ph(__A), (__v16hf)__W);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtepi16_ph(__mmask16 __U, __m256i __A) {
  return (__m256h)__builtin_ia32_selectph_256((__mmask16)__U,
                                              (__v16hf)_mm256_cvtepi16_ph(__A),
                                              (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_cvtph_epu16(__m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2uw128_mask(
      (__v8hf)__A, (__v8hu)_mm_undefined_si128(), (__mmask8)-1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtph_epu16(__m128i __W, __mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2uw128_mask((__v8hf)__A, (__v8hu)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtph_epu16(__mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2uw128_mask(
      (__v8hf)__A, (__v8hu)_mm_setzero_si128(), (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_cvtph_epu16(__m256h __A) {
  return (__m256i)__builtin_ia32_vcvtph2uw256_mask(
      (__v16hf)__A, (__v16hu)_mm256_undefined_si256(), (__mmask16)-1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtph_epu16(__m256i __W, __mmask16 __U, __m256h __A) {
  return (__m256i)__builtin_ia32_vcvtph2uw256_mask((__v16hf)__A, (__v16hu)__W,
                                                   (__mmask16)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtph_epu16(__mmask16 __U, __m256h __A) {
  return (__m256i)__builtin_ia32_vcvtph2uw256_mask(
      (__v16hf)__A, (__v16hu)_mm256_setzero_si256(), (__mmask16)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_cvttph_epu16(__m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2uw128_mask(
      (__v8hf)__A, (__v8hu)_mm_undefined_si128(), (__mmask8)-1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_cvttph_epu16(__m128i __W, __mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2uw128_mask((__v8hf)__A, (__v8hu)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvttph_epu16(__mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2uw128_mask(
      (__v8hf)__A, (__v8hu)_mm_setzero_si128(), (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_cvttph_epu16(__m256h __A) {
  return (__m256i)__builtin_ia32_vcvttph2uw256_mask(
      (__v16hf)__A, (__v16hu)_mm256_undefined_si256(), (__mmask16)-1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvttph_epu16(__m256i __W, __mmask16 __U, __m256h __A) {
  return (__m256i)__builtin_ia32_vcvttph2uw256_mask((__v16hf)__A, (__v16hu)__W,
                                                    (__mmask16)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvttph_epu16(__mmask16 __U, __m256h __A) {
  return (__m256i)__builtin_ia32_vcvttph2uw256_mask(
      (__v16hf)__A, (__v16hu)_mm256_setzero_si256(), (__mmask16)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_cvtepu16_ph(__m128i __A) {
  return (__m128h) __builtin_convertvector((__v8hu)__A, __v8hf);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtepu16_ph(__m128h __W, __mmask8 __U, __m128i __A) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)_mm_cvtepu16_ph(__A), (__v8hf)__W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtepu16_ph(__mmask8 __U, __m128i __A) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)_mm_cvtepu16_ph(__A), (__v8hf)_mm_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_cvtepu16_ph(__m256i __A) {
  return (__m256h) __builtin_convertvector((__v16hu)__A, __v16hf);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtepu16_ph(__m256h __W, __mmask16 __U, __m256i __A) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U, (__v16hf)_mm256_cvtepu16_ph(__A), (__v16hf)__W);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtepu16_ph(__mmask16 __U, __m256i __A) {
  return (__m256h)__builtin_ia32_selectph_256((__mmask16)__U,
                                              (__v16hf)_mm256_cvtepu16_ph(__A),
                                              (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_cvtph_epi32(__m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2dq128_mask(
      (__v8hf)__A, (__v4si)_mm_undefined_si128(), (__mmask8)-1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtph_epi32(__m128i __W, __mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2dq128_mask((__v8hf)__A, (__v4si)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtph_epi32(__mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2dq128_mask(
      (__v8hf)__A, (__v4si)_mm_setzero_si128(), (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_cvtph_epi32(__m128h __A) {
  return (__m256i)__builtin_ia32_vcvtph2dq256_mask(
      (__v8hf)__A, (__v8si)_mm256_undefined_si256(), (__mmask8)-1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtph_epi32(__m256i __W, __mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvtph2dq256_mask((__v8hf)__A, (__v8si)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtph_epi32(__mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvtph2dq256_mask(
      (__v8hf)__A, (__v8si)_mm256_setzero_si256(), (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_cvtph_epu32(__m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2udq128_mask(
      (__v8hf)__A, (__v4su)_mm_undefined_si128(), (__mmask8)-1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtph_epu32(__m128i __W, __mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2udq128_mask((__v8hf)__A, (__v4su)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtph_epu32(__mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2udq128_mask(
      (__v8hf)__A, (__v4su)_mm_setzero_si128(), (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_cvtph_epu32(__m128h __A) {
  return (__m256i)__builtin_ia32_vcvtph2udq256_mask(
      (__v8hf)__A, (__v8su)_mm256_undefined_si256(), (__mmask8)-1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtph_epu32(__m256i __W, __mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvtph2udq256_mask((__v8hf)__A, (__v8su)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtph_epu32(__mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvtph2udq256_mask(
      (__v8hf)__A, (__v8su)_mm256_setzero_si256(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_cvtepi32_ph(__m128i __A) {
  return (__m128h)__builtin_ia32_vcvtdq2ph128_mask(
      (__v4si)__A, (__v8hf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtepi32_ph(__m128h __W, __mmask8 __U, __m128i __A) {
  return (__m128h)__builtin_ia32_vcvtdq2ph128_mask((__v4si)__A, (__v8hf)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtepi32_ph(__mmask8 __U, __m128i __A) {
  return (__m128h)__builtin_ia32_vcvtdq2ph128_mask(
      (__v4si)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_cvtepi32_ph(__m256i __A) {
  return (__m128h) __builtin_convertvector((__v8si)__A, __v8hf);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtepi32_ph(__m128h __W, __mmask8 __U, __m256i __A) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)_mm256_cvtepi32_ph(__A), (__v8hf)__W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtepi32_ph(__mmask8 __U, __m256i __A) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)_mm256_cvtepi32_ph(__A), (__v8hf)_mm_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_cvtepu32_ph(__m128i __A) {
  return (__m128h)__builtin_ia32_vcvtudq2ph128_mask(
      (__v4su)__A, (__v8hf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtepu32_ph(__m128h __W, __mmask8 __U, __m128i __A) {
  return (__m128h)__builtin_ia32_vcvtudq2ph128_mask((__v4su)__A, (__v8hf)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtepu32_ph(__mmask8 __U, __m128i __A) {
  return (__m128h)__builtin_ia32_vcvtudq2ph128_mask(
      (__v4su)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_cvtepu32_ph(__m256i __A) {
  return (__m128h) __builtin_convertvector((__v8su)__A, __v8hf);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtepu32_ph(__m128h __W, __mmask8 __U, __m256i __A) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)_mm256_cvtepu32_ph(__A), (__v8hf)__W);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtepu32_ph(__mmask8 __U, __m256i __A) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, (__v8hf)_mm256_cvtepu32_ph(__A), (__v8hf)_mm_setzero_ph());
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_cvttph_epi32(__m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2dq128_mask(
      (__v8hf)__A, (__v4si)_mm_undefined_si128(), (__mmask8)-1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_cvttph_epi32(__m128i __W, __mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2dq128_mask((__v8hf)__A, (__v4si)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvttph_epi32(__mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2dq128_mask(
      (__v8hf)__A, (__v4si)_mm_setzero_si128(), (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_cvttph_epi32(__m128h __A) {
  return (__m256i)__builtin_ia32_vcvttph2dq256_mask(
      (__v8hf)__A, (__v8si)_mm256_undefined_si256(), (__mmask8)-1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvttph_epi32(__m256i __W, __mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvttph2dq256_mask((__v8hf)__A, (__v8si)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvttph_epi32(__mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvttph2dq256_mask(
      (__v8hf)__A, (__v8si)_mm256_setzero_si256(), (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_cvttph_epu32(__m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2udq128_mask(
      (__v8hf)__A, (__v4su)_mm_undefined_si128(), (__mmask8)-1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_cvttph_epu32(__m128i __W, __mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2udq128_mask((__v8hf)__A, (__v4su)__W,
                                                     (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvttph_epu32(__mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2udq128_mask(
      (__v8hf)__A, (__v4su)_mm_setzero_si128(), (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_cvttph_epu32(__m128h __A) {
  return (__m256i)__builtin_ia32_vcvttph2udq256_mask(
      (__v8hf)__A, (__v8su)_mm256_undefined_si256(), (__mmask8)-1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvttph_epu32(__m256i __W, __mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvttph2udq256_mask((__v8hf)__A, (__v8su)__W,
                                                     (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvttph_epu32(__mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvttph2udq256_mask(
      (__v8hf)__A, (__v8su)_mm256_setzero_si256(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_cvtepi64_ph(__m128i __A) {
  return (__m128h)__builtin_ia32_vcvtqq2ph128_mask(
      (__v2di)__A, (__v8hf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtepi64_ph(__m128h __W, __mmask8 __U, __m128i __A) {
  return (__m128h)__builtin_ia32_vcvtqq2ph128_mask((__v2di)__A, (__v8hf)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtepi64_ph(__mmask8 __U, __m128i __A) {
  return (__m128h)__builtin_ia32_vcvtqq2ph128_mask(
      (__v2di)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_cvtepi64_ph(__m256i __A) {
  return (__m128h)__builtin_ia32_vcvtqq2ph256_mask(
      (__v4di)__A, (__v8hf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtepi64_ph(__m128h __W, __mmask8 __U, __m256i __A) {
  return (__m128h)__builtin_ia32_vcvtqq2ph256_mask((__v4di)__A, (__v8hf)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtepi64_ph(__mmask8 __U, __m256i __A) {
  return (__m128h)__builtin_ia32_vcvtqq2ph256_mask(
      (__v4di)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_cvtph_epi64(__m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2qq128_mask(
      (__v8hf)__A, (__v2di)_mm_undefined_si128(), (__mmask8)-1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtph_epi64(__m128i __W, __mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2qq128_mask((__v8hf)__A, (__v2di)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtph_epi64(__mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2qq128_mask(
      (__v8hf)__A, (__v2di)_mm_setzero_si128(), (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_cvtph_epi64(__m128h __A) {
  return (__m256i)__builtin_ia32_vcvtph2qq256_mask(
      (__v8hf)__A, (__v4di)_mm256_undefined_si256(), (__mmask8)-1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtph_epi64(__m256i __W, __mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvtph2qq256_mask((__v8hf)__A, (__v4di)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtph_epi64(__mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvtph2qq256_mask(
      (__v8hf)__A, (__v4di)_mm256_setzero_si256(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_cvtepu64_ph(__m128i __A) {
  return (__m128h)__builtin_ia32_vcvtuqq2ph128_mask(
      (__v2du)__A, (__v8hf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtepu64_ph(__m128h __W, __mmask8 __U, __m128i __A) {
  return (__m128h)__builtin_ia32_vcvtuqq2ph128_mask((__v2du)__A, (__v8hf)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtepu64_ph(__mmask8 __U, __m128i __A) {
  return (__m128h)__builtin_ia32_vcvtuqq2ph128_mask(
      (__v2du)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_cvtepu64_ph(__m256i __A) {
  return (__m128h)__builtin_ia32_vcvtuqq2ph256_mask(
      (__v4du)__A, (__v8hf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtepu64_ph(__m128h __W, __mmask8 __U, __m256i __A) {
  return (__m128h)__builtin_ia32_vcvtuqq2ph256_mask((__v4du)__A, (__v8hf)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtepu64_ph(__mmask8 __U, __m256i __A) {
  return (__m128h)__builtin_ia32_vcvtuqq2ph256_mask(
      (__v4du)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_cvtph_epu64(__m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2uqq128_mask(
      (__v8hf)__A, (__v2du)_mm_undefined_si128(), (__mmask8)-1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_cvtph_epu64(__m128i __W, __mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2uqq128_mask((__v8hf)__A, (__v2du)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtph_epu64(__mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvtph2uqq128_mask(
      (__v8hf)__A, (__v2du)_mm_setzero_si128(), (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_cvtph_epu64(__m128h __A) {
  return (__m256i)__builtin_ia32_vcvtph2uqq256_mask(
      (__v8hf)__A, (__v4du)_mm256_undefined_si256(), (__mmask8)-1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtph_epu64(__m256i __W, __mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvtph2uqq256_mask((__v8hf)__A, (__v4du)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtph_epu64(__mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvtph2uqq256_mask(
      (__v8hf)__A, (__v4du)_mm256_setzero_si256(), (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_cvttph_epi64(__m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2qq128_mask(
      (__v8hf)__A, (__v2di)_mm_undefined_si128(), (__mmask8)-1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_cvttph_epi64(__m128i __W, __mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2qq128_mask((__v8hf)__A, (__v2di)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvttph_epi64(__mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2qq128_mask(
      (__v8hf)__A, (__v2di)_mm_setzero_si128(), (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_cvttph_epi64(__m128h __A) {
  return (__m256i)__builtin_ia32_vcvttph2qq256_mask(
      (__v8hf)__A, (__v4di)_mm256_undefined_si256(), (__mmask8)-1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvttph_epi64(__m256i __W, __mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvttph2qq256_mask((__v8hf)__A, (__v4di)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvttph_epi64(__mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvttph2qq256_mask(
      (__v8hf)__A, (__v4di)_mm256_setzero_si256(), (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_cvttph_epu64(__m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2uqq128_mask(
      (__v8hf)__A, (__v2du)_mm_undefined_si128(), (__mmask8)-1);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_cvttph_epu64(__m128i __W, __mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2uqq128_mask((__v8hf)__A, (__v2du)__W,
                                                     (__mmask8)__U);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvttph_epu64(__mmask8 __U, __m128h __A) {
  return (__m128i)__builtin_ia32_vcvttph2uqq128_mask(
      (__v8hf)__A, (__v2du)_mm_setzero_si128(), (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_cvttph_epu64(__m128h __A) {
  return (__m256i)__builtin_ia32_vcvttph2uqq256_mask(
      (__v8hf)__A, (__v4du)_mm256_undefined_si256(), (__mmask8)-1);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvttph_epu64(__m256i __W, __mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvttph2uqq256_mask((__v8hf)__A, (__v4du)__W,
                                                     (__mmask8)__U);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvttph_epu64(__mmask8 __U, __m128h __A) {
  return (__m256i)__builtin_ia32_vcvttph2uqq256_mask(
      (__v8hf)__A, (__v4du)_mm256_setzero_si256(), (__mmask8)__U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_cvtxph_ps(__m128h __A) {
  return (__m128)__builtin_ia32_vcvtph2psx128_mask(
      (__v8hf)__A, (__v4sf)_mm_undefined_ps(), (__mmask8)-1);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_mask_cvtxph_ps(__m128 __W,
                                                                  __mmask8 __U,
                                                                  __m128h __A) {
  return (__m128)__builtin_ia32_vcvtph2psx128_mask((__v8hf)__A, (__v4sf)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtxph_ps(__mmask8 __U, __m128h __A) {
  return (__m128)__builtin_ia32_vcvtph2psx128_mask(
      (__v8hf)__A, (__v4sf)_mm_setzero_ps(), (__mmask8)__U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_cvtxph_ps(__m128h __A) {
  return (__m256)__builtin_ia32_vcvtph2psx256_mask(
      (__v8hf)__A, (__v8sf)_mm256_undefined_ps(), (__mmask8)-1);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtxph_ps(__m256 __W, __mmask8 __U, __m128h __A) {
  return (__m256)__builtin_ia32_vcvtph2psx256_mask((__v8hf)__A, (__v8sf)__W,
                                                   (__mmask8)__U);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtxph_ps(__mmask8 __U, __m128h __A) {
  return (__m256)__builtin_ia32_vcvtph2psx256_mask(
      (__v8hf)__A, (__v8sf)_mm256_setzero_ps(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_cvtxps_ph(__m128 __A) {
  return (__m128h)__builtin_ia32_vcvtps2phx128_mask(
      (__v4sf)__A, (__v8hf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_mask_cvtxps_ph(__m128h __W,
                                                                   __mmask8 __U,
                                                                   __m128 __A) {
  return (__m128h)__builtin_ia32_vcvtps2phx128_mask((__v4sf)__A, (__v8hf)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_cvtxps_ph(__mmask8 __U, __m128 __A) {
  return (__m128h)__builtin_ia32_vcvtps2phx128_mask(
      (__v4sf)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_cvtxps_ph(__m256 __A) {
  return (__m128h)__builtin_ia32_vcvtps2phx256_mask(
      (__v8sf)__A, (__v8hf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_cvtxps_ph(__m128h __W, __mmask8 __U, __m256 __A) {
  return (__m128h)__builtin_ia32_vcvtps2phx256_mask((__v8sf)__A, (__v8hf)__W,
                                                    (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_cvtxps_ph(__mmask8 __U, __m256 __A) {
  return (__m128h)__builtin_ia32_vcvtps2phx256_mask(
      (__v8sf)__A, (__v8hf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_fmadd_ph(__m128h __A,
                                                             __m128h __B,
                                                             __m128h __C) {
  return (__m128h)__builtin_ia32_vfmaddph((__v8hf)__A, (__v8hf)__B,
                                          (__v8hf)__C);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_mask_fmadd_ph(__m128h __A,
                                                                  __mmask8 __U,
                                                                  __m128h __B,
                                                                  __m128h __C) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddph((__v8hf)__A, (__v8hf)__B, (__v8hf)__C),
      (__v8hf)__A);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask3_fmadd_ph(__m128h __A, __m128h __B, __m128h __C, __mmask8 __U) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddph((__v8hf)__A, (__v8hf)__B, (__v8hf)__C),
      (__v8hf)__C);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_fmadd_ph(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddph((__v8hf)__A, (__v8hf)__B, (__v8hf)__C),
      (__v8hf)_mm_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_fmsub_ph(__m128h __A,
                                                             __m128h __B,
                                                             __m128h __C) {
  return (__m128h)__builtin_ia32_vfmaddph((__v8hf)__A, (__v8hf)__B,
                                          -(__v8hf)__C);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_mask_fmsub_ph(__m128h __A,
                                                                  __mmask8 __U,
                                                                  __m128h __B,
                                                                  __m128h __C) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, _mm_fmsub_ph((__v8hf)__A, (__v8hf)__B, (__v8hf)__C),
      (__v8hf)__A);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_fmsub_ph(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U, _mm_fmsub_ph((__v8hf)__A, (__v8hf)__B, (__v8hf)__C),
      (__v8hf)_mm_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask3_fnmadd_ph(__m128h __A, __m128h __B, __m128h __C, __mmask8 __U) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddph(-(__v8hf)__A, (__v8hf)__B, (__v8hf)__C),
      (__v8hf)__C);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_fnmadd_ph(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddph(-(__v8hf)__A, (__v8hf)__B, (__v8hf)__C),
      (__v8hf)_mm_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_fnmsub_ph(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddph(-(__v8hf)__A, (__v8hf)__B, -(__v8hf)__C),
      (__v8hf)_mm_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_fmadd_ph(__m256h __A,
                                                                __m256h __B,
                                                                __m256h __C) {
  return (__m256h)__builtin_ia32_vfmaddph256((__v16hf)__A, (__v16hf)__B,
                                             (__v16hf)__C);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_fmadd_ph(__m256h __A, __mmask16 __U, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddph256((__v16hf)__A, (__v16hf)__B, (__v16hf)__C),
      (__v16hf)__A);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask3_fmadd_ph(__m256h __A, __m256h __B, __m256h __C, __mmask16 __U) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddph256((__v16hf)__A, (__v16hf)__B, (__v16hf)__C),
      (__v16hf)__C);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_fmadd_ph(__mmask16 __U, __m256h __A, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddph256((__v16hf)__A, (__v16hf)__B, (__v16hf)__C),
      (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_fmsub_ph(__m256h __A,
                                                                __m256h __B,
                                                                __m256h __C) {
  return (__m256h)__builtin_ia32_vfmaddph256((__v16hf)__A, (__v16hf)__B,
                                             -(__v16hf)__C);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_fmsub_ph(__m256h __A, __mmask16 __U, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddph256((__v16hf)__A, (__v16hf)__B, -(__v16hf)__C),
      (__v16hf)__A);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_fmsub_ph(__mmask16 __U, __m256h __A, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddph256((__v16hf)__A, (__v16hf)__B, -(__v16hf)__C),
      (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask3_fnmadd_ph(__m256h __A, __m256h __B, __m256h __C, __mmask16 __U) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddph256(-(__v16hf)__A, (__v16hf)__B, (__v16hf)__C),
      (__v16hf)__C);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_fnmadd_ph(__mmask16 __U, __m256h __A, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddph256(-(__v16hf)__A, (__v16hf)__B, (__v16hf)__C),
      (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_fnmsub_ph(__mmask16 __U, __m256h __A, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddph256(-(__v16hf)__A, (__v16hf)__B, -(__v16hf)__C),
      (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_fmaddsub_ph(__m128h __A,
                                                                __m128h __B,
                                                                __m128h __C) {
  return (__m128h)__builtin_ia32_vfmaddsubph((__v8hf)__A, (__v8hf)__B,
                                             (__v8hf)__C);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_fmaddsub_ph(__m128h __A, __mmask8 __U, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddsubph((__v8hf)__A, (__v8hf)__B, (__v8hf)__C),
      (__v8hf)__A);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask3_fmaddsub_ph(__m128h __A, __m128h __B, __m128h __C, __mmask8 __U) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddsubph((__v8hf)__A, (__v8hf)__B, (__v8hf)__C),
      (__v8hf)__C);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_fmaddsub_ph(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddsubph((__v8hf)__A, (__v8hf)__B, (__v8hf)__C),
      (__v8hf)_mm_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_fmsubadd_ph(__m128h __A,
                                                                __m128h __B,
                                                                __m128h __C) {
  return (__m128h)__builtin_ia32_vfmaddsubph((__v8hf)__A, (__v8hf)__B,
                                             -(__v8hf)__C);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_fmsubadd_ph(__m128h __A, __mmask8 __U, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddsubph((__v8hf)__A, (__v8hf)__B, -(__v8hf)__C),
      (__v8hf)__A);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_fmsubadd_ph(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddsubph((__v8hf)__A, (__v8hf)__B, -(__v8hf)__C),
      (__v8hf)_mm_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_fmaddsub_ph(__m256h __A, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_vfmaddsubph256((__v16hf)__A, (__v16hf)__B,
                                                (__v16hf)__C);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_fmaddsub_ph(__m256h __A, __mmask16 __U, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddsubph256((__v16hf)__A, (__v16hf)__B, (__v16hf)__C),
      (__v16hf)__A);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask3_fmaddsub_ph(__m256h __A, __m256h __B, __m256h __C, __mmask16 __U) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddsubph256((__v16hf)__A, (__v16hf)__B, (__v16hf)__C),
      (__v16hf)__C);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_fmaddsub_ph(__mmask16 __U, __m256h __A, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddsubph256((__v16hf)__A, (__v16hf)__B, (__v16hf)__C),
      (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_fmsubadd_ph(__m256h __A, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_vfmaddsubph256((__v16hf)__A, (__v16hf)__B,
                                                -(__v16hf)__C);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_fmsubadd_ph(__m256h __A, __mmask16 __U, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddsubph256((__v16hf)__A, (__v16hf)__B, -(__v16hf)__C),
      (__v16hf)__A);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_fmsubadd_ph(__mmask16 __U, __m256h __A, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddsubph256((__v16hf)__A, (__v16hf)__B, -(__v16hf)__C),
      (__v16hf)_mm256_setzero_ph());
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask3_fmsub_ph(__m128h __A, __m128h __B, __m128h __C, __mmask8 __U) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddph((__v8hf)__A, (__v8hf)__B, -(__v8hf)__C),
      (__v8hf)__C);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask3_fmsub_ph(__m256h __A, __m256h __B, __m256h __C, __mmask16 __U) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddph256((__v16hf)__A, (__v16hf)__B, -(__v16hf)__C),
      (__v16hf)__C);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask3_fmsubadd_ph(__m128h __A, __m128h __B, __m128h __C, __mmask8 __U) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddsubph((__v8hf)__A, (__v8hf)__B, -(__v8hf)__C),
      (__v8hf)__C);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask3_fmsubadd_ph(__m256h __A, __m256h __B, __m256h __C, __mmask16 __U) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddsubph256((__v16hf)__A, (__v16hf)__B, -(__v16hf)__C),
      (__v16hf)__C);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_fnmadd_ph(__m128h __A,
                                                              __m128h __B,
                                                              __m128h __C) {
  return (__m128h)__builtin_ia32_vfmaddph((__v8hf)__A, -(__v8hf)__B,
                                          (__v8hf)__C);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_fnmadd_ph(__m128h __A, __mmask8 __U, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddph((__v8hf)__A, -(__v8hf)__B, (__v8hf)__C),
      (__v8hf)__A);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_fnmadd_ph(__m256h __A,
                                                                 __m256h __B,
                                                                 __m256h __C) {
  return (__m256h)__builtin_ia32_vfmaddph256((__v16hf)__A, -(__v16hf)__B,
                                             (__v16hf)__C);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_fnmadd_ph(__m256h __A, __mmask16 __U, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddph256((__v16hf)__A, -(__v16hf)__B, (__v16hf)__C),
      (__v16hf)__A);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_fnmsub_ph(__m128h __A,
                                                              __m128h __B,
                                                              __m128h __C) {
  return (__m128h)__builtin_ia32_vfmaddph((__v8hf)__A, -(__v8hf)__B,
                                          -(__v8hf)__C);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_fnmsub_ph(__m128h __A, __mmask8 __U, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddph((__v8hf)__A, -(__v8hf)__B, -(__v8hf)__C),
      (__v8hf)__A);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask3_fnmsub_ph(__m128h __A, __m128h __B, __m128h __C, __mmask8 __U) {
  return (__m128h)__builtin_ia32_selectph_128(
      (__mmask8)__U,
      __builtin_ia32_vfmaddph((__v8hf)__A, -(__v8hf)__B, -(__v8hf)__C),
      (__v8hf)__C);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_fnmsub_ph(__m256h __A,
                                                                 __m256h __B,
                                                                 __m256h __C) {
  return (__m256h)__builtin_ia32_vfmaddph256((__v16hf)__A, -(__v16hf)__B,
                                             -(__v16hf)__C);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_fnmsub_ph(__m256h __A, __mmask16 __U, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddph256((__v16hf)__A, -(__v16hf)__B, -(__v16hf)__C),
      (__v16hf)__A);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask3_fnmsub_ph(__m256h __A, __m256h __B, __m256h __C, __mmask16 __U) {
  return (__m256h)__builtin_ia32_selectph_256(
      (__mmask16)__U,
      __builtin_ia32_vfmaddph256((__v16hf)__A, -(__v16hf)__B, -(__v16hf)__C),
      (__v16hf)__C);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_fcmul_pch(__m128h __A,
                                                              __m128h __B) {
  return (__m128h)__builtin_ia32_vfcmulcph128_mask(
      (__v4sf)__A, (__v4sf)__B, (__v4sf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_fcmul_pch(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_vfcmulcph128_mask((__v4sf)__A, (__v4sf)__B,
                                                   (__v4sf)__W, (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_fcmul_pch(__mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_vfcmulcph128_mask(
      (__v4sf)__A, (__v4sf)__B, (__v4sf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm256_fcmul_pch(__m256h __A,
                                                                 __m256h __B) {
  return (__m256h)__builtin_ia32_vfcmulcph256_mask(
      (__v8sf)__A, (__v8sf)__B, (__v8sf)_mm256_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_fcmul_pch(__m256h __W, __mmask8 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_vfcmulcph256_mask((__v8sf)__A, (__v8sf)__B,
                                                   (__v8sf)__W, (__mmask8)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_fcmul_pch(__mmask8 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_vfcmulcph256_mask(
      (__v8sf)__A, (__v8sf)__B, (__v8sf)_mm256_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_fcmadd_pch(__m128h __A,
                                                               __m128h __B,
                                                               __m128h __C) {
  return (__m128h)__builtin_ia32_vfcmaddcph128_mask((__v4sf)__A, (__v4sf)__B,
                                                    (__v4sf)__C, (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_fcmadd_pch(__m128h __A, __mmask8 __U, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_selectps_128(
      __U,
      __builtin_ia32_vfcmaddcph128_mask((__v4sf)__A, (__v4sf)(__m128h)__B,
                                        (__v4sf)__C, (__mmask8)__U),
      (__v4sf)__A);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask3_fcmadd_pch(__m128h __A, __m128h __B, __m128h __C, __mmask8 __U) {
  return (__m128h)__builtin_ia32_vfcmaddcph128_mask((__v4sf)__A, (__v4sf)__B,
                                                    (__v4sf)__C, (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_fcmadd_pch(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_vfcmaddcph128_maskz(
      (__v4sf)__A, (__v4sf)__B, (__v4sf)__C, (__mmask8)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_fcmadd_pch(__m256h __A,
                                                                  __m256h __B,
                                                                  __m256h __C) {
  return (__m256h)__builtin_ia32_vfcmaddcph256_mask((__v8sf)__A, (__v8sf)__B,
                                                    (__v8sf)__C, (__mmask8)-1);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_fcmadd_pch(__m256h __A, __mmask8 __U, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectps_256(
      __U,
      __builtin_ia32_vfcmaddcph256_mask((__v8sf)__A, (__v8sf)__B, (__v8sf)__C,
                                        (__mmask8)__U),
      (__v8sf)__A);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask3_fcmadd_pch(__m256h __A, __m256h __B, __m256h __C, __mmask8 __U) {
  return (__m256h)__builtin_ia32_vfcmaddcph256_mask((__v8sf)__A, (__v8sf)__B,
                                                    (__v8sf)__C, (__mmask8)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_fcmadd_pch(__mmask8 __U, __m256h __A, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_vfcmaddcph256_maskz(
      (__v8sf)__A, (__v8sf)__B, (__v8sf)__C, (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_fmul_pch(__m128h __A,
                                                             __m128h __B) {
  return (__m128h)__builtin_ia32_vfmulcph128_mask(
      (__v4sf)__A, (__v4sf)__B, (__v4sf)_mm_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_mask_fmul_pch(__m128h __W,
                                                                  __mmask8 __U,
                                                                  __m128h __A,
                                                                  __m128h __B) {
  return (__m128h)__builtin_ia32_vfmulcph128_mask((__v4sf)__A, (__v4sf)__B,
                                                  (__v4sf)__W, (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_fmul_pch(__mmask8 __U, __m128h __A, __m128h __B) {
  return (__m128h)__builtin_ia32_vfmulcph128_mask(
      (__v4sf)__A, (__v4sf)__B, (__v4sf)_mm_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_fmul_pch(__m256h __A,
                                                                __m256h __B) {
  return (__m256h)__builtin_ia32_vfmulcph256_mask(
      (__v8sf)__A, (__v8sf)__B, (__v8sf)_mm256_undefined_ph(), (__mmask8)-1);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_fmul_pch(__m256h __W, __mmask8 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_vfmulcph256_mask((__v8sf)__A, (__v8sf)__B,
                                                  (__v8sf)__W, (__mmask8)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_fmul_pch(__mmask8 __U, __m256h __A, __m256h __B) {
  return (__m256h)__builtin_ia32_vfmulcph256_mask(
      (__v8sf)__A, (__v8sf)__B, (__v8sf)_mm256_setzero_ph(), (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_fmadd_pch(__m128h __A,
                                                              __m128h __B,
                                                              __m128h __C) {
  return (__m128h)__builtin_ia32_vfmaddcph128_mask((__v4sf)__A, (__v4sf)__B,
                                                   (__v4sf)__C, (__mmask8)-1);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask_fmadd_pch(__m128h __A, __mmask8 __U, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_selectps_128(
      __U,
      __builtin_ia32_vfmaddcph128_mask((__v4sf)__A, (__v4sf)__B, (__v4sf)__C,
                                       (__mmask8)__U),
      (__v4sf)__A);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_mask3_fmadd_pch(__m128h __A, __m128h __B, __m128h __C, __mmask8 __U) {
  return (__m128h)__builtin_ia32_vfmaddcph128_mask((__v4sf)__A, (__v4sf)__B,
                                                   (__v4sf)__C, (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_maskz_fmadd_pch(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return (__m128h)__builtin_ia32_vfmaddcph128_maskz((__v4sf)__A, (__v4sf)__B,
                                                    (__v4sf)__C, (__mmask8)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256))) _mm256_fmadd_pch(__m256h __A,
                                                                 __m256h __B,
                                                                 __m256h __C) {
  return (__m256h)__builtin_ia32_vfmaddcph256_mask((__v8sf)__A, (__v8sf)__B,
                                                   (__v8sf)__C, (__mmask8)-1);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_fmadd_pch(__m256h __A, __mmask8 __U, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_selectps_256(
      __U,
      __builtin_ia32_vfmaddcph256_mask((__v8sf)__A, (__v8sf)__B, (__v8sf)__C,
                                       (__mmask8)__U),
      (__v8sf)__A);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask3_fmadd_pch(__m256h __A, __m256h __B, __m256h __C, __mmask8 __U) {
  return (__m256h)__builtin_ia32_vfmaddcph256_mask((__v8sf)__A, (__v8sf)__B,
                                                   (__v8sf)__C, (__mmask8)__U);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_maskz_fmadd_pch(__mmask8 __U, __m256h __A, __m256h __B, __m256h __C) {
  return (__m256h)__builtin_ia32_vfmaddcph256_maskz((__v8sf)__A, (__v8sf)__B,
                                                    (__v8sf)__C, (__mmask8)__U);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128))) _mm_mask_blend_ph(__mmask8 __U,
                                                                  __m128h __A,
                                                                  __m128h __W) {
  return (__m128h)__builtin_ia32_selectph_128((__mmask8)__U, (__v8hf)__W,
                                              (__v8hf)__A);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_mask_blend_ph(__mmask16 __U, __m256h __A, __m256h __W) {
  return (__m256h)__builtin_ia32_selectph_256((__mmask16)__U, (__v16hf)__W,
                                              (__v16hf)__A);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_permutex2var_ph(__m128h __A, __m128i __I, __m128h __B) {
  return (__m128h)__builtin_ia32_vpermi2varhi128((__v8hi)__A, (__v8hi)__I,
                                                 (__v8hi)__B);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_permutex2var_ph(__m256h __A, __m256i __I, __m256h __B) {
  return (__m256h)__builtin_ia32_vpermi2varhi256((__v16hi)__A, (__v16hi)__I,
                                                 (__v16hi)__B);
}

static __inline__ __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_permutexvar_ph(__m128i __A, __m128h __B) {
  return (__m128h)__builtin_ia32_permvarhi128((__v8hi)__B, (__v8hi)__A);
}

static __inline__ __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_permutexvar_ph(__m256i __A, __m256h __B) {
  return (__m256h)__builtin_ia32_permvarhi256((__v16hi)__B, (__v16hi)__A);
}

static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_reduce_add_ph(__m256h __W) {
  return __builtin_ia32_reduce_fadd_ph256(-0.0f16, __W);
}

static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_reduce_mul_ph(__m256h __W) {
  return __builtin_ia32_reduce_fmul_ph256(1.0f16, __W);
}

static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_reduce_max_ph(__m256h __V) {
  return __builtin_ia32_reduce_fmax_ph256(__V);
}

static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(256)))
_mm256_reduce_min_ph(__m256h __V) {
  return __builtin_ia32_reduce_fmin_ph256(__V);
}

static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_reduce_add_ph(__m128h __W) {
  return __builtin_ia32_reduce_fadd_ph128(-0.0f16, __W);
}

static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_reduce_mul_ph(__m128h __W) {
  return __builtin_ia32_reduce_fmul_ph128(1.0f16, __W);
}

static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_reduce_max_ph(__m128h __V) {
  return __builtin_ia32_reduce_fmax_ph128(__V);
}

static __inline__ _Float16 __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16, avx512vl"), __min_vector_width__(128)))
_mm_reduce_min_ph(__m128h __V) {
  return __builtin_ia32_reduce_fmin_ph128(__V);
}
# 284 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3





# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512bf16intrin.h" 1 3
# 31 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512bf16intrin.h" 3
typedef __bf16 __v32bf __attribute__((__vector_size__(64), __aligned__(64)));
typedef __bf16 __m512bh __attribute__((__vector_size__(64), __aligned__(64)));
typedef __bf16 __bfloat16 __attribute__((deprecated("use __bf16 instead")));
# 52 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512bf16intrin.h" 3
static __inline__ float __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16"))) _mm_cvtsbh_ss(__bf16 __A) {
  return __builtin_ia32_cvtsbf162ss_32(__A);
}
# 68 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512bf16intrin.h" 3
static __inline__ __m512bh __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16"), __min_vector_width__(512)))
_mm512_cvtne2ps_pbh(__m512 __A, __m512 __B) {
  return (__m512bh)__builtin_ia32_cvtne2ps2bf16_512((__v16sf) __A,
                                                    (__v16sf) __B);
}
# 91 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512bf16intrin.h" 3
static __inline__ __m512bh __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16"), __min_vector_width__(512)))
_mm512_mask_cvtne2ps_pbh(__m512bh __W, __mmask32 __U, __m512 __A, __m512 __B) {
  return (__m512bh)__builtin_ia32_selectpbf_512((__mmask32)__U,
                                        (__v32bf)_mm512_cvtne2ps_pbh(__A, __B),
                                        (__v32bf)__W);
}
# 113 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512bf16intrin.h" 3
static __inline__ __m512bh __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16"), __min_vector_width__(512)))
_mm512_maskz_cvtne2ps_pbh(__mmask32 __U, __m512 __A, __m512 __B) {
  return (__m512bh)__builtin_ia32_selectpbf_512((__mmask32)__U,
                                        (__v32bf)_mm512_cvtne2ps_pbh(__A, __B),
                                        (__v32bf)_mm512_setzero_si512());
}
# 129 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512bf16intrin.h" 3
static __inline__ __m256bh __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16"), __min_vector_width__(512)))
_mm512_cvtneps_pbh(__m512 __A) {
  return (__m256bh)__builtin_ia32_cvtneps2bf16_512_mask((__v16sf)__A,
                                              (__v16bf)_mm256_undefined_si256(),
                                              (__mmask16)-1);
}
# 150 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512bf16intrin.h" 3
static __inline__ __m256bh __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16"), __min_vector_width__(512)))
_mm512_mask_cvtneps_pbh(__m256bh __W, __mmask16 __U, __m512 __A) {
  return (__m256bh)__builtin_ia32_cvtneps2bf16_512_mask((__v16sf)__A,
                                                        (__v16bf)__W,
                                                        (__mmask16)__U);
}
# 169 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512bf16intrin.h" 3
static __inline__ __m256bh __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16"), __min_vector_width__(512)))
_mm512_maskz_cvtneps_pbh(__mmask16 __U, __m512 __A) {
  return (__m256bh)__builtin_ia32_cvtneps2bf16_512_mask((__v16sf)__A,
                                                (__v16bf)_mm256_setzero_si256(),
                                                (__mmask16)__U);
}
# 190 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512bf16intrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16"), __min_vector_width__(512)))
_mm512_dpbf16_ps(__m512 __D, __m512bh __A, __m512bh __B) {
  return (__m512)__builtin_ia32_dpbf16ps_512((__v16sf) __D,
                                             (__v32bf) __A,
                                             (__v32bf) __B);
}
# 214 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512bf16intrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16"), __min_vector_width__(512)))
_mm512_mask_dpbf16_ps(__m512 __D, __mmask16 __U, __m512bh __A, __m512bh __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                       (__v16sf)_mm512_dpbf16_ps(__D, __A, __B),
                                       (__v16sf)__D);
}
# 238 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512bf16intrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16"), __min_vector_width__(512)))
_mm512_maskz_dpbf16_ps(__mmask16 __U, __m512 __D, __m512bh __A, __m512bh __B) {
  return (__m512)__builtin_ia32_selectps_512((__mmask16)__U,
                                       (__v16sf)_mm512_dpbf16_ps(__D, __A, __B),
                                       (__v16sf)_mm512_setzero_si512());
}
# 252 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512bf16intrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16"), __min_vector_width__(512))) _mm512_cvtpbh_ps(__m256bh __A) {
  return _mm512_castsi512_ps((__m512i)_mm512_slli_epi32(
      (__m512i)_mm512_cvtepi16_epi32((__m256i)__A), 16));
}
# 267 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512bf16intrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16"), __min_vector_width__(512)))
_mm512_maskz_cvtpbh_ps(__mmask16 __U, __m256bh __A) {
  return _mm512_castsi512_ps((__m512i)_mm512_slli_epi32(
      (__m512i)_mm512_maskz_cvtepi16_epi32((__mmask16)__U, (__m256i)__A), 16));
}
# 285 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512bf16intrin.h" 3
static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512bf16"), __min_vector_width__(512)))
_mm512_mask_cvtpbh_ps(__m512 __S, __mmask16 __U, __m256bh __A) {
  return _mm512_castsi512_ps((__m512i)_mm512_mask_slli_epi32(
      (__m512i)__S, (__mmask16)__U,
      (__m512i)_mm512_cvtepi16_epi32((__m256i)__A), 16));
}
# 290 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3





# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbf16intrin.h" 1 3
# 52 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbf16intrin.h" 3
static __inline__ __m128bh __attribute__((__always_inline__, __nodebug__, __target__("avx512vl, avx512bf16"), __min_vector_width__(128)))
_mm_cvtne2ps_pbh(__m128 __A, __m128 __B) {
  return (__m128bh)__builtin_ia32_cvtne2ps2bf16_128((__v4sf) __A,
                                                    (__v4sf) __B);
}
# 75 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbf16intrin.h" 3
static __inline__ __m128bh __attribute__((__always_inline__, __nodebug__, __target__("avx512vl, avx512bf16"), __min_vector_width__(128)))
_mm_mask_cvtne2ps_pbh(__m128bh __W, __mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128bh)__builtin_ia32_selectpbf_128((__mmask8)__U,
                                             (__v8bf)_mm_cvtne2ps_pbh(__A, __B),
                                             (__v8bf)__W);
}
# 97 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbf16intrin.h" 3
static __inline__ __m128bh __attribute__((__always_inline__, __nodebug__, __target__("avx512vl, avx512bf16"), __min_vector_width__(128)))
_mm_maskz_cvtne2ps_pbh(__mmask8 __U, __m128 __A, __m128 __B) {
  return (__m128bh)__builtin_ia32_selectpbf_128((__mmask8)__U,
                                             (__v8bf)_mm_cvtne2ps_pbh(__A, __B),
                                             (__v8bf)_mm_setzero_si128());
}
# 116 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbf16intrin.h" 3
static __inline__ __m256bh __attribute__((__always_inline__, __nodebug__, __target__("avx512vl, avx512bf16"), __min_vector_width__(256)))
_mm256_cvtne2ps_pbh(__m256 __A, __m256 __B) {
  return (__m256bh)__builtin_ia32_cvtne2ps2bf16_256((__v8sf) __A,
                                                    (__v8sf) __B);
}
# 139 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbf16intrin.h" 3
static __inline__ __m256bh __attribute__((__always_inline__, __nodebug__, __target__("avx512vl, avx512bf16"), __min_vector_width__(256)))
_mm256_mask_cvtne2ps_pbh(__m256bh __W, __mmask16 __U, __m256 __A, __m256 __B) {
  return (__m256bh)__builtin_ia32_selectpbf_256((__mmask16)__U,
                                         (__v16bf)_mm256_cvtne2ps_pbh(__A, __B),
                                         (__v16bf)__W);
}
# 161 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbf16intrin.h" 3
static __inline__ __m256bh __attribute__((__always_inline__, __nodebug__, __target__("avx512vl, avx512bf16"), __min_vector_width__(256)))
_mm256_maskz_cvtne2ps_pbh(__mmask16 __U, __m256 __A, __m256 __B) {
  return (__m256bh)__builtin_ia32_selectpbf_256((__mmask16)__U,
                                         (__v16bf)_mm256_cvtne2ps_pbh(__A, __B),
                                         (__v16bf)_mm256_setzero_si256());
}
# 196 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbf16intrin.h" 3
static __inline__ __m128bh __attribute__((__always_inline__, __nodebug__, __target__("avx512vl, avx512bf16"), __min_vector_width__(128)))
_mm_mask_cvtneps_pbh(__m128bh __W, __mmask8 __U, __m128 __A) {
  return (__m128bh)__builtin_ia32_cvtneps2bf16_128_mask((__v4sf) __A,
                                                        (__v8bf)__W,
                                                        (__mmask8)__U);
}
# 216 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbf16intrin.h" 3
static __inline__ __m128bh __attribute__((__always_inline__, __nodebug__, __target__("avx512vl, avx512bf16"), __min_vector_width__(128)))
_mm_maskz_cvtneps_pbh(__mmask8 __U, __m128 __A) {
  return (__m128bh)__builtin_ia32_cvtneps2bf16_128_mask((__v4sf) __A,
                                                    (__v8bf)_mm_setzero_si128(),
                                                    (__mmask8)__U);
}
# 249 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbf16intrin.h" 3
static __inline__ __m128bh __attribute__((__always_inline__, __nodebug__, __target__("avx512vl, avx512bf16"), __min_vector_width__(256)))
_mm256_mask_cvtneps_pbh(__m128bh __W, __mmask8 __U, __m256 __A) {
  return (__m128bh)__builtin_ia32_cvtneps2bf16_256_mask((__v8sf)__A,
                                                        (__v8bf)__W,
                                                        (__mmask8)__U);
}
# 268 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbf16intrin.h" 3
static __inline__ __m128bh __attribute__((__always_inline__, __nodebug__, __target__("avx512vl, avx512bf16"), __min_vector_width__(256)))
_mm256_maskz_cvtneps_pbh(__mmask8 __U, __m256 __A) {
  return (__m128bh)__builtin_ia32_cvtneps2bf16_256_mask((__v8sf)__A,
                                                    (__v8bf)_mm_setzero_si128(),
                                                    (__mmask8)__U);
}
# 289 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbf16intrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl, avx512bf16"), __min_vector_width__(128)))
_mm_dpbf16_ps(__m128 __D, __m128bh __A, __m128bh __B) {
  return (__m128)__builtin_ia32_dpbf16ps_128((__v4sf)__D,
                                             (__v8bf)__A,
                                             (__v8bf)__B);
}
# 313 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbf16intrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl, avx512bf16"), __min_vector_width__(128)))
_mm_mask_dpbf16_ps(__m128 __D, __mmask8 __U, __m128bh __A, __m128bh __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                           (__v4sf)_mm_dpbf16_ps(__D, __A, __B),
                                           (__v4sf)__D);
}
# 337 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbf16intrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl, avx512bf16"), __min_vector_width__(128)))
_mm_maskz_dpbf16_ps(__mmask8 __U, __m128 __D, __m128bh __A, __m128bh __B) {
  return (__m128)__builtin_ia32_selectps_128((__mmask8)__U,
                                           (__v4sf)_mm_dpbf16_ps(__D, __A, __B),
                                           (__v4sf)_mm_setzero_si128());
}
# 358 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbf16intrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl, avx512bf16"), __min_vector_width__(256)))
_mm256_dpbf16_ps(__m256 __D, __m256bh __A, __m256bh __B) {
  return (__m256)__builtin_ia32_dpbf16ps_256((__v8sf)__D,
                                             (__v16bf)__A,
                                             (__v16bf)__B);
}
# 382 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbf16intrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl, avx512bf16"), __min_vector_width__(256)))
_mm256_mask_dpbf16_ps(__m256 __D, __mmask8 __U, __m256bh __A, __m256bh __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                        (__v8sf)_mm256_dpbf16_ps(__D, __A, __B),
                                        (__v8sf)__D);
}
# 406 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbf16intrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl, avx512bf16"), __min_vector_width__(256)))
_mm256_maskz_dpbf16_ps(__mmask8 __U, __m256 __D, __m256bh __A, __m256bh __B) {
  return (__m256)__builtin_ia32_selectps_256((__mmask8)__U,
                                        (__v8sf)_mm256_dpbf16_ps(__D, __A, __B),
                                        (__v8sf)_mm256_setzero_si256());
}
# 423 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbf16intrin.h" 3
static __inline__ __bf16 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl, avx512bf16"), __min_vector_width__(128))) _mm_cvtness_sbh(float __A) {
  __v4sf __V = {__A, 0, 0, 0};
  __v8bf __R = __builtin_ia32_cvtneps2bf16_128_mask(
      (__v4sf)__V, (__v8bf)_mm_undefined_si128(), (__mmask8)-1);
  return (__bf16)__R[0];
}
# 437 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbf16intrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl, avx512bf16"), __min_vector_width__(128))) _mm_cvtpbh_ps(__m128bh __A) {
  return _mm_castsi128_ps(
      (__m128i)_mm_slli_epi32((__m128i)_mm_cvtepi16_epi32((__m128i)__A), 16));
}
# 449 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbf16intrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl, avx512bf16"), __min_vector_width__(256))) _mm256_cvtpbh_ps(__m128bh __A) {
  return _mm256_castsi256_ps((__m256i)_mm256_slli_epi32(
      (__m256i)_mm256_cvtepi16_epi32((__m128i)__A), 16));
}
# 464 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbf16intrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl, avx512bf16"), __min_vector_width__(128)))
_mm_maskz_cvtpbh_ps(__mmask8 __U, __m128bh __A) {
  return _mm_castsi128_ps((__m128i)_mm_slli_epi32(
      (__m128i)_mm_maskz_cvtepi16_epi32((__mmask8)__U, (__m128i)__A), 16));
}
# 480 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbf16intrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl, avx512bf16"), __min_vector_width__(256)))
_mm256_maskz_cvtpbh_ps(__mmask8 __U, __m128bh __A) {
  return _mm256_castsi256_ps((__m256i)_mm256_slli_epi32(
      (__m256i)_mm256_maskz_cvtepi16_epi32((__mmask8)__U, (__m128i)__A), 16));
}
# 499 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbf16intrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl, avx512bf16"), __min_vector_width__(128)))
_mm_mask_cvtpbh_ps(__m128 __S, __mmask8 __U, __m128bh __A) {
  return _mm_castsi128_ps((__m128i)_mm_mask_slli_epi32(
      (__m128i)__S, (__mmask8)__U, (__m128i)_mm_cvtepi16_epi32((__m128i)__A),
      16));
}
# 519 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlbf16intrin.h" 3
static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("avx512vl, avx512bf16"), __min_vector_width__(256)))
_mm256_mask_cvtpbh_ps(__m256 __S, __mmask8 __U, __m128bh __A) {
  return _mm256_castsi256_ps((__m256i)_mm256_mask_slli_epi32(
      (__m256i)__S, (__mmask8)__U, (__m256i)_mm256_cvtepi16_epi32((__m128i)__A),
      16));
}
# 296 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3





# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/pkuintrin.h" 1 3
# 20 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/pkuintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("pku")))
_rdpkru_u32(void)
{
  return __builtin_ia32_rdpkru();
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("pku")))
_wrpkru(unsigned int __val)
{
  __builtin_ia32_wrpkru(__val);
}
# 302 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/vpclmulqdqintrin.h" 1 3
# 307 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/vaesintrin.h" 1 3
# 36 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/vaesintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("vaes"), __min_vector_width__(256)))
 _mm256_aesenc_epi128(__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_aesenc256((__v4di) __A,
              (__v4di) __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("vaes"), __min_vector_width__(256)))
 _mm256_aesdec_epi128(__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_aesdec256((__v4di) __A,
              (__v4di) __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("vaes"), __min_vector_width__(256)))
 _mm256_aesenclast_epi128(__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_aesenclast256((__v4di) __A,
              (__v4di) __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("vaes"), __min_vector_width__(256)))
 _mm256_aesdeclast_epi128(__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_aesdeclast256((__v4di) __A,
              (__v4di) __B);
}


static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,vaes"), __min_vector_width__(512)))
 _mm512_aesenc_epi128(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_aesenc512((__v8di) __A,
              (__v8di) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,vaes"), __min_vector_width__(512)))
 _mm512_aesdec_epi128(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_aesdec512((__v8di) __A,
              (__v8di) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,vaes"), __min_vector_width__(512)))
 _mm512_aesenclast_epi128(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_aesenclast512((__v8di) __A,
              (__v8di) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,vaes"), __min_vector_width__(512)))
 _mm512_aesdeclast_epi128(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_aesdeclast512((__v8di) __A,
              (__v8di) __B);
}
# 312 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/gfniintrin.h" 1 3
# 55 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/gfniintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("gfni"), __min_vector_width__(128)))
_mm_gf2p8mul_epi8(__m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_vgf2p8mulb_v16qi((__v16qi) __A,
              (__v16qi) __B);
}
# 73 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/gfniintrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx,gfni"), __min_vector_width__(256)))
_mm256_gf2p8mul_epi8(__m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_vgf2p8mulb_v32qi((__v32qi) __A,
              (__v32qi) __B);
}
# 110 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/gfniintrin.h" 3
static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512f,gfni"), __min_vector_width__(512)))
_mm512_gf2p8mul_epi8(__m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_vgf2p8mulb_v64qi((__v64qi) __A,
              (__v64qi) __B);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,gfni"), __min_vector_width__(512)))
_mm512_mask_gf2p8mul_epi8(__m512i __S, __mmask64 __U, __m512i __A, __m512i __B)
{
  return (__m512i) __builtin_ia32_selectb_512(__U,
              (__v64qi) _mm512_gf2p8mul_epi8(__A, __B),
              (__v64qi) __S);
}

static __inline__ __m512i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,gfni"), __min_vector_width__(512)))
_mm512_maskz_gf2p8mul_epi8(__mmask64 __U, __m512i __A, __m512i __B)
{
  return _mm512_mask_gf2p8mul_epi8((__m512i)_mm512_setzero_si512(),
              __U, __A, __B);
}
# 169 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/gfniintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,avx512vl,gfni"), __min_vector_width__(128)))
_mm_mask_gf2p8mul_epi8(__m128i __S, __mmask16 __U, __m128i __A, __m128i __B)
{
  return (__m128i) __builtin_ia32_selectb_128(__U,
              (__v16qi) _mm_gf2p8mul_epi8(__A, __B),
              (__v16qi) __S);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,avx512vl,gfni"), __min_vector_width__(128)))
_mm_maskz_gf2p8mul_epi8(__mmask16 __U, __m128i __A, __m128i __B)
{
  return _mm_mask_gf2p8mul_epi8((__m128i)_mm_setzero_si128(),
              __U, __A, __B);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,avx512vl,gfni"), __min_vector_width__(256)))
_mm256_mask_gf2p8mul_epi8(__m256i __S, __mmask32 __U, __m256i __A, __m256i __B)
{
  return (__m256i) __builtin_ia32_selectb_256(__U,
              (__v32qi) _mm256_gf2p8mul_epi8(__A, __B),
              (__v32qi) __S);
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avx512bw,avx512vl,gfni"), __min_vector_width__(256)))
_mm256_maskz_gf2p8mul_epi8(__mmask32 __U, __m256i __A, __m256i __B)
{
  return _mm256_mask_gf2p8mul_epi8((__m256i)_mm256_setzero_si256(),
              __U, __A, __B);
}
# 317 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxvnniint8intrin.h" 1 3
# 55 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxvnniint8intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint8"), __min_vector_width__(128))) _mm_dpbssd_epi32(__m128i __W,
                                                                 __m128i __A,
                                                                 __m128i __B) {
  return (__m128i)__builtin_ia32_vpdpbssd128((__v4si)__W, (__v4si)__A,
                                             (__v4si)__B);
}
# 92 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxvnniint8intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint8"), __min_vector_width__(256)))
_mm256_dpbssd_epi32(__m256i __W, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_vpdpbssd256((__v8si)__W, (__v8si)__A,
                                             (__v8si)__B);
}
# 129 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxvnniint8intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint8"), __min_vector_width__(128))) _mm_dpbssds_epi32(__m128i __W,
                                                                  __m128i __A,
                                                                  __m128i __B) {
  return (__m128i)__builtin_ia32_vpdpbssds128((__v4si)__W, (__v4si)__A,
                                              (__v4si)__B);
}
# 167 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxvnniint8intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint8"), __min_vector_width__(256)))
_mm256_dpbssds_epi32(__m256i __W, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_vpdpbssds256((__v8si)__W, (__v8si)__A,
                                              (__v8si)__B);
}
# 203 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxvnniint8intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint8"), __min_vector_width__(128))) _mm_dpbsud_epi32(__m128i __W,
                                                                 __m128i __A,
                                                                 __m128i __B) {
  return (__m128i)__builtin_ia32_vpdpbsud128((__v4si)__W, (__v4si)__A,
                                             (__v4si)__B);
}
# 240 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxvnniint8intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint8"), __min_vector_width__(256)))
_mm256_dpbsud_epi32(__m256i __W, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_vpdpbsud256((__v8si)__W, (__v8si)__A,
                                             (__v8si)__B);
}
# 277 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxvnniint8intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint8"), __min_vector_width__(128))) _mm_dpbsuds_epi32(__m128i __W,
                                                                  __m128i __A,
                                                                  __m128i __B) {
  return (__m128i)__builtin_ia32_vpdpbsuds128((__v4si)__W, (__v4si)__A,
                                              (__v4si)__B);
}
# 315 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxvnniint8intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint8"), __min_vector_width__(256)))
_mm256_dpbsuds_epi32(__m256i __W, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_vpdpbsuds256((__v8si)__W, (__v8si)__A,
                                              (__v8si)__B);
}
# 351 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxvnniint8intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint8"), __min_vector_width__(128))) _mm_dpbuud_epi32(__m128i __W,
                                                                 __m128i __A,
                                                                 __m128i __B) {
  return (__m128i)__builtin_ia32_vpdpbuud128((__v4si)__W, (__v4si)__A,
                                             (__v4si)__B);
}
# 388 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxvnniint8intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint8"), __min_vector_width__(256)))
_mm256_dpbuud_epi32(__m256i __W, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_vpdpbuud256((__v8si)__W, (__v8si)__A,
                                             (__v8si)__B);
}
# 425 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxvnniint8intrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint8"), __min_vector_width__(128))) _mm_dpbuuds_epi32(__m128i __W,
                                                                  __m128i __A,
                                                                  __m128i __B) {
  return (__m128i)__builtin_ia32_vpdpbuuds128((__v4si)__W, (__v4si)__A,
                                              (__v4si)__B);
}
# 463 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avxvnniint8intrin.h" 3
static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("avxvnniint8"), __min_vector_width__(256)))
_mm256_dpbuuds_epi32(__m256i __W, __m256i __A, __m256i __B) {
  return (__m256i)__builtin_ia32_vpdpbuuds256((__v8si)__W, (__v8si)__A,
                                              (__v8si)__B);
}
# 322 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3
# 347 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("rdpid")))
_rdpid_u32(void) {
  return __builtin_ia32_rdpid();
}




static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("rdrnd")))
_rdrand16_step(unsigned short *__p)
{
  return (int)__builtin_ia32_rdrand16_step(__p);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("rdrnd")))
_rdrand32_step(unsigned int *__p)
{
  return (int)__builtin_ia32_rdrand32_step(__p);
}


static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("rdrnd")))
_rdrand64_step(unsigned long long *__p)
{
  return (int)__builtin_ia32_rdrand64_step(__p);
}
# 396 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("fsgsbase")))
_readfsbase_u32(void)
{
  return __builtin_ia32_rdfsbase32();
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("fsgsbase")))
_readfsbase_u64(void)
{
  return __builtin_ia32_rdfsbase64();
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("fsgsbase")))
_readgsbase_u32(void)
{
  return __builtin_ia32_rdgsbase32();
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("fsgsbase")))
_readgsbase_u64(void)
{
  return __builtin_ia32_rdgsbase64();
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("fsgsbase")))
_writefsbase_u32(unsigned int __V)
{
  __builtin_ia32_wrfsbase32(__V);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("fsgsbase")))
_writefsbase_u64(unsigned long long __V)
{
  __builtin_ia32_wrfsbase64(__V);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("fsgsbase")))
_writegsbase_u32(unsigned int __V)
{
  __builtin_ia32_wrgsbase32(__V);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("fsgsbase")))
_writegsbase_u64(unsigned long long __V)
{
  __builtin_ia32_wrgsbase64(__V);
}
# 456 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 3
static __inline__ short __attribute__((__always_inline__, __nodebug__, __target__("movbe")))
_loadbe_i16(void const * __P) {
  struct __loadu_i16 {
    unsigned short __v;
  } __attribute__((__packed__, __may_alias__));
  return (short)__builtin_bswap16(((const struct __loadu_i16*)__P)->__v);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("movbe")))
_storebe_i16(void * __P, short __D) {
  struct __storeu_i16 {
    unsigned short __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_i16*)__P)->__v = __builtin_bswap16((unsigned short)__D);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("movbe")))
_loadbe_i32(void const * __P) {
  struct __loadu_i32 {
    unsigned int __v;
  } __attribute__((__packed__, __may_alias__));
  return (int)__builtin_bswap32(((const struct __loadu_i32*)__P)->__v);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("movbe")))
_storebe_i32(void * __P, int __D) {
  struct __storeu_i32 {
    unsigned int __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_i32*)__P)->__v = __builtin_bswap32((unsigned int)__D);
}


static __inline__ long long __attribute__((__always_inline__, __nodebug__, __target__("movbe")))
_loadbe_i64(void const * __P) {
  struct __loadu_i64 {
    unsigned long long __v;
  } __attribute__((__packed__, __may_alias__));
  return (long long)__builtin_bswap64(((const struct __loadu_i64*)__P)->__v);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("movbe")))
_storebe_i64(void * __P, long long __D) {
  struct __storeu_i64 {
    unsigned long long __v;
  } __attribute__((__packed__, __may_alias__));
  ((struct __storeu_i64*)__P)->__v = __builtin_bswap64((unsigned long long)__D);
}





# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/rtmintrin.h" 1 3
# 29 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/rtmintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("rtm")))
_xbegin(void)
{
  return (unsigned int)__builtin_ia32_xbegin();
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("rtm")))
_xend(void)
{
  __builtin_ia32_xend();
}
# 510 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3
# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xtestintrin.h" 1 3
# 21 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xtestintrin.h" 3
static __inline__ int
    __attribute__((__always_inline__, __nodebug__, __target__("rtm")))
    _xtest(void) {
  return __builtin_ia32_xtest();
}
# 511 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/shaintrin.h" 1 3
# 23 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/shaintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sha"), __min_vector_width__(128)))
_mm_sha1nexte_epu32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_sha1nexte((__v4si)__X, (__v4si)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sha"), __min_vector_width__(128)))
_mm_sha1msg1_epu32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_sha1msg1((__v4si)__X, (__v4si)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sha"), __min_vector_width__(128)))
_mm_sha1msg2_epu32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_sha1msg2((__v4si)__X, (__v4si)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sha"), __min_vector_width__(128)))
_mm_sha256rnds2_epu32(__m128i __X, __m128i __Y, __m128i __Z)
{
  return (__m128i)__builtin_ia32_sha256rnds2((__v4si)__X, (__v4si)__Y, (__v4si)__Z);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sha"), __min_vector_width__(128)))
_mm_sha256msg1_epu32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_sha256msg1((__v4si)__X, (__v4si)__Y);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sha"), __min_vector_width__(128)))
_mm_sha256msg2_epu32(__m128i __X, __m128i __Y)
{
  return (__m128i)__builtin_ia32_sha256msg2((__v4si)__X, (__v4si)__Y);
}
# 516 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/fxsrintrin.h" 1 3
# 29 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/fxsrintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("fxsr")))
_fxsave(void *__p)
{
  __builtin_ia32_fxsave(__p);
}
# 47 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/fxsrintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("fxsr")))
_fxrstor(void *__p)
{
  __builtin_ia32_fxrstor(__p);
}
# 64 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/fxsrintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("fxsr")))
_fxsave64(void *__p)
{
  __builtin_ia32_fxsave64(__p);
}
# 82 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/fxsrintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("fxsr")))
_fxrstor64(void *__p)
{
  __builtin_ia32_fxrstor64(__p);
}
# 521 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3



# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xsaveintrin.h" 1 3
# 24 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xsaveintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsave")))
_xsave(void *__p, unsigned long long __m) {
  __builtin_ia32_xsave(__p, __m);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsave")))
_xrstor(void *__p, unsigned long long __m) {
  __builtin_ia32_xrstor(__p, __m);
}
# 49 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xsaveintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsave")))
_xsave64(void *__p, unsigned long long __m) {
  __builtin_ia32_xsave64(__p, __m);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsave")))
_xrstor64(void *__p, unsigned long long __m) {
  __builtin_ia32_xrstor64(__p, __m);
}
# 525 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3



# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xsaveoptintrin.h" 1 3
# 20 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xsaveoptintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsaveopt")))
_xsaveopt(void *__p, unsigned long long __m) {
  __builtin_ia32_xsaveopt(__p, __m);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsaveopt")))
_xsaveopt64(void *__p, unsigned long long __m) {
  __builtin_ia32_xsaveopt64(__p, __m);
}
# 529 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xsavecintrin.h" 1 3
# 20 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xsavecintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsavec")))
_xsavec(void *__p, unsigned long long __m) {
  __builtin_ia32_xsavec(__p, __m);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsavec")))
_xsavec64(void *__p, unsigned long long __m) {
  __builtin_ia32_xsavec64(__p, __m);
}
# 534 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xsavesintrin.h" 1 3
# 20 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xsavesintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsaves")))
_xsaves(void *__p, unsigned long long __m) {
  __builtin_ia32_xsaves(__p, __m);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsaves")))
_xrstors(void *__p, unsigned long long __m) {
  __builtin_ia32_xrstors(__p, __m);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsaves")))
_xrstors64(void *__p, unsigned long long __m) {
  __builtin_ia32_xrstors64(__p, __m);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("xsaves")))
_xsaves64(void *__p, unsigned long long __m) {
  __builtin_ia32_xsaves64(__p, __m);
}
# 539 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/cetintrin.h" 1 3
# 21 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/cetintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _incsspd(int __a) {
  __builtin_ia32_incsspd((unsigned int)__a);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _incsspq(unsigned long long __a) {
  __builtin_ia32_incsspq(__a);
}



static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _inc_ssp(unsigned int __a) {
  __builtin_ia32_incsspq(__a);
}






static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _rdsspd(unsigned int __a) {
  return __builtin_ia32_rdsspd(__a);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _rdsspd_i32(void) {
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wuninitialized"
  unsigned int t;
  return __builtin_ia32_rdsspd(t);
#pragma clang diagnostic pop
}


static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _rdsspq(unsigned long long __a) {
  return __builtin_ia32_rdsspq(__a);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _rdsspq_i64(void) {
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wuninitialized"
  unsigned long long t;
  return __builtin_ia32_rdsspq(t);
#pragma clang diagnostic pop
}



static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _get_ssp(void) {
  return __builtin_ia32_rdsspq(0);
}






static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _saveprevssp(void) {
  __builtin_ia32_saveprevssp();
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _rstorssp(void * __p) {
  __builtin_ia32_rstorssp(__p);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _wrssd(unsigned int __a, void * __p) {
  __builtin_ia32_wrssd(__a, __p);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _wrssq(unsigned long long __a, void * __p) {
  __builtin_ia32_wrssq(__a, __p);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _wrussd(unsigned int __a, void * __p) {
  __builtin_ia32_wrussd(__a, __p);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _wrussq(unsigned long long __a, void * __p) {
  __builtin_ia32_wrussq(__a, __p);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _setssbsy(void) {
  __builtin_ia32_setssbsy();
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("shstk"))) _clrssbsy(void * __p) {
  __builtin_ia32_clrssbsy(__p);
}
# 544 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/adxintrin.h" 1 3
# 21 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/adxintrin.h" 3
static __inline unsigned char __attribute__((__always_inline__, __nodebug__, __target__("adx")))
_addcarryx_u32(unsigned char __cf, unsigned int __x, unsigned int __y,
               unsigned int *__p)
{
  return __builtin_ia32_addcarryx_u32(__cf, __x, __y, __p);
}


static __inline unsigned char __attribute__((__always_inline__, __nodebug__, __target__("adx")))
_addcarryx_u64(unsigned char __cf, unsigned long long __x,
               unsigned long long __y, unsigned long long *__p)
{
  return __builtin_ia32_addcarryx_u64(__cf, __x, __y, __p);
}



static __inline unsigned char __attribute__((__always_inline__, __nodebug__))
_addcarry_u32(unsigned char __cf, unsigned int __x, unsigned int __y,
              unsigned int *__p)
{
  return __builtin_ia32_addcarryx_u32(__cf, __x, __y, __p);
}


static __inline unsigned char __attribute__((__always_inline__, __nodebug__))
_addcarry_u64(unsigned char __cf, unsigned long long __x,
              unsigned long long __y, unsigned long long *__p)
{
  return __builtin_ia32_addcarryx_u64(__cf, __x, __y, __p);
}


static __inline unsigned char __attribute__((__always_inline__, __nodebug__))
_subborrow_u32(unsigned char __cf, unsigned int __x, unsigned int __y,
              unsigned int *__p)
{
  return __builtin_ia32_subborrow_u32(__cf, __x, __y, __p);
}


static __inline unsigned char __attribute__((__always_inline__, __nodebug__))
_subborrow_u64(unsigned char __cf, unsigned long long __x,
               unsigned long long __y, unsigned long long *__p)
{
  return __builtin_ia32_subborrow_u64(__cf, __x, __y, __p);
}
# 549 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3



# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/rdseedintrin.h" 1 3
# 20 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/rdseedintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("rdseed")))
_rdseed16_step(unsigned short *__p)
{
  return (int) __builtin_ia32_rdseed16_step(__p);
}

static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("rdseed")))
_rdseed32_step(unsigned int *__p)
{
  return (int) __builtin_ia32_rdseed32_step(__p);
}


static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("rdseed")))
_rdseed64_step(unsigned long long *__p)
{
  return (int) __builtin_ia32_rdseed64_step(__p);
}
# 553 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/wbnoinvdintrin.h" 1 3
# 17 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/wbnoinvdintrin.h" 3
static __inline__ void
  __attribute__((__always_inline__, __nodebug__, __target__("wbnoinvd")))
_wbnoinvd (void)
{
  __builtin_ia32_wbnoinvd ();
}
# 558 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/cldemoteintrin.h" 1 3
# 28 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/cldemoteintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("cldemote")))
_cldemote(const void * __P) {
  __builtin_ia32_cldemote(__P);
}
# 563 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/waitpkgintrin.h" 1 3
# 20 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/waitpkgintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("waitpkg")))
_umonitor (void * __address)
{
  __builtin_ia32_umonitor (__address);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("waitpkg")))
_umwait (unsigned int __control, unsigned long long __counter)
{
  return __builtin_ia32_umwait (__control,
    (unsigned int)(__counter >> 32), (unsigned int)__counter);
}

static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("waitpkg")))
_tpause (unsigned int __control, unsigned long long __counter)
{
  return __builtin_ia32_tpause (__control,
    (unsigned int)(__counter >> 32), (unsigned int)__counter);
}
# 568 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3





# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/movdirintrin.h" 1 3
# 17 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/movdirintrin.h" 3
static __inline__ void
__attribute__((__always_inline__, __nodebug__, __target__("movdiri")))
_directstoreu_u32 (void *__dst, unsigned int __value)
{
  __builtin_ia32_directstore_u32((unsigned int *)__dst, (unsigned int)__value);
}




static __inline__ void
__attribute__((__always_inline__, __nodebug__, __target__("movdiri")))
_directstoreu_u64 (void *__dst, unsigned long __value)
{
  __builtin_ia32_directstore_u64((unsigned long *)__dst, __value);
}
# 42 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/movdirintrin.h" 3
static __inline__ void
__attribute__((__always_inline__, __nodebug__, __target__("movdir64b")))
_movdir64b (void *__dst __attribute__((align_value(64))), const void *__src)
{
  __builtin_ia32_movdir64b(__dst, __src);
}
# 574 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/pconfigintrin.h" 1 3
# 25 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/pconfigintrin.h" 3
static __inline unsigned int __attribute__((__always_inline__, __nodebug__, __target__("pconfig")))
_pconfig_u32(unsigned int __leaf, long unsigned int __d[])
{
  unsigned int __result;
  __asm__ ("pconfig"
           : "=a" (__result), "=b" (__d[0]), "=c" (__d[1]), "=d" (__d[2])
           : "a" (__leaf), "b" (__d[0]), "c" (__d[1]), "d" (__d[2])
           : "cc");
  return __result;
}
# 579 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/sgxintrin.h" 1 3
# 23 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/sgxintrin.h" 3
static __inline unsigned int __attribute__((__always_inline__, __nodebug__, __target__("sgx")))
_enclu_u32(unsigned int __leaf, long unsigned int __d[])
{
  unsigned int __result;
  __asm__ ("enclu"
           : "=a" (__result), "=b" (__d[0]), "=c" (__d[1]), "=d" (__d[2])
           : "a" (__leaf), "b" (__d[0]), "c" (__d[1]), "d" (__d[2])
           : "cc");
  return __result;
}

static __inline unsigned int __attribute__((__always_inline__, __nodebug__, __target__("sgx")))
_encls_u32(unsigned int __leaf, long unsigned int __d[])
{
  unsigned int __result;
  __asm__ ("encls"
           : "=a" (__result), "=b" (__d[0]), "=c" (__d[1]), "=d" (__d[2])
           : "a" (__leaf), "b" (__d[0]), "c" (__d[1]), "d" (__d[2])
           : "cc");
  return __result;
}

static __inline unsigned int __attribute__((__always_inline__, __nodebug__, __target__("sgx")))
_enclv_u32(unsigned int __leaf, long unsigned int __d[])
{
  unsigned int __result;
  __asm__ ("enclv"
           : "=a" (__result), "=b" (__d[0]), "=c" (__d[1]), "=d" (__d[2])
           : "a" (__leaf), "b" (__d[0]), "c" (__d[1]), "d" (__d[2])
           : "cc");
  return __result;
}
# 584 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ptwriteintrin.h" 1 3
# 21 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ptwriteintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("ptwrite")))
_ptwrite32(unsigned int __value) {
  __builtin_ia32_ptwrite32(__value);
}



static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("ptwrite")))
_ptwrite64(unsigned long long __value) {
  __builtin_ia32_ptwrite64(__value);
}
# 589 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/invpcidintrin.h" 1 3
# 17 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/invpcidintrin.h" 3
static __inline__ void
  __attribute__((__always_inline__, __nodebug__, __target__("invpcid")))
_invpcid(unsigned int __type, void *__descriptor) {
  __builtin_ia32_invpcid(__type, __descriptor);
}
# 594 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3



# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/amxfp16intrin.h" 1 3
# 598 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3
# 629 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 3
# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/keylockerintrin.h" 1 3
# 95 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/keylockerintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("kl"), __min_vector_width__(128)))
_mm_loadiwkey (unsigned int __ctl, __m128i __intkey,
               __m128i __enkey_lo, __m128i __enkey_hi) {
  __builtin_ia32_loadiwkey (__intkey, __enkey_lo, __enkey_hi, __ctl);
}
# 130 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/keylockerintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("kl"), __min_vector_width__(128)))
_mm_encodekey128_u32(unsigned int __htype, __m128i __key, void *__h) {
  return __builtin_ia32_encodekey128_u32(__htype, (__v2di)__key, __h);
}
# 167 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/keylockerintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("kl"), __min_vector_width__(128)))
_mm_encodekey256_u32(unsigned int __htype, __m128i __key_lo, __m128i __key_hi,
                     void *__h) {
  return __builtin_ia32_encodekey256_u32(__htype, (__v2di)__key_lo,
                                         (__v2di)__key_hi, __h);
}
# 206 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/keylockerintrin.h" 3
static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("kl"), __min_vector_width__(128)))
_mm_aesenc128kl_u8(__m128i* __odata, __m128i __idata, const void *__h) {
  return __builtin_ia32_aesenc128kl_u8((__v2di *)__odata, (__v2di)__idata, __h);
}
# 245 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/keylockerintrin.h" 3
static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("kl"), __min_vector_width__(128)))
_mm_aesenc256kl_u8(__m128i* __odata, __m128i __idata, const void *__h) {
  return __builtin_ia32_aesenc256kl_u8((__v2di *)__odata, (__v2di)__idata, __h);
}
# 284 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/keylockerintrin.h" 3
static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("kl"), __min_vector_width__(128)))
_mm_aesdec128kl_u8(__m128i* __odata, __m128i __idata, const void *__h) {
  return __builtin_ia32_aesdec128kl_u8((__v2di *)__odata, (__v2di)__idata, __h);
}
# 323 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/keylockerintrin.h" 3
static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("kl"), __min_vector_width__(128)))
_mm_aesdec256kl_u8(__m128i* __odata, __m128i __idata, const void *__h) {
  return __builtin_ia32_aesdec256kl_u8((__v2di *)__odata, (__v2di)__idata, __h);
}
# 381 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/keylockerintrin.h" 3
static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("kl,widekl"), __min_vector_width__(128)))
_mm_aesencwide128kl_u8(__m128i __odata[8], const __m128i __idata[8], const void* __h) {
  return __builtin_ia32_aesencwide128kl_u8((__v2di *)__odata,
                                           (const __v2di *)__idata, __h);
}
# 427 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/keylockerintrin.h" 3
static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("kl,widekl"), __min_vector_width__(128)))
_mm_aesencwide256kl_u8(__m128i __odata[8], const __m128i __idata[8], const void* __h) {
  return __builtin_ia32_aesencwide256kl_u8((__v2di *)__odata,
                                           (const __v2di *)__idata, __h);
}
# 473 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/keylockerintrin.h" 3
static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("kl,widekl"), __min_vector_width__(128)))
_mm_aesdecwide128kl_u8(__m128i __odata[8], const __m128i __idata[8], const void* __h) {
  return __builtin_ia32_aesdecwide128kl_u8((__v2di *)__odata,
                                           (const __v2di *)__idata, __h);
}
# 519 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/keylockerintrin.h" 3
static __inline__ unsigned char __attribute__((__always_inline__, __nodebug__, __target__("kl,widekl"), __min_vector_width__(128)))
_mm_aesdecwide256kl_u8(__m128i __odata[8], const __m128i __idata[8], const void* __h) {
  return __builtin_ia32_aesdecwide256kl_u8((__v2di *)__odata,
                                           (const __v2di *)__idata, __h);
}
# 630 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/amxintrin.h" 1 3
# 41 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/amxintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("amx-tile")))
_tile_loadconfig(const void *__config) {
  __builtin_ia32_tile_loadconfig(__config);
}
# 57 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/amxintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("amx-tile")))
_tile_storeconfig(void *__config) {
  __builtin_ia32_tile_storeconfig(__config);
}







static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("amx-tile"))) _tile_release(void) {
  __builtin_ia32_tilerelease();
}
# 232 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/amxintrin.h" 3
static __inline__ _tile1024i __attribute__((__always_inline__, __nodebug__, __target__("amx-int8")))
_tile_loadd_internal(unsigned short m, unsigned short n, const void *base,
                     long unsigned int stride) {
  return __builtin_ia32_tileloadd64_internal(m, n, base,
                                             (long unsigned int)(stride));
}


static __inline__ _tile1024i __attribute__((__always_inline__, __nodebug__, __target__("amx-int8")))
_tile_loaddt1_internal(unsigned short m, unsigned short n, const void *base,
                       long unsigned int stride) {
  return __builtin_ia32_tileloaddt164_internal(m, n, base,
                                               (long unsigned int)(stride));
}


static __inline__ _tile1024i __attribute__((__always_inline__, __nodebug__, __target__("amx-int8")))
_tile_dpbssd_internal(unsigned short m, unsigned short n, unsigned short k,
                      _tile1024i dst, _tile1024i src1, _tile1024i src2) {
  return __builtin_ia32_tdpbssd_internal(m, n, k, dst, src1, src2);
}


static __inline__ _tile1024i __attribute__((__always_inline__, __nodebug__, __target__("amx-int8")))
_tile_dpbsud_internal(unsigned short m, unsigned short n, unsigned short k,
                      _tile1024i dst, _tile1024i src1, _tile1024i src2) {
  return __builtin_ia32_tdpbsud_internal(m, n, k, dst, src1, src2);
}


static __inline__ _tile1024i __attribute__((__always_inline__, __nodebug__, __target__("amx-int8")))
_tile_dpbusd_internal(unsigned short m, unsigned short n, unsigned short k,
                      _tile1024i dst, _tile1024i src1, _tile1024i src2) {
  return __builtin_ia32_tdpbusd_internal(m, n, k, dst, src1, src2);
}


static __inline__ _tile1024i __attribute__((__always_inline__, __nodebug__, __target__("amx-int8")))
_tile_dpbuud_internal(unsigned short m, unsigned short n, unsigned short k,
                      _tile1024i dst, _tile1024i src1, _tile1024i src2) {
  return __builtin_ia32_tdpbuud_internal(m, n, k, dst, src1, src2);
}


static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("amx-int8")))
_tile_stored_internal(unsigned short m, unsigned short n, void *base,
                      long unsigned int stride, _tile1024i tile) {
  return __builtin_ia32_tilestored64_internal(m, n, base,
                                              (long unsigned int)(stride), tile);
}


static __inline__ _tile1024i __attribute__((__always_inline__, __nodebug__, __target__("amx-bf16")))
_tile_dpbf16ps_internal(unsigned short m, unsigned short n, unsigned short k,
                        _tile1024i dst, _tile1024i src1, _tile1024i src2) {
  return __builtin_ia32_tdpbf16ps_internal(m, n, k, dst, src1, src2);
}


static __inline__ _tile1024i __attribute__((__always_inline__, __nodebug__, __target__("amx-fp16")))
_tile_dpfp16ps_internal(unsigned short m, unsigned short n, unsigned short k,
                        _tile1024i dst, _tile1024i src1, _tile1024i src2) {
  return __builtin_ia32_tdpfp16ps_internal(m, n, k, dst, src1, src2);
}
# 310 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/amxintrin.h" 3
__attribute__((__always_inline__, __nodebug__, __target__("amx-tile")))
static __inline__ void __tile_loadd(__tile1024i *dst, const void *base,
                                    long unsigned int stride) {
  dst->tile = _tile_loadd_internal(dst->row, dst->col, base, stride);
}
# 331 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/amxintrin.h" 3
__attribute__((__always_inline__, __nodebug__, __target__("amx-tile")))
static __inline__ void __tile_stream_loadd(__tile1024i *dst, const void *base,
                                           long unsigned int stride) {
  dst->tile = _tile_loaddt1_internal(dst->row, dst->col, base, stride);
}
# 353 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/amxintrin.h" 3
__attribute__((__always_inline__, __nodebug__, __target__("amx-int8")))
static __inline__ void __tile_dpbssd(__tile1024i *dst, __tile1024i src0,
                                     __tile1024i src1) {
  dst->tile = _tile_dpbssd_internal(src0.row, src1.col, src0.col, dst->tile,
                                    src0.tile, src1.tile);
}
# 376 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/amxintrin.h" 3
__attribute__((__always_inline__, __nodebug__, __target__("amx-int8")))
static __inline__ void __tile_dpbsud(__tile1024i *dst, __tile1024i src0,
                                     __tile1024i src1) {
  dst->tile = _tile_dpbsud_internal(src0.row, src1.col, src0.col, dst->tile,
                                    src0.tile, src1.tile);
}
# 399 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/amxintrin.h" 3
__attribute__((__always_inline__, __nodebug__, __target__("amx-int8")))
static __inline__ void __tile_dpbusd(__tile1024i *dst, __tile1024i src0,
                                     __tile1024i src1) {
  dst->tile = _tile_dpbusd_internal(src0.row, src1.col, src0.col, dst->tile,
                                    src0.tile, src1.tile);
}
# 422 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/amxintrin.h" 3
__attribute__((__always_inline__, __nodebug__, __target__("amx-int8")))
static __inline__ void __tile_dpbuud(__tile1024i *dst, __tile1024i src0,
                                     __tile1024i src1) {
  dst->tile = _tile_dpbuud_internal(src0.row, src1.col, src0.col, dst->tile,
                                    src0.tile, src1.tile);
}
# 440 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/amxintrin.h" 3
__attribute__((__always_inline__, __nodebug__, __target__("amx-tile")))
static __inline__ void __tile_stored(void *base, long unsigned int stride,
                                     __tile1024i src) {
  _tile_stored_internal(src.row, src.col, base, stride, src.tile);
}
# 454 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/amxintrin.h" 3
__attribute__((__always_inline__, __nodebug__, __target__("amx-tile")))
static __inline__ void __tile_zero(__tile1024i *dst) {
  dst->tile = __builtin_ia32_tilezero_internal(dst->row, dst->col);
}
# 474 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/amxintrin.h" 3
__attribute__((__always_inline__, __nodebug__, __target__("amx-bf16")))
static __inline__ void __tile_dpbf16ps(__tile1024i *dst, __tile1024i src0,
                                       __tile1024i src1) {
  dst->tile = _tile_dpbf16ps_internal(src0.row, src1.col, src0.col, dst->tile,
                                      src0.tile, src1.tile);
}
# 496 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/amxintrin.h" 3
__attribute__((__always_inline__, __nodebug__, __target__("amx-fp16")))
static __inline__ void __tile_dpfp16ps(__tile1024i *dst, __tile1024i src0,
                                       __tile1024i src1) {
  dst->tile = _tile_dpfp16ps_internal(src0.row, src1.col, src0.col, dst->tile,
                                      src0.tile, src1.tile);
}
# 635 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vp2intersectintrin.h" 1 3
# 50 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vp2intersectintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vp2intersect"), __min_vector_width__(512)))
_mm512_2intersect_epi32(__m512i __a, __m512i __b, __mmask16 *__m0, __mmask16 *__m1) {
  __builtin_ia32_vp2intersect_d_512((__v16si)__a, (__v16si)__b, __m0, __m1);
}
# 70 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vp2intersectintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vp2intersect"), __min_vector_width__(512)))
_mm512_2intersect_epi64(__m512i __a, __m512i __b, __mmask8 *__m0, __mmask8 *__m1) {
  __builtin_ia32_vp2intersect_q_512((__v8di)__a, (__v8di)__b, __m0, __m1);
}
# 640 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3





# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlvp2intersectintrin.h" 1 3
# 69 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlvp2intersectintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vp2intersect"), __min_vector_width__(256)))
_mm256_2intersect_epi32(__m256i __a, __m256i __b, __mmask8 *__m0, __mmask8 *__m1) {
  __builtin_ia32_vp2intersect_d_256((__v8si)__a, (__v8si)__b, __m0, __m1);
}
# 89 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlvp2intersectintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vp2intersect"), __min_vector_width__(256)))
_mm256_2intersect_epi64(__m256i __a, __m256i __b, __mmask8 *__m0, __mmask8 *__m1) {
  __builtin_ia32_vp2intersect_q_256((__v4di)__a, (__v4di)__b, __m0, __m1);
}
# 109 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlvp2intersectintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vp2intersect"), __min_vector_width__(128)))
_mm_2intersect_epi32(__m128i __a, __m128i __b, __mmask8 *__m0, __mmask8 *__m1) {
  __builtin_ia32_vp2intersect_d_128((__v4si)__a, (__v4si)__b, __m0, __m1);
}
# 129 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/avx512vlvp2intersectintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("avx512vl,avx512vp2intersect"), __min_vector_width__(128)))
_mm_2intersect_epi64(__m128i __a, __m128i __b, __mmask8 *__m0, __mmask8 *__m1) {
  __builtin_ia32_vp2intersect_q_128((__v2di)__a, (__v2di)__b, __m0, __m1);
}
# 646 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3





# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/enqcmdintrin.h" 1 3
# 35 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/enqcmdintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("enqcmd")))
_enqcmd (void *__dst, const void *__src)
{
  return __builtin_ia32_enqcmd(__dst, __src);
}
# 55 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/enqcmdintrin.h" 3
static __inline__ int __attribute__((__always_inline__, __nodebug__, __target__("enqcmd")))
_enqcmds (void *__dst, const void *__src)
{
  return __builtin_ia32_enqcmds(__dst, __src);
}
# 652 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/serializeintrin.h" 1 3
# 23 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/serializeintrin.h" 3
static __inline__ void
__attribute__((__always_inline__, __nodebug__, __target__("serialize")))
_serialize (void)
{
  __builtin_ia32_serialize ();
}
# 657 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tsxldtrkintrin.h" 1 3
# 31 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tsxldtrkintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("tsxldtrk")))
_xsusldtrk (void)
{
    __builtin_ia32_xsusldtrk();
}
# 48 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tsxldtrkintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("tsxldtrk")))
_xresldtrk (void)
{
    __builtin_ia32_xresldtrk();
}
# 662 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3
# 750 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 3
# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/svmlintrin.h" 1 3
# 54 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/svmlintrin.h" 3
__m128i _mm_div_epi8(__m128i __a, __m128i __b);





__m128i _mm_div_epu8(__m128i __a, __m128i __b);





__m128i _mm_div_epi16(__m128i __a, __m128i __b);





__m128i _mm_div_epu16(__m128i __a, __m128i __b);





__m128i _mm_div_epi32(__m128i __a, __m128i __b);





__m128i _mm_div_epu32(__m128i __a, __m128i __b);





__m128i _mm_div_epi64(__m128i __a, __m128i __b);





__m128i _mm_div_epu64(__m128i __a, __m128i __b);





__m128i _mm_idiv_epi32(__m128i __a, __m128i __b);






__m128i _mm_idivrem_epi32(__m128i *__mem_addr, __m128i __a, __m128i __b);





__m128i _mm_irem_epi32(__m128i __a, __m128i __b);





__m128i _mm_rem_epi8(__m128i __a, __m128i __b);





__m128i _mm_rem_epu8(__m128i __a, __m128i __b);





__m128i _mm_rem_epi16(__m128i __a, __m128i __b);





__m128i _mm_rem_epu16(__m128i __a, __m128i __b);





__m128i _mm_rem_epi32(__m128i __a, __m128i __b);





__m128i _mm_rem_epu32(__m128i __a, __m128i __b);





__m128i _mm_rem_epi64(__m128i __a, __m128i __b);





__m128i _mm_rem_epu64(__m128i __a, __m128i __b);





__m128i _mm_udiv_epi32(__m128i __a, __m128i __b);






__m128i _mm_udivrem_epi32(__m128i *__mem_addr, __m128i __a, __m128i __b);





__m128i _mm_urem_epi32(__m128i __a, __m128i __b);





__m128 _mm_cbrt_ps(__m128 __a);





__m128d _mm_cbrt_pd(__m128d __a);






__m128 _mm_cexp_ps(__m128 __a);





__m128 _mm_clog_ps(__m128 __a);





__m128 _mm_csqrt_ps(__m128 __a);






__m128 _mm_exp_ps(__m128 __a);






__m128d _mm_exp_pd(__m128d __a);






__m128 _mm_exp10_ps(__m128 __a);






__m128d _mm_exp10_pd(__m128d __a);






__m128 _mm_exp2_ps(__m128 __a);






__m128d _mm_exp2_pd(__m128d __a);






__m128 _mm_expm1_ps(__m128 __a);






__m128d _mm_expm1_pd(__m128d __a);





__m128 _mm_invcbrt_ps(__m128 __a);





__m128d _mm_invcbrt_pd(__m128d __a);





__m128 _mm_invsqrt_ps(__m128 __a);





__m128d _mm_invsqrt_pd(__m128d __a);





__m128 _mm_log_ps(__m128 __a);





__m128d _mm_log_pd(__m128d __a);





__m128 _mm_log10_ps(__m128 __a);





__m128d _mm_log10_pd(__m128d __a);





__m128 _mm_log1p_ps(__m128 __a);





__m128d _mm_log1p_pd(__m128d __a);





__m128 _mm_log2_ps(__m128 __a);





__m128d _mm_log2_pd(__m128d __a);







__m128 _mm_logb_ps(__m128 __a);







__m128d _mm_logb_pd(__m128d __a);






__m128 _mm_pow_ps(__m128 __a, __m128 __b);






__m128d _mm_pow_pd(__m128d __a, __m128d __b);






__m128 _mm_svml_sqrt_ps(__m128 __a);






__m128d _mm_svml_sqrt_pd(__m128d __a);





__m128 _mm_acos_ps(__m128 __a);





__m128d _mm_acos_pd(__m128d __a);






__m128 _mm_acosh_ps(__m128 __a);






__m128d _mm_acosh_pd(__m128d __a);





__m128 _mm_asin_ps(__m128 __a);





__m128d _mm_asin_pd(__m128d __a);






__m128 _mm_asinh_ps(__m128 __a);






__m128d _mm_asinh_pd(__m128d __a);






__m128 _mm_atan_ps(__m128 __a);






__m128d _mm_atan_pd(__m128d __a);






__m128 _mm_atan2_ps(__m128 __a, __m128 __b);






__m128d _mm_atan2_pd(__m128d __a, __m128d __b);






__m128 _mm_atanh_ps(__m128 __a);






__m128d _mm_atanh_pd(__m128d __a);





__m128 _mm_cos_ps(__m128 __a);





__m128d _mm_cos_pd(__m128d __a);





__m128 _mm_cosd_ps(__m128 __a);





__m128d _mm_cosd_pd(__m128d __a);






__m128 _mm_cosh_ps(__m128 __a);






__m128d _mm_cosh_pd(__m128d __a);







__m128 _mm_hypot_ps(__m128 __a, __m128 __b);







__m128d _mm_hypot_pd(__m128d __a, __m128d __b);





__m128 _mm_sin_ps(__m128 __a);





__m128d _mm_sin_pd(__m128d __a);






__m128 _mm_sincos_ps(__m128 *__mem_addr, __m128 __a);






__m128d _mm_sincos_pd(__m128d *__mem_addr, __m128d __a);





__m128 _mm_sind_ps(__m128 __a);





__m128d _mm_sind_pd(__m128d __a);






__m128 _mm_sinh_ps(__m128 __a);






__m128d _mm_sinh_pd(__m128d __a);





__m128 _mm_tan_ps(__m128 __a);





__m128d _mm_tan_pd(__m128d __a);





__m128 _mm_tand_ps(__m128 __a);





__m128d _mm_tand_pd(__m128d __a);






__m128 _mm_tanh_ps(__m128 __a);






__m128d _mm_tanh_pd(__m128d __a);






__m128 _mm_cdfnorm_ps(__m128 __a);






__m128d _mm_cdfnorm_pd(__m128d __a);






__m128 _mm_cdfnorminv_ps(__m128 __a);






__m128d _mm_cdfnorminv_pd(__m128d __a);





__m128 _mm_erf_ps(__m128 __a);





__m128d _mm_erf_pd(__m128d __a);





__m128 _mm_erfc_ps(__m128 __a);





__m128d _mm_erfc_pd(__m128d __a);





__m128 _mm_erfcinv_ps(__m128 __a);





__m128d _mm_erfcinv_pd(__m128d __a);





__m128 _mm_erfinv_ps(__m128 __a);





__m128d _mm_erfinv_pd(__m128d __a);






__m128 _mm_svml_round_ps(__m128 __a);






__m128d _mm_svml_round_pd(__m128d __a);
# 780 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/svmlintrin.h" 3
__m128 _mm_svml_ceil_ps(__m128 __a);
# 789 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/svmlintrin.h" 3
__m128d _mm_svml_ceil_pd(__m128d __a);
# 798 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/svmlintrin.h" 3
__m128 _mm_svml_floor_ps(__m128 __a);
# 807 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/svmlintrin.h" 3
__m128d _mm_svml_floor_pd(__m128d __a);
# 816 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/svmlintrin.h" 3
__m128 _mm_trunc_ps(__m128 __a);
# 825 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/svmlintrin.h" 3
__m128d _mm_trunc_pd(__m128d __a);
# 837 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/svmlintrin.h" 3
__m256 _mm256_cbrt_ps(__m256 __a);





__m256d _mm256_cbrt_pd(__m256d __a);






__m256 _mm256_cexp_ps(__m256 __a);





__m256 _mm256_clog_ps(__m256 __a);





__m256 _mm256_csqrt_ps(__m256 __a);






__m256 _mm256_exp_ps(__m256 __a);






__m256d _mm256_exp_pd(__m256d __a);






__m256 _mm256_exp10_ps(__m256 __a);






__m256d _mm256_exp10_pd(__m256d __a);






__m256 _mm256_exp2_ps(__m256 __a);






__m256d _mm256_exp2_pd(__m256d __a);






__m256 _mm256_expm1_ps(__m256 __a);






__m256d _mm256_expm1_pd(__m256d __a);





__m256 _mm256_invcbrt_ps(__m256 __a);





__m256d _mm256_invcbrt_pd(__m256d __a);





__m256 _mm256_invsqrt_ps(__m256 __a);





__m256d _mm256_invsqrt_pd(__m256d __a);





__m256 _mm256_log_ps(__m256 __a);





__m256d _mm256_log_pd(__m256d __a);





__m256 _mm256_log10_ps(__m256 __a);





__m256d _mm256_log10_pd(__m256d __a);





__m256 _mm256_log1p_ps(__m256 __a);





__m256d _mm256_log1p_pd(__m256d __a);





__m256 _mm256_log2_ps(__m256 __a);





__m256d _mm256_log2_pd(__m256d __a);







__m256 _mm256_logb_ps(__m256 __a);







__m256d _mm256_logb_pd(__m256d __a);






__m256 _mm256_pow_ps(__m256 __a, __m256 __b);






__m256d _mm256_pow_pd(__m256d __a, __m256d __b);






__m256 _mm256_svml_sqrt_ps(__m256 __a);






__m256d _mm256_svml_sqrt_pd(__m256d __a);





__m256 _mm256_acos_ps(__m256 __a);





__m256d _mm256_acos_pd(__m256d __a);






__m256 _mm256_acosh_ps(__m256 __a);






__m256d _mm256_acosh_pd(__m256d __a);





__m256 _mm256_asin_ps(__m256 __a);





__m256d _mm256_asin_pd(__m256d __a);






__m256 _mm256_asinh_ps(__m256 __a);






__m256d _mm256_asinh_pd(__m256d __a);






__m256 _mm256_atan_ps(__m256 __a);






__m256d _mm256_atan_pd(__m256d __a);






__m256 _mm256_atan2_ps(__m256 __a, __m256 __b);






__m256d _mm256_atan2_pd(__m256d __a, __m256d __b);






__m256 _mm256_atanh_ps(__m256 __a);






__m256d _mm256_atanh_pd(__m256d __a);





__m256 _mm256_cos_ps(__m256 __a);





__m256d _mm256_cos_pd(__m256d __a);





__m256 _mm256_cosd_ps(__m256 __a);





__m256d _mm256_cosd_pd(__m256d __a);






__m256 _mm256_cosh_ps(__m256 __a);






__m256d _mm256_cosh_pd(__m256d __a);







__m256 _mm256_hypot_ps(__m256 __a, __m256 __b);







__m256d _mm256_hypot_pd(__m256d __a, __m256d __b);





__m256 _mm256_sin_ps(__m256 __a);





__m256d _mm256_sin_pd(__m256d __a);






__m256 _mm256_sincos_ps(__m256 *__mem_addr, __m256 __a);






__m256d _mm256_sincos_pd(__m256d *__mem_addr, __m256d __a);





__m256 _mm256_sind_ps(__m256 __a);





__m256d _mm256_sind_pd(__m256d __a);






__m256 _mm256_sinh_ps(__m256 __a);






__m256d _mm256_sinh_pd(__m256d __a);





__m256 _mm256_tan_ps(__m256 __a);





__m256d _mm256_tan_pd(__m256d __a);





__m256 _mm256_tand_ps(__m256 __a);





__m256d _mm256_tand_pd(__m256d __a);






__m256 _mm256_tanh_ps(__m256 __a);






__m256d _mm256_tanh_pd(__m256d __a);






__m256 _mm256_cdfnorm_ps(__m256 __a);






__m256d _mm256_cdfnorm_pd(__m256d __a);






__m256 _mm256_cdfnorminv_ps(__m256 __a);






__m256d _mm256_cdfnorminv_pd(__m256d __a);





__m256 _mm256_erf_ps(__m256 __a);





__m256d _mm256_erf_pd(__m256d __a);





__m256 _mm256_erfc_ps(__m256 __a);





__m256d _mm256_erfc_pd(__m256d __a);





__m256 _mm256_erfcinv_ps(__m256 __a);





__m256d _mm256_erfcinv_pd(__m256d __a);





__m256 _mm256_erfinv_ps(__m256 __a);





__m256d _mm256_erfinv_pd(__m256d __a);






__m256 _mm256_svml_round_ps(__m256 __a);






__m256d _mm256_svml_round_pd(__m256d __a);
# 1426 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/svmlintrin.h" 3
__m256i _mm256_div_epi8(__m256i __a, __m256i __b);





__m256i _mm256_div_epu8(__m256i __a, __m256i __b);





__m256i _mm256_div_epi16(__m256i __a, __m256i __b);





__m256i _mm256_div_epu16(__m256i __a, __m256i __b);





__m256i _mm256_div_epi32(__m256i __a, __m256i __b);





__m256i _mm256_div_epu32(__m256i __a, __m256i __b);





__m256i _mm256_div_epi64(__m256i __a, __m256i __b);





__m256i _mm256_div_epu64(__m256i __a, __m256i __b);





__m256i _mm256_idiv_epi32(__m256i __a, __m256i __b);






__m256i _mm256_idivrem_epi32(__m256i *__mem_addr, __m256i __a, __m256i __b);





__m256i _mm256_irem_epi32(__m256i __a, __m256i __b);





__m256i _mm256_rem_epi8(__m256i __a, __m256i __b);





__m256i _mm256_rem_epu8(__m256i __a, __m256i __b);





__m256i _mm256_rem_epi16(__m256i __a, __m256i __b);





__m256i _mm256_rem_epu16(__m256i __a, __m256i __b);





__m256i _mm256_rem_epi32(__m256i __a, __m256i __b);





__m256i _mm256_rem_epu32(__m256i __a, __m256i __b);





__m256i _mm256_rem_epi64(__m256i __a, __m256i __b);





__m256i _mm256_rem_epu64(__m256i __a, __m256i __b);





__m256i _mm256_udiv_epi32(__m256i __a, __m256i __b);






__m256i _mm256_udivrem_epi32(__m256i *__mem_addr, __m256i __a, __m256i __b);





__m256i _mm256_urem_epi32(__m256i __a, __m256i __b);
# 1567 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/svmlintrin.h" 3
__m512i _mm512_div_epi8(__m512i __a, __m512i __b);





__m512i _mm512_div_epu8(__m512i __a, __m512i __b);





__m512i _mm512_div_epi16(__m512i __a, __m512i __b);





__m512i _mm512_div_epu16(__m512i __a, __m512i __b);





__m512i _mm512_div_epi32(__m512i __a, __m512i __b);





__m512i _mm512_div_epu32(__m512i __a, __m512i __b);






__m512i _mm512_mask_div_epi32(__m512i __src, __mmask16 __k, __m512i __a,
                              __m512i __b);






__m512i _mm512_mask_div_epu32(__m512i __src, __mmask16 __k, __m512i __a,
                              __m512i __b);





__m512i _mm512_div_epi64(__m512i __a, __m512i __b);





__m512i _mm512_div_epu64(__m512i __a, __m512i __b);





__m512i _mm512_rem_epi8(__m512i __a, __m512i __b);





__m512i _mm512_rem_epu8(__m512i __a, __m512i __b);





__m512i _mm512_rem_epi16(__m512i __a, __m512i __b);





__m512i _mm512_rem_epu16(__m512i __a, __m512i __b);





__m512i _mm512_rem_epi32(__m512i __a, __m512i __b);





__m512i _mm512_rem_epu32(__m512i __a, __m512i __b);






__m512i _mm512_mask_rem_epi32(__m512i __src, __mmask16 __k, __m512i __a,
                              __m512i __b);







__m512i _mm512_mask_rem_epu32(__m512i __src, __mmask16 __k, __m512i __a,
                              __m512i __b);





__m512i _mm512_rem_epi64(__m512i __a, __m512i __b);





__m512i _mm512_rem_epu64(__m512i __a, __m512i __b);





__m512 _mm512_cbrt_ps(__m512 __a);






__m512 _mm512_mask_cbrt_ps(__m512 __src, __mmask16 __k, __m512 __a);





__m512d _mm512_cbrt_pd(__m512d __a);






__m512d _mm512_mask_cbrt_pd(__m512d __src, __mmask8 __k, __m512d __a);






__m512 _mm512_exp_ps(__m512 __a);







__m512 _mm512_mask_exp_ps(__m512 __src, __mmask16 __k, __m512 __a);






__m512d _mm512_exp_pd(__m512d __a);







__m512d _mm512_mask_exp_pd(__m512d __src, __mmask8 __k, __m512d __a);






__m512 _mm512_exp10_ps(__m512 __a);







__m512 _mm512_mask_exp10_ps(__m512 __src, __mmask16 __k, __m512 __a);






__m512d _mm512_exp10_pd(__m512d __a);







__m512d _mm512_mask_exp10_pd(__m512d __src, __mmask8 __k, __m512d __a);






__m512 _mm512_exp2_ps(__m512 __a);







__m512 _mm512_mask_exp2_ps(__m512 __src, __mmask16 __k, __m512 __a);






__m512d _mm512_exp2_pd(__m512d __a);







__m512d _mm512_mask_exp2_pd(__m512d __src, __mmask8 __k, __m512d __a);






__m512 _mm512_expm1_ps(__m512 __a);







__m512 _mm512_mask_expm1_ps(__m512 __src, __mmask16 __k, __m512 __a);






__m512d _mm512_expm1_pd(__m512d __a);







__m512d _mm512_mask_expm1_pd(__m512d __src, __mmask8 __k, __m512d __a);







__m512 _mm512_hypot_ps(__m512 __a, __m512 __b);
# 1853 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/svmlintrin.h" 3
__m512 _mm512_mask_hypot_ps(__m512 __src, __mmask16 __k, __m512 __a,
                            __m512 __b);







__m512d _mm512_hypot_pd(__m512d __a, __m512d __b);
# 1871 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/svmlintrin.h" 3
__m512d _mm512_mask_hypot_pd(__m512d __src, __mmask8 __k, __m512d __a,
                             __m512d __b);





__m512 _mm512_invsqrt_ps(__m512 __a);







__m512 _mm512_mask_invsqrt_ps(__m512 __src, __mmask16 __k, __m512 __a);





__m512d _mm512_invsqrt_pd(__m512d __a);







__m512d _mm512_mask_invsqrt_pd(__m512d __src, __mmask8 __k, __m512d __a);





__m512 _mm512_log_ps(__m512 __a);







__m512 _mm512_mask_log_ps(__m512 __src, __mmask16 __k, __m512 __a);





__m512d _mm512_log_pd(__m512d __a);







__m512d _mm512_mask_log_pd(__m512d __src, __mmask8 __k, __m512d __a);





__m512 _mm512_log10_ps(__m512 __a);







__m512 _mm512_mask_log10_ps(__m512 __src, __mmask16 __k, __m512 __a);





__m512d _mm512_log10_pd(__m512d __a);







__m512d _mm512_mask_log10_pd(__m512d __src, __mmask8 __k, __m512d __a);





__m512 _mm512_log1p_ps(__m512 __a);







__m512 _mm512_mask_log1p_ps(__m512 __src, __mmask16 __k, __m512 __a);





__m512d _mm512_log1p_pd(__m512d __a);







__m512d _mm512_mask_log1p_pd(__m512d __src, __mmask8 __k, __m512d __a);





__m512 _mm512_log2_ps(__m512 __a);







__m512 _mm512_mask_log2_ps(__m512 __src, __mmask16 __k, __m512 __a);





__m512d _mm512_log2_pd(__m512d __a);







__m512d _mm512_mask_log2_pd(__m512d __src, __mmask8 __k, __m512d __a);







__m512 _mm512_logb_ps(__m512 __a);
# 2029 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/svmlintrin.h" 3
__m512 _mm512_mask_logb_ps(__m512 __src, __mmask16 __k, __m512 __a);







__m512d _mm512_logb_pd(__m512d __a);
# 2046 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/svmlintrin.h" 3
__m512d _mm512_mask_logb_pd(__m512d __src, __mmask8 __k, __m512d __a);






__m512 _mm512_pow_ps(__m512 __a, __m512 __b);







__m512 _mm512_mask_pow_ps(__m512 __src, __mmask16 __k, __m512 __a, __m512 __b);






__m512d _mm512_pow_pd(__m512d __a, __m512d __b);







__m512d _mm512_mask_pow_pd(__m512d __src, __mmask8 __k, __m512d __a,
                           __m512d __b);





static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_recip_ps(__m512 __a)
{
  return _mm512_div_ps(_mm512_set1_ps(1.0f), __a);
}






static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_recip_ps(__m512 __src, __mmask16 __k, __m512 __a)
{
  return _mm512_mask_div_ps(__src, __k, _mm512_set1_ps(1.0f), __a);
}





static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_recip_pd(__m512d __a)
{
  return _mm512_div_pd(_mm512_set1_pd(1.0), __a);
}






static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_recip_pd(__m512d __src, __mmask8 __k, __m512d __a)
{
  return _mm512_mask_div_pd(__src, __k, _mm512_set1_pd(1.0), __a);
}





__m512 _mm512_acos_ps(__m512 __a);







__m512 _mm512_mask_acos_ps(__m512 __src, __mmask16 __k, __m512 __a);





__m512d _mm512_acos_pd(__m512d __a);







__m512d _mm512_mask_acos_pd(__m512d __src, __mmask8 __k, __m512d __a);






__m512 _mm512_acosh_ps(__m512 __a);







__m512 _mm512_mask_acosh_ps(__m512 __src, __mmask16 __k, __m512 __a);






__m512d _mm512_acosh_pd(__m512d __a);







__m512d _mm512_mask_acosh_pd(__m512d __src, __mmask8 __k, __m512d __a);





__m512 _mm512_asin_ps(__m512 __a);







__m512 _mm512_mask_asin_ps(__m512 __src, __mmask16 __k, __m512 __a);





__m512d _mm512_asin_pd(__m512d __a);







__m512d _mm512_mask_asin_pd(__m512d __src, __mmask8 __k, __m512d __a);






__m512 _mm512_asinh_ps(__m512 __a);







__m512 _mm512_mask_asinh_ps(__m512 __src, __mmask16 __k, __m512 __a);






__m512d _mm512_asinh_pd(__m512d __a);







__m512d _mm512_mask_asinh_pd(__m512d __src, __mmask8 __k, __m512d __a);






__m512 _mm512_atan_ps(__m512 __a);







__m512 _mm512_mask_atan_ps(__m512 __src, __mmask16 __k, __m512 __a);






__m512d _mm512_atan_pd(__m512d __a);







__m512d _mm512_mask_atan_pd(__m512d __src, __mmask8 __k, __m512d __a);






__m512 _mm512_atan2_ps(__m512 __a, __m512 __b);







__m512 _mm512_mask_atan2_ps(__m512 __src, __mmask16 __k, __m512 __a,
                            __m512 __b);






__m512d _mm512_atan2_pd(__m512d __a, __m512d __b);







__m512d _mm512_mask_atan2_pd(__m512d __src, __mmask8 __k, __m512d __a,
                             __m512d __b);






__m512 _mm512_atanh_ps(__m512 __a);







__m512 _mm512_mask_atanh_ps(__m512 __src, __mmask16 __k, __m512 __a);






__m512d _mm512_atanh_pd(__m512d __a);







__m512d _mm512_mask_atanh_pd(__m512d __src, __mmask8 __k, __m512d __a);





__m512 _mm512_cos_ps(__m512 __a);







__m512 _mm512_mask_cos_ps(__m512 __src, __mmask16 __k, __m512 __a);





__m512d _mm512_cos_pd(__m512d __a);







__m512d _mm512_mask_cos_pd(__m512d __src, __mmask8 __k, __m512d __a);





__m512 _mm512_cosd_ps(__m512 __a);







__m512 _mm512_mask_cosd_ps(__m512 __src, __mmask16 __k, __m512 __a);





__m512d _mm512_cosd_pd(__m512d __a);







__m512d _mm512_mask_cosd_pd(__m512d __src, __mmask8 __k, __m512d __a);






__m512 _mm512_cosh_ps(__m512 __a);







__m512 _mm512_mask_cosh_ps(__m512 __src, __mmask16 __k, __m512 __a);






__m512d _mm512_cosh_pd(__m512d __a);







__m512d _mm512_mask_cosh_pd(__m512d __src, __mmask8 __k, __m512d __a);





__m512 _mm512_sin_ps(__m512 __a);







__m512 _mm512_mask_sin_ps(__m512 __src, __mmask16 __k, __m512 __a);





__m512d _mm512_sin_pd(__m512d __a);







__m512d _mm512_mask_sin_pd(__m512d __src, __mmask8 __k, __m512d __a);






__m512 _mm512_sincos_ps(__m512 *__cos_res, __m512 __a);
# 2458 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/svmlintrin.h" 3
__m512 _mm512_mask_sincos_ps(__m512 *__cos_res, __m512 __sin_src,
                             __m512 __cos_src, __mmask16 __k, __m512 __a);






__m512d _mm512_sincos_pd(__m512d *__cos_res, __m512d __a);
# 2476 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/svmlintrin.h" 3
__m512d _mm512_mask_sincos_pd(__m512d *__cos_res, __m512d __sin_src,
                              __m512d __cos_src, __mmask8 __k, __m512d __a);





__m512 _mm512_sind_ps(__m512 __a);







__m512 _mm512_mask_sind_ps(__m512 __src, __mmask16 __k, __m512 __a);





__m512d _mm512_sind_pd(__m512d __a);







__m512d _mm512_mask_sind_pd(__m512d __src, __mmask8 __k, __m512d __a);






__m512 _mm512_sinh_ps(__m512 __a);







__m512 _mm512_mask_sinh_ps(__m512 __src, __mmask16 __k, __m512 __a);






__m512d _mm512_sinh_pd(__m512d __a);







__m512d _mm512_mask_sinh_pd(__m512d __src, __mmask8 __k, __m512d __a);





__m512 _mm512_tan_ps(__m512 __a);







__m512 _mm512_mask_tan_ps(__m512 __src, __mmask16 __k, __m512 __a);





__m512d _mm512_tan_pd(__m512d __a);







__m512d _mm512_mask_tan_pd(__m512d __src, __mmask8 __k, __m512d __a);





__m512 _mm512_tand_ps(__m512 __a);







__m512 _mm512_mask_tand_ps(__m512 __src, __mmask16 __k, __m512 __a);





__m512d _mm512_tand_pd(__m512d __a);







__m512d _mm512_mask_tand_pd(__m512d __src, __mmask8 __k, __m512d __a);






__m512 _mm512_tanh_ps(__m512 __a);







__m512 _mm512_mask_tanh_ps(__m512 __src, __mmask16 __k, __m512 __a);






__m512d _mm512_tanh_pd(__m512d __a);







__m512d _mm512_mask_tanh_pd(__m512d __src, __mmask8 __k, __m512d __a);






__m512 _mm512_cdfnorm_ps(__m512 __a);







__m512 _mm512_mask_cdfnorm_ps(__m512 __src, __mmask16 __k, __m512 __a);






__m512d _mm512_cdfnorm_pd(__m512d __a);







__m512d _mm512_mask_cdfnorm_pd(__m512d __src, __mmask8 __k, __m512d __a);






__m512 _mm512_cdfnorminv_ps(__m512 __a);







__m512 _mm512_mask_cdfnorminv_ps(__m512 __src, __mmask16 __k, __m512 __a);






__m512d _mm512_cdfnorminv_pd(__m512d __a);







__m512d _mm512_mask_cdfnorminv_pd(__m512d __src, __mmask8 __k, __m512d __a);





__m512 _mm512_erf_ps(__m512 __a);






__m512 _mm512_mask_erf_ps(__m512 __src, __mmask16 __k, __m512 __a);





__m512d _mm512_erf_pd(__m512d __a);






__m512d _mm512_mask_erf_pd(__m512d __src, __mmask8 __k, __m512d __a);





__m512 _mm512_erfc_ps(__m512 __a);







__m512 _mm512_mask_erfc_ps(__m512 __src, __mmask16 __k, __m512 __a);





__m512d _mm512_erfc_pd(__m512d __a);







__m512d _mm512_mask_erfc_pd(__m512d __src, __mmask8 __k, __m512d __a);





__m512 _mm512_erfcinv_ps(__m512 __a);







__m512 _mm512_mask_erfcinv_ps(__m512 __src, __mmask16 __k, __m512 __a);





__m512d _mm512_erfcinv_pd(__m512d __a);







__m512d _mm512_mask_erfcinv_pd(__m512d __src, __mmask8 __k, __m512d __a);





__m512 _mm512_erfinv_ps(__m512 __a);







__m512 _mm512_mask_erfinv_ps(__m512 __src, __mmask16 __k, __m512 __a);





__m512d _mm512_erfinv_pd(__m512d __a);







__m512d _mm512_mask_erfinv_pd(__m512d __src, __mmask8 __k, __m512d __a);






__m512 _mm512_nearbyint_ps(__m512 __a);







__m512 _mm512_mask_nearbyint_ps(__m512 __src, __mmask16 __k, __m512 __a);






__m512d _mm512_nearbyint_pd(__m512d __a);







__m512d _mm512_mask_nearbyint_pd(__m512d __src, __mmask8 __k, __m512d __a);





__m512 _mm512_rint_ps(__m512 __a);







__m512 _mm512_mask_rint_ps(__m512 __src, __mmask16 __k, __m512 __a);





__m512d _mm512_rint_pd(__m512d __a);







__m512d _mm512_mask_rint_pd(__m512d __src, __mmask8 __k, __m512d __a);






__m512d _mm512_svml_round_pd(__m512d __a);







__m512d _mm512_mask_svml_round_pd(__m512d __src, __mmask8 __k, __m512d __a);






static __inline __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_trunc_ps(__m512 __a)
{
  return ((__m512)__builtin_ia32_rndscaleps_mask((__v16sf)(__m512)(__a), (int)((0x00 | 0x03)), (__v16sf)_mm512_undefined_ps(), (__mmask16)-1, 0x04));
}







static __inline__ __m512 __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_trunc_ps (__m512 __src, __mmask16 __k, __m512 __a)
{
  return ((__m512)__builtin_ia32_rndscaleps_mask((__v16sf)(__m512)(__a), (int)((0x00 | 0x03)), (__v16sf)(__m512)(__src), (__mmask16)(__k), 0x04));
}






static __inline __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_trunc_pd(__m512d __a)
{
  return ((__m512d)__builtin_ia32_rndscalepd_mask((__v8df)(__m512d)(__a), (int)((0x00 | 0x03)), (__v8df)_mm512_undefined_pd(), (__mmask8)-1, 0x04));
}







static __inline__ __m512d __attribute__((__always_inline__, __nodebug__, __target__("avx512f"), __min_vector_width__(512)))
_mm512_mask_trunc_pd (__m512d __src, __mmask8 __k, __m512d __a)
{
  return ((__m512d)__builtin_ia32_rndscalepd_mask((__v8df)(__m512d)(__a), (int)((0x00 | 0x03)), (__v8df)(__m512d)(__src), (__mmask8)(__k), 0x04));
}
# 2926 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/svmlintrin.h" 3
__m128h _mm_cbrt_ph(__m128h __a);





__m256h _mm256_cbrt_ph(__m256h __a);





__m512h _mm512_cbrt_ph(__m512h __a);






__m512h _mm512_mask_cbrt_ph(__m512h __src, __mmask32 __k, __m512h __a);






__m128h _mm_exp_ph(__m128h __a);






__m256h _mm256_exp_ph(__m256h __a);






__m512h _mm512_exp_ph(__m512h __a);







__m512h _mm512_mask_exp_ph(__m512h __src, __mmask32 __k, __m512h __a);






__m128h _mm_exp10_ph(__m128h __a);






__m256h _mm256_exp10_ph(__m256h __a);






__m512h _mm512_exp10_ph(__m512h __a);







__m512h _mm512_mask_exp10_ph(__m512h __src, __mmask32 __k, __m512h __a);






__m128h _mm_exp2_ph(__m128h __a);






__m256h _mm256_exp2_ph(__m256h __a);






__m512h _mm512_exp2_ph(__m512h __a);







__m512h _mm512_mask_exp2_ph(__m512h __src, __mmask32 __k, __m512h __a);






__m128h _mm_expm1_ph(__m128h __a);






__m256h _mm256_expm1_ph(__m256h __a);






__m512h _mm512_expm1_ph(__m512h __a);







__m512h _mm512_mask_expm1_ph(__m512h __src, __mmask32 __k, __m512h __a);






__m512h _mm512_hypot_ph(__m512h __a, __m512h __b);
# 3077 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/svmlintrin.h" 3
__m512h _mm512_mask_hypot_ph(__m512h __src, __mmask32 __k, __m512h __a,
                             __m512h __b);





__m128h _mm_invcbrt_ph(__m128h __a);





__m256h _mm256_invcbrt_ph(__m256h __a);





__m128h _mm_invsqrt_ph(__m128h __a);





__m256h _mm256_invsqrt_ph(__m256h __a);





__m512h _mm512_invsqrt_ph(__m512h __a);







__m512h _mm512_mask_invsqrt_ph(__m512h __src, __mmask32 __k, __m512h __a);





__m128h _mm_log_ph(__m128h __a);





__m256h _mm256_log_ph(__m256h __a);





__m512h _mm512_log_ph(__m512h __a);







__m512h _mm512_mask_log_ph(__m512h __src, __mmask32 __k, __m512h __a);





__m128h _mm_log10_ph(__m128h __a);





__m256h _mm256_log10_ph(__m256h __a);





__m512h _mm512_log10_ph(__m512h __a);







__m512h _mm512_mask_log10_ph(__m512h __src, __mmask32 __k, __m512h __a);





__m128h _mm_log1p_ph(__m128h __a);





__m256h _mm256_log1p_ph(__m256h __a);





__m512h _mm512_log1p_ph(__m512h __a);







__m512h _mm512_mask_log1p_ph(__m512h __src, __mmask32 __k, __m512h __a);





__m128h _mm_log2_ph(__m128h __a);





__m256h _mm256_log2_ph(__m256h __a);





__m512h _mm512_log2_ph(__m512h __a);






__m512h _mm512_mask_log2_ph(__m512h __src, __mmask32 __k, __m512h __a);







__m128h _mm_logb_ph(__m128h __a);







__m256h _mm256_logb_ph(__m256h __a);







__m512h _mm512_logb_ph(__m512h __a);
# 3252 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/svmlintrin.h" 3
__m512h _mm512_mask_logb_ph(__m512h __src, __mmask32 __k, __m512h __a);






__m128h _mm_pow_ph(__m128h __a, __m128h __b);






__m256h _mm256_pow_ph(__m256h __a, __m256h __b);






__m512h _mm512_pow_ph(__m512h __a, __m512h __b);







__m512h _mm512_mask_pow_ph(__m512h __src, __mmask32 __k, __m512h __a,
                           __m512h __b);





static __inline __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_recip_ph(__m512h __a)
{
  return _mm512_rcp_ph(__a);
}






static __inline __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_recip_ph(__m512h __src, __mmask32 __k, __m512h __a)
{
  return _mm512_mask_rcp_ph(__src, __k, __a);
}






__m128h _mm_svml_sqrt_ph(__m128h __a);






__m256h _mm256_svml_sqrt_ph(__m256h __a);





__m128h _mm_acos_ph(__m128h __a);





__m256h _mm256_acos_ph(__m256h __a);





__m512h _mm512_acos_ph(__m512h __a);







__m512h _mm512_mask_acos_ph(__m512h __src, __mmask32 __k, __m512h __a);






__m128h _mm_acosh_ph(__m128h __a);






__m256h _mm256_acosh_ph(__m256h __a);






__m512h _mm512_acosh_ph(__m512h __a);







__m512h _mm512_mask_acosh_ph(__m512h __src, __mmask32 __k, __m512h __a);





__m128h _mm_asin_ph(__m128h __a);





__m256h _mm256_asin_ph(__m256h __a);





__m512h _mm512_asin_ph(__m512h __a);







__m512h _mm512_mask_asin_ph(__m512h __src, __mmask32 __k, __m512h __a);






__m128h _mm_asinh_ph(__m128h __a);






__m256h _mm256_asinh_ph(__m256h __a);






__m512h _mm512_asinh_ph(__m512h __a);







__m512h _mm512_mask_asinh_ph(__m512h __src, __mmask32 __k, __m512h __a);





__m128h _mm_atan_ph(__m128h __a);





__m256h _mm256_atan_ph(__m256h __a);





__m512h _mm512_atan_ph(__m512h __a);







__m512h _mm512_mask_atan_ph(__m512h __src, __mmask32 __k, __m512h __a);






__m128h _mm_atan2_ph(__m128h __a, __m128h __b);






__m256h _mm256_atan2_ph(__m256h __a, __m256h __b);






__m512h _mm512_atan2_ph(__m512h __a, __m512h __b);







__m512h _mm512_mask_atan2_ph(__m512h __src, __mmask32 __k, __m512h __a,
                             __m512h __b);






__m128h _mm_atanh_ph(__m128h __a);






__m256h _mm256_atanh_ph(__m256h __a);






__m512h _mm512_atanh_ph(__m512h __a);







__m512h _mm512_mask_atanh_ph(__m512h __src, __mmask32 __k, __m512h __a);





__m128h _mm_cos_ph(__m128h __a);





__m256h _mm256_cos_ph(__m256h __a);





__m512h _mm512_cos_ph(__m512h __a);







__m512h _mm512_mask_cos_ph(__m512h __src, __mmask32 __k, __m512h __a);





__m128h _mm_cosd_ph(__m128h __a);





__m256h _mm256_cosd_ph(__m256h __a);





__m512h _mm512_cosd_ph(__m512h __a);







__m512h _mm512_mask_cosd_ph(__m512h __src, __mmask32 __k, __m512h __a);






__m128h _mm_cosh_ph(__m128h __a);






__m256h _mm256_cosh_ph(__m256h __a);






__m512h _mm512_cosh_ph(__m512h __a);







__m512h _mm512_mask_cosh_ph(__m512h __src, __mmask32 __k, __m512h __a);






__m128h _mm_hypot_ph(__m128h __a, __m128h __b);






__m256h _mm256_hypot_ph(__m256h __a, __m256h __b);





__m128h _mm_sin_ph(__m128h __a);





__m256h _mm256_sin_ph(__m256h __a);





__m512h _mm512_sin_ph(__m512h __a);






__m512h _mm512_mask_sin_ph(__m512h __src, __mmask32 __k, __m512h __a);






__m128h _mm_sincos_ph(__m128h *__mem_addr, __m128h __a);






__m256h _mm256_sincos_ph(__m256h *__mem_addr, __m256h __a);






__m512h _mm512_sincos_ph(__m512h *__mem_addr, __m512h __a);
# 3662 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/svmlintrin.h" 3
__m512h _mm512_mask_sincos_ph(__m512h *__mem_addr, __m512h __sin_src,
                              __m512h __cos_src, __mmask32 __k, __m512h __a);





__m128h _mm_sind_ph(__m128h __a);





__m256h _mm256_sind_ph(__m256h __a);





__m512h _mm512_sind_ph(__m512h __a);






__m512h _mm512_mask_sind_ph(__m512h __src, __mmask32 __k, __m512h __a);





__m128h _mm_sinh_ph(__m128h __a);





__m256h _mm256_sinh_ph(__m256h __a);





__m512h _mm512_sinh_ph(__m512h __a);







__m512h _mm512_mask_sinh_ph(__m512h __src, __mmask32 __k, __m512h __a);





__m128h _mm_tan_ph(__m128h __a);





__m256h _mm256_tan_ph(__m256h __a);





__m512h _mm512_tan_ph(__m512h __a);







__m512h _mm512_mask_tan_ph(__m512h __src, __mmask32 __k, __m512h __a);





__m128h _mm_tand_ph(__m128h __a);





__m256h _mm256_tand_ph(__m256h __a);





__m512h _mm512_tand_ph(__m512h __a);







__m512h _mm512_mask_tand_ph(__m512h __src, __mmask32 __k, __m512h __a);






__m128h _mm_tanh_ph(__m128h __a);






__m256h _mm256_tanh_ph(__m256h __a);






__m512h _mm512_tanh_ph(__m512h __a);







__m512h _mm512_mask_tanh_ph(__m512h __src, __mmask32 __k, __m512h __a);






__m128h _mm_cdfnorm_ph(__m128h __a);






__m256h _mm256_cdfnorm_ph(__m256h __a);






__m512h _mm512_cdfnorm_ph(__m512h __a);







__m512h _mm512_mask_cdfnorm_ph(__m512h __src, __mmask32 __k, __m512h __a);






__m128h _mm_cdfnorminv_ph(__m128h __a);






__m256h _mm256_cdfnorminv_ph(__m256h __a);






__m512h _mm512_cdfnorminv_ph(__m512h __a);







__m512h _mm512_mask_cdfnorminv_ph(__m512h __src, __mmask32 __k, __m512h __a);





__m128h _mm_erf_ph(__m128h __a);





__m256h _mm256_erf_ph(__m256h __a);





__m512h _mm512_erf_ph(__m512h __a);






__m512h _mm512_mask_erf_ph(__m512h __src, __mmask32 __k, __m512h __a);





__m128h _mm_erfc_ph(__m128h __a);





__m256h _mm256_erfc_ph(__m256h __a);





__m512h _mm512_erfc_ph(__m512h __a);







__m512h _mm512_mask_erfc_ph(__m512h __src, __mmask32 __k, __m512h __a);





__m128h _mm_erfcinv_ph(__m128h __a);





__m256h _mm256_erfcinv_ph(__m256h __a);





__m512h _mm512_erfcinv_ph(__m512h __a);







__m512h _mm512_mask_erfcinv_ph(__m512h __src, __mmask32 __k, __m512h __a);





__m128h _mm_erfinv_ph(__m128h __a);





__m256h _mm256_erfinv_ph(__m256h __a);





__m512h _mm512_erfinv_ph(__m512h __a);







__m512h _mm512_mask_erfinv_ph(__m512h __src, __mmask32 __k, __m512h __a);






static __inline __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_svml_ceil_ph(__m128h __a)
{
  return ((__m128h)__builtin_ia32_rndscaleph_128_mask( (__v8hf)(__m128h)(__a), (int)(0x02), (__v8hf)_mm_setzero_ph(), (__mmask8)-1));
}






static __inline __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(256)))
_mm256_svml_ceil_ph(__m256h __a)
{
  return ((__m256h)__builtin_ia32_rndscaleph_256_mask( (__v16hf)(__m256h)(__a), (int)(0x02), (__v16hf)_mm256_setzero_ph(), (__mmask16)-1));
}






static __inline __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_ceil_ph(__m512h __a)
{
  return ((__m512h)__builtin_ia32_rndscaleph_mask( (__v32hf)(__m512h)(__a), (int)(0x02), (__v32hf)(__m512h)(__a), (__mmask32)-1, 0x04));
}







static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_ceil_ph(__m512h __src, __mmask32 __k, __m512h __a)
{
  return ((__m512h)__builtin_ia32_rndscaleph_mask( (__v32hf)(__m512h)(__a), (int)(0x02), (__v32hf)(__m512h)(__src), (__mmask32)(__k), 0x04));
}






static __inline __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_svml_floor_ph(__m128h __a)
{
  return ((__m128h)__builtin_ia32_rndscaleph_128_mask( (__v8hf)(__m128h)(__a), (int)(0x01), (__v8hf)_mm_setzero_ph(), (__mmask8)-1));
}






static __inline __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(256)))
_mm256_svml_floor_ph(__m256h __a)
{
  return ((__m256h)__builtin_ia32_rndscaleph_256_mask( (__v16hf)(__m256h)(__a), (int)(0x01), (__v16hf)_mm256_setzero_ph(), (__mmask16)-1));
}






static __inline __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_floor_ph(__m512h __a)
{
  return ((__m512h)__builtin_ia32_rndscaleph_mask( (__v32hf)(__m512h)(__a), (int)(0x01), (__v32hf)(__m512h)(__a), (__mmask32)-1, 0x04));
}







static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_floor_ph(__m512h __src, __mmask32 __k, __m512h __a)
{
  return ((__m512h)__builtin_ia32_rndscaleph_mask( (__v32hf)(__m512h)(__a), (int)(0x01), (__v32hf)(__m512h)(__src), (__mmask32)(__k), 0x04));
}






__m512h _mm512_nearbyint_ph(__m512h __a);







__m512h _mm512_mask_nearbyint_ph(__m512h __src, __mmask32 __k, __m512h __a);





__m512h _mm512_rint_ph(__m512h __a);







__m512h _mm512_mask_rint_ph(__m512h __src, __mmask32 __k, __m512h __a);






__m128h _mm_svml_round_ph(__m128h __a);






__m256h _mm256_svml_round_ph(__m256h __a);






__m512h _mm512_svml_round_ph(__m512h __a);







__m512h _mm512_mask_svml_round_ph(__m512h __src, __mmask32 __k, __m512h __a);






static __inline __m128h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(128)))
_mm_trunc_ph(__m128h __a)
{
  return ((__m128h)__builtin_ia32_rndscaleph_128_mask( (__v8hf)(__m128h)(__a), (int)((0x00 | 0x03)), (__v8hf)_mm_setzero_ph(), (__mmask8)-1));
}






static __inline __m256h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(256)))
_mm256_trunc_ph(__m256h __a)
{
  return ((__m256h)__builtin_ia32_rndscaleph_256_mask( (__v16hf)(__m256h)(__a), (int)((0x00 | 0x03)), (__v16hf)_mm256_setzero_ph(), (__mmask16)-1));
}






static __inline __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_trunc_ph(__m512h __a)
{
  return ((__m512h)__builtin_ia32_rndscaleph_mask( (__v32hf)(__m512h)(__a), (int)((0x00 | 0x03)), (__v32hf)(__m512h)(__a), (__mmask32)-1, 0x04));
}







static __inline__ __m512h __attribute__((__always_inline__, __nodebug__, __target__("avx512fp16"), __min_vector_width__(512)))
_mm512_mask_trunc_ph(__m512h __src, __mmask32 __k, __m512h __a)
{
  return ((__m512h)__builtin_ia32_rndscaleph_mask( (__v32hf)(__m512h)(__a), (int)((0x00 | 0x03)), (__v32hf)(__m512h)(__src), (__mmask32)(__k), 0x04));
}
# 751 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 2 3





static __inline__ void __attribute__((__always_inline__, __nodebug__))
_clac(void) {
  __asm__ __volatile__ ("clac" : : : "memory");
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_stac(void) {
  __asm__ __volatile__ ("stac" : : : "memory");
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_lgdt(void *__ptr) {
  __asm__ __volatile__("lgdt %0" : : "m"(*(short *)(__ptr)) : "memory");
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
_sgdt(void *__ptr) {
  __asm__ __volatile__("sgdt %0" : "=m"(*(short *)(__ptr)) : : "memory");
}







void __cpuid(int[4], int);
void __cpuidex(int[4], int, int);




static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__))
__readmsr(unsigned int __register) {






  unsigned int __edx;
  unsigned int __eax;
  __asm__ ("rdmsr" : "=d"(__edx), "=a"(__eax) : "c"(__register));
  return (((unsigned long long)__edx) << 32) | (unsigned long long)__eax;
}

static __inline__ void __attribute__((__always_inline__, __nodebug__))
__writemsr(unsigned int __register, unsigned long long __data) {
  __asm__ ("wrmsr" : : "d"((unsigned)(__data >> 32)), "a"((unsigned)__data), "c"(__register));
}
# 887 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/immintrin.h" 3
#pragma float_control(pop)
# 34 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/x86intrin.h" 2 3



# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mm3dnow.h" 1 3
# 14 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mm3dnow.h" 3
# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/prfchwintrin.h" 1 3
# 41 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/prfchwintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__))
_m_prefetch(void *__P)
{
  __builtin_prefetch (__P, 0, 3 );
}
# 62 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/prfchwintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__))
_m_prefetchw(volatile const void *__P)
{
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wcast-qual"
  __builtin_prefetch ((const void*)__P, 1, 3 );
#pragma clang diagnostic pop
}
# 15 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mm3dnow.h" 2 3

typedef float __v2sf __attribute__((__vector_size__(8)));




static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("3dnow")))
_m_femms(void) {
  __builtin_ia32_femms();
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pavgusb(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pavgusb((__v8qi)__m1, (__v8qi)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pf2id(__m64 __m) {
  return (__m64)__builtin_ia32_pf2id((__v2sf)__m);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfacc(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfacc((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfadd(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfadd((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfcmpeq(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfcmpeq((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfcmpge(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfcmpge((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfcmpgt(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfcmpgt((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfmax(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfmax((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfmin(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfmin((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfmul(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfmul((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfrcp(__m64 __m) {
  return (__m64)__builtin_ia32_pfrcp((__v2sf)__m);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfrcpit1(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfrcpit1((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfrcpit2(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfrcpit2((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfrsqrt(__m64 __m) {
  return (__m64)__builtin_ia32_pfrsqrt((__v2sf)__m);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfrsqrtit1(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfrsqit1((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfsub(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfsub((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pfsubr(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfsubr((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pi2fd(__m64 __m) {
  return (__m64)__builtin_ia32_pi2fd((__v2si)__m);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnow"), __min_vector_width__(64)))
_m_pmulhrw(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pmulhrw((__v4hi)__m1, (__v4hi)__m2);
}





static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnowa"), __min_vector_width__(64)))
_m_pf2iw(__m64 __m) {
  return (__m64)__builtin_ia32_pf2iw((__v2sf)__m);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnowa"), __min_vector_width__(64)))
_m_pfnacc(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfnacc((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnowa"), __min_vector_width__(64)))
_m_pfpnacc(__m64 __m1, __m64 __m2) {
  return (__m64)__builtin_ia32_pfpnacc((__v2sf)__m1, (__v2sf)__m2);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnowa"), __min_vector_width__(64)))
_m_pi2fw(__m64 __m) {
  return (__m64)__builtin_ia32_pi2fw((__v2si)__m);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnowa"), __min_vector_width__(64)))
_m_pswapdsf(__m64 __m) {
  return (__m64)__builtin_ia32_pswapdsf((__v2sf)__m);
}

static __inline__ __m64 __attribute__((__always_inline__, __nodebug__, __target__("3dnowa"), __min_vector_width__(64)))
_m_pswapdsi(__m64 __m) {
  return (__m64)__builtin_ia32_pswapdsi((__v2si)__m);
}
# 38 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/x86intrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/prfchwintrin.h" 1 3
# 43 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/x86intrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ammintrin.h" 1 3
# 69 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ammintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4a"), __min_vector_width__(128)))
_mm_extract_si64(__m128i __x, __m128i __y)
{
  return (__m128i)__builtin_ia32_extrq((__v2di)__x, (__v16qi)__y);
}
# 139 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ammintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("sse4a"), __min_vector_width__(128)))
_mm_insert_si64(__m128i __x, __m128i __y)
{
  return (__m128i)__builtin_ia32_insertq((__v2di)__x, (__v2di)__y);
}
# 157 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ammintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse4a"), __min_vector_width__(128)))
_mm_stream_sd(double *__p, __m128d __a)
{
  __builtin_ia32_movntsd(__p, (__v2df)__a);
}
# 175 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/ammintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("sse4a"), __min_vector_width__(128)))
_mm_stream_ss(float *__p, __m128 __a)
{
  __builtin_ia32_movntss(__p, (__v4sf)__a);
}
# 48 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/x86intrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/fma4intrin.h" 1 3
# 23 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/fma4intrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_macc_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddps((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_macc_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddpd((__v2df)__A, (__v2df)__B, (__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_macc_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddss((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_macc_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsd((__v2df)__A, (__v2df)__B, (__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_msub_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddps((__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_msub_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddpd((__v2df)__A, (__v2df)__B, -(__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_msub_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddss((__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_msub_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsd((__v2df)__A, (__v2df)__B, -(__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_nmacc_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddps(-(__v4sf)__A, (__v4sf)__B, (__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_nmacc_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddpd(-(__v2df)__A, (__v2df)__B, (__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_nmacc_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddss(-(__v4sf)__A, (__v4sf)__B, (__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_nmacc_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsd(-(__v2df)__A, (__v2df)__B, (__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_nmsub_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddps(-(__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_nmsub_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddpd(-(__v2df)__A, (__v2df)__B, -(__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_nmsub_ss(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddss(-(__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_nmsub_sd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsd(-(__v2df)__A, (__v2df)__B, -(__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_maddsub_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddsubps((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_maddsub_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsubpd((__v2df)__A, (__v2df)__B, (__v2df)__C);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_msubadd_ps(__m128 __A, __m128 __B, __m128 __C)
{
  return (__m128)__builtin_ia32_vfmaddsubps((__v4sf)__A, (__v4sf)__B, -(__v4sf)__C);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(128)))
_mm_msubadd_pd(__m128d __A, __m128d __B, __m128d __C)
{
  return (__m128d)__builtin_ia32_vfmaddsubpd((__v2df)__A, (__v2df)__B, -(__v2df)__C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(256)))
_mm256_macc_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddps256((__v8sf)__A, (__v8sf)__B, (__v8sf)__C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(256)))
_mm256_macc_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddpd256((__v4df)__A, (__v4df)__B, (__v4df)__C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(256)))
_mm256_msub_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddps256((__v8sf)__A, (__v8sf)__B, -(__v8sf)__C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(256)))
_mm256_msub_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddpd256((__v4df)__A, (__v4df)__B, -(__v4df)__C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(256)))
_mm256_nmacc_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddps256(-(__v8sf)__A, (__v8sf)__B, (__v8sf)__C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(256)))
_mm256_nmacc_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddpd256(-(__v4df)__A, (__v4df)__B, (__v4df)__C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(256)))
_mm256_nmsub_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddps256(-(__v8sf)__A, (__v8sf)__B, -(__v8sf)__C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(256)))
_mm256_nmsub_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddpd256(-(__v4df)__A, (__v4df)__B, -(__v4df)__C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(256)))
_mm256_maddsub_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddsubps256((__v8sf)__A, (__v8sf)__B, (__v8sf)__C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(256)))
_mm256_maddsub_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddsubpd256((__v4df)__A, (__v4df)__B, (__v4df)__C);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(256)))
_mm256_msubadd_ps(__m256 __A, __m256 __B, __m256 __C)
{
  return (__m256)__builtin_ia32_vfmaddsubps256((__v8sf)__A, (__v8sf)__B, -(__v8sf)__C);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("fma4"), __min_vector_width__(256)))
_mm256_msubadd_pd(__m256d __A, __m256d __B, __m256d __C)
{
  return (__m256d)__builtin_ia32_vfmaddsubpd256((__v4df)__A, (__v4df)__B, -(__v4df)__C);
}
# 53 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/x86intrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xopintrin.h" 1 3
# 17 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xopintrin.h" 3
# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/fma4intrin.h" 1 3
# 18 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xopintrin.h" 2 3





static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_maccs_epi16(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpmacssww((__v8hi)__A, (__v8hi)__B, (__v8hi)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_macc_epi16(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpmacsww((__v8hi)__A, (__v8hi)__B, (__v8hi)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_maccsd_epi16(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpmacsswd((__v8hi)__A, (__v8hi)__B, (__v4si)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_maccd_epi16(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpmacswd((__v8hi)__A, (__v8hi)__B, (__v4si)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_maccs_epi32(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpmacssdd((__v4si)__A, (__v4si)__B, (__v4si)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_macc_epi32(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpmacsdd((__v4si)__A, (__v4si)__B, (__v4si)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_maccslo_epi32(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpmacssdql((__v4si)__A, (__v4si)__B, (__v2di)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_macclo_epi32(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpmacsdql((__v4si)__A, (__v4si)__B, (__v2di)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_maccshi_epi32(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpmacssdqh((__v4si)__A, (__v4si)__B, (__v2di)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_macchi_epi32(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpmacsdqh((__v4si)__A, (__v4si)__B, (__v2di)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_maddsd_epi16(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpmadcsswd((__v8hi)__A, (__v8hi)__B, (__v4si)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_maddd_epi16(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpmadcswd((__v8hi)__A, (__v8hi)__B, (__v4si)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_haddw_epi8(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphaddbw((__v16qi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_haddd_epi8(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphaddbd((__v16qi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_haddq_epi8(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphaddbq((__v16qi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_haddd_epi16(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphaddwd((__v8hi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_haddq_epi16(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphaddwq((__v8hi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_haddq_epi32(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphadddq((__v4si)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_haddw_epu8(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphaddubw((__v16qi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_haddd_epu8(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphaddubd((__v16qi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_haddq_epu8(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphaddubq((__v16qi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_haddd_epu16(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphadduwd((__v8hi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_haddq_epu16(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphadduwq((__v8hi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_haddq_epu32(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphaddudq((__v4si)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_hsubw_epi8(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphsubbw((__v16qi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_hsubd_epi16(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphsubwd((__v8hi)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_hsubq_epi32(__m128i __A)
{
  return (__m128i)__builtin_ia32_vphsubdq((__v4si)__A);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_cmov_si128(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)(((__v2du)__A & (__v2du)__C) | ((__v2du)__B & ~(__v2du)__C));
}

static __inline__ __m256i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(256)))
_mm256_cmov_si256(__m256i __A, __m256i __B, __m256i __C)
{
  return (__m256i)(((__v4du)__A & (__v4du)__C) | ((__v4du)__B & ~(__v4du)__C));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_perm_epi8(__m128i __A, __m128i __B, __m128i __C)
{
  return (__m128i)__builtin_ia32_vpperm((__v16qi)__A, (__v16qi)__B, (__v16qi)__C);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_rot_epi8(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vprotb((__v16qi)__A, (__v16qi)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_rot_epi16(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vprotw((__v8hi)__A, (__v8hi)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_rot_epi32(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vprotd((__v4si)__A, (__v4si)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_rot_epi64(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vprotq((__v2di)__A, (__v2di)__B);
}
# 239 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xopintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_shl_epi8(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpshlb((__v16qi)__A, (__v16qi)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_shl_epi16(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpshlw((__v8hi)__A, (__v8hi)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_shl_epi32(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpshld((__v4si)__A, (__v4si)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_shl_epi64(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpshlq((__v2di)__A, (__v2di)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_sha_epi8(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpshab((__v16qi)__A, (__v16qi)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_sha_epi16(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpshaw((__v8hi)__A, (__v8hi)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_sha_epi32(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpshad((__v4si)__A, (__v4si)__B);
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_sha_epi64(__m128i __A, __m128i __B)
{
  return (__m128i)__builtin_ia32_vpshaq((__v2di)__A, (__v2di)__B);
}
# 328 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xopintrin.h" 3
static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comlt_epu8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomub((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (0)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comle_epu8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomub((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (1)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comgt_epu8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomub((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (2)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comge_epu8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomub((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (3)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comeq_epu8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomub((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (4)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comneq_epu8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomub((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (5)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comfalse_epu8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomub((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (6)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comtrue_epu8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomub((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (7)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comlt_epu16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (0)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comle_epu16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (1)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comgt_epu16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (2)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comge_epu16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (3)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comeq_epu16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (4)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comneq_epu16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (5)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comfalse_epu16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (6)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comtrue_epu16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (7)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comlt_epu32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomud((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (0)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comle_epu32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomud((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (1)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comgt_epu32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomud((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (2)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comge_epu32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomud((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (3)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comeq_epu32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomud((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (4)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comneq_epu32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomud((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (5)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comfalse_epu32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomud((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (6)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comtrue_epu32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomud((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (7)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comlt_epu64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (0)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comle_epu64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (1)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comgt_epu64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (2)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comge_epu64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (3)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comeq_epu64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (4)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comneq_epu64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (5)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comfalse_epu64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (6)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comtrue_epu64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomuq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (7)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comlt_epi8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomb((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (0)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comle_epi8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomb((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (1)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comgt_epi8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomb((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (2)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comge_epi8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomb((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (3)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comeq_epi8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomb((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (4)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comneq_epi8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomb((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (5)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comfalse_epi8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomb((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (6)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comtrue_epi8(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomb((__v16qi)(__m128i)(__A), (__v16qi)(__m128i)(__B), (7)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comlt_epi16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (0)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comle_epi16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (1)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comgt_epi16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (2)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comge_epi16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (3)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comeq_epi16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (4)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comneq_epi16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (5)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comfalse_epi16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (6)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comtrue_epi16(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomw((__v8hi)(__m128i)(__A), (__v8hi)(__m128i)(__B), (7)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comlt_epi32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomd((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (0)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comle_epi32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomd((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (1)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comgt_epi32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomd((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (2)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comge_epi32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomd((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (3)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comeq_epi32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomd((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (4)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comneq_epi32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomd((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (5)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comfalse_epi32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomd((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (6)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comtrue_epi32(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomd((__v4si)(__m128i)(__A), (__v4si)(__m128i)(__B), (7)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comlt_epi64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (0)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comle_epi64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (1)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comgt_epi64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (2)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comge_epi64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (3)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comeq_epi64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (4)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comneq_epi64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (5)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comfalse_epi64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (6)));
}

static __inline__ __m128i __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_comtrue_epi64(__m128i __A, __m128i __B)
{
  return ((__m128i)__builtin_ia32_vpcomq((__v2di)(__m128i)(__A), (__v2di)(__m128i)(__B), (7)));
}
# 731 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/xopintrin.h" 3
static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_frcz_ss(__m128 __A)
{
  return (__m128)__builtin_ia32_vfrczss((__v4sf)__A);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_frcz_sd(__m128d __A)
{
  return (__m128d)__builtin_ia32_vfrczsd((__v2df)__A);
}

static __inline__ __m128 __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_frcz_ps(__m128 __A)
{
  return (__m128)__builtin_ia32_vfrczps((__v4sf)__A);
}

static __inline__ __m128d __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(128)))
_mm_frcz_pd(__m128d __A)
{
  return (__m128d)__builtin_ia32_vfrczpd((__v2df)__A);
}

static __inline__ __m256 __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(256)))
_mm256_frcz_ps(__m256 __A)
{
  return (__m256)__builtin_ia32_vfrczps256((__v8sf)__A);
}

static __inline__ __m256d __attribute__((__always_inline__, __nodebug__, __target__("xop"), __min_vector_width__(256)))
_mm256_frcz_pd(__m256d __A)
{
  return (__m256d)__builtin_ia32_vfrczpd256((__v4df)__A);
}
# 58 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/x86intrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tbmintrin.h" 1 3
# 24 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/tbmintrin.h" 3
static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blcfill_u32(unsigned int __a)
{
  return __a & (__a + 1);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blci_u32(unsigned int __a)
{
  return __a | ~(__a + 1);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blcic_u32(unsigned int __a)
{
  return ~__a & (__a + 1);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blcmsk_u32(unsigned int __a)
{
  return __a ^ (__a + 1);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blcs_u32(unsigned int __a)
{
  return __a | (__a + 1);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blsfill_u32(unsigned int __a)
{
  return __a | (__a - 1);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blsic_u32(unsigned int __a)
{
  return ~__a | (__a - 1);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__t1mskc_u32(unsigned int __a)
{
  return ~__a | (__a + 1);
}

static __inline__ unsigned int __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__tzmsk_u32(unsigned int __a)
{
  return ~__a & (__a - 1);
}






static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blcfill_u64(unsigned long long __a)
{
  return __a & (__a + 1);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blci_u64(unsigned long long __a)
{
  return __a | ~(__a + 1);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blcic_u64(unsigned long long __a)
{
  return ~__a & (__a + 1);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blcmsk_u64(unsigned long long __a)
{
  return __a ^ (__a + 1);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blcs_u64(unsigned long long __a)
{
  return __a | (__a + 1);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blsfill_u64(unsigned long long __a)
{
  return __a | (__a - 1);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__blsic_u64(unsigned long long __a)
{
  return ~__a | (__a - 1);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__t1mskc_u64(unsigned long long __a)
{
  return ~__a | (__a + 1);
}

static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("tbm")))
__tzmsk_u64(unsigned long long __a)
{
  return ~__a & (__a - 1);
}
# 63 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/x86intrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/lwpintrin.h" 1 3
# 31 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/lwpintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("lwp")))
__llwpcb (void *__addr)
{
  __builtin_ia32_llwpcb(__addr);
}
# 46 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/lwpintrin.h" 3
static __inline__ void* __attribute__((__always_inline__, __nodebug__, __target__("lwp")))
__slwpcb (void)
{
  return __builtin_ia32_slwpcb();
}
# 68 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/x86intrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mwaitxintrin.h" 1 3
# 19 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/mwaitxintrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("mwaitx")))
_mm_monitorx(void * __p, unsigned __extensions, unsigned __hints)
{
  __builtin_ia32_monitorx(__p, __extensions, __hints);
}

static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("mwaitx")))
_mm_mwaitx(unsigned __extensions, unsigned __hints, unsigned __clock)
{
  __builtin_ia32_mwaitx(__extensions, __hints, __clock);
}
# 73 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/x86intrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/clzerointrin.h" 1 3
# 28 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/clzerointrin.h" 3
static __inline__ void __attribute__((__always_inline__, __nodebug__, __target__("clzero")))
_mm_clzero (void * __line)
{
  __builtin_ia32_clzero ((void *)__line);
}
# 78 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/x86intrin.h" 2 3




# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/rdpruintrin.h" 1 3
# 30 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/rdpruintrin.h" 3
static __inline__ unsigned long long __attribute__((__always_inline__, __nodebug__, __target__("rdpru")))
__rdpru (int reg_id)
{
  return __builtin_ia32_rdpru(reg_id);
}
# 83 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/x86intrin.h" 2 3


#pragma float_control(pop)
# 86 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c" 2
# 105 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
# 1 "/usr/include/stdio.h" 1 3 4
# 33 "/usr/include/stdio.h" 3 4
# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/stddef.h" 1 3 4
# 34 "/usr/include/stdio.h" 2 3 4
# 44 "/usr/include/stdio.h" 3 4
struct _IO_FILE;



typedef struct _IO_FILE FILE;
# 64 "/usr/include/stdio.h" 3 4
typedef struct _IO_FILE __FILE;
# 74 "/usr/include/stdio.h" 3 4
# 1 "/usr/include/libio.h" 1 3 4
# 32 "/usr/include/libio.h" 3 4
# 1 "/usr/include/_G_config.h" 1 3 4
# 15 "/usr/include/_G_config.h" 3 4
# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/stddef.h" 1 3 4
# 16 "/usr/include/_G_config.h" 2 3 4




# 1 "/usr/include/wchar.h" 1 3 4
# 82 "/usr/include/wchar.h" 3 4
typedef struct
{
  int __count;
  union
  {

    unsigned int __wch;



    char __wchb[4];
  } __value;
} __mbstate_t;
# 21 "/usr/include/_G_config.h" 2 3 4
typedef struct
{
  __off_t __pos;
  __mbstate_t __state;
} _G_fpos_t;
typedef struct
{
  __off64_t __pos;
  __mbstate_t __state;
} _G_fpos64_t;
# 33 "/usr/include/libio.h" 2 3 4
# 50 "/usr/include/libio.h" 3 4
# 1 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/stdarg.h" 1 3 4
# 30 "/opt/intel/oneapi/compiler/2023.1.0/linux/lib/clang/16/include/stdarg.h" 3 4
typedef __builtin_va_list __gnuc_va_list;
# 51 "/usr/include/libio.h" 2 3 4
# 145 "/usr/include/libio.h" 3 4
struct _IO_jump_t; struct _IO_FILE;
# 155 "/usr/include/libio.h" 3 4
typedef void _IO_lock_t;





struct _IO_marker {
  struct _IO_marker *_next;
  struct _IO_FILE *_sbuf;



  int _pos;
# 178 "/usr/include/libio.h" 3 4
};


enum __codecvt_result
{
  __codecvt_ok,
  __codecvt_partial,
  __codecvt_error,
  __codecvt_noconv
};
# 246 "/usr/include/libio.h" 3 4
struct _IO_FILE {
  int _flags;




  char* _IO_read_ptr;
  char* _IO_read_end;
  char* _IO_read_base;
  char* _IO_write_base;
  char* _IO_write_ptr;
  char* _IO_write_end;
  char* _IO_buf_base;
  char* _IO_buf_end;

  char *_IO_save_base;
  char *_IO_backup_base;
  char *_IO_save_end;

  struct _IO_marker *_markers;

  struct _IO_FILE *_chain;

  int _fileno;



  int _flags2;

  __off_t _old_offset;



  unsigned short _cur_column;
  signed char _vtable_offset;
  char _shortbuf[1];



  _IO_lock_t *_lock;
# 294 "/usr/include/libio.h" 3 4
  __off64_t _offset;
# 303 "/usr/include/libio.h" 3 4
  void *__pad1;
  void *__pad2;
  void *__pad3;
  void *__pad4;
  size_t __pad5;

  int _mode;

  char _unused2[15 * sizeof (int) - 4 * sizeof (void *) - sizeof (size_t)];

};


typedef struct _IO_FILE _IO_FILE;


struct _IO_FILE_plus;

extern struct _IO_FILE_plus _IO_2_1_stdin_;
extern struct _IO_FILE_plus _IO_2_1_stdout_;
extern struct _IO_FILE_plus _IO_2_1_stderr_;
# 339 "/usr/include/libio.h" 3 4
typedef __ssize_t __io_read_fn (void *__cookie, char *__buf, size_t __nbytes);







typedef __ssize_t __io_write_fn (void *__cookie, const char *__buf,
     size_t __n);







typedef int __io_seek_fn (void *__cookie, __off64_t *__pos, int __w);


typedef int __io_close_fn (void *__cookie);
# 391 "/usr/include/libio.h" 3 4
extern int __underflow (_IO_FILE *);
extern int __uflow (_IO_FILE *);
extern int __overflow (_IO_FILE *, int);
# 435 "/usr/include/libio.h" 3 4
extern int _IO_getc (_IO_FILE *__fp);
extern int _IO_putc (int __c, _IO_FILE *__fp);
extern int _IO_feof (_IO_FILE *__fp) __attribute__ ((__nothrow__ ));
extern int _IO_ferror (_IO_FILE *__fp) __attribute__ ((__nothrow__ ));

extern int _IO_peekc_locked (_IO_FILE *__fp);





extern void _IO_flockfile (_IO_FILE *) __attribute__ ((__nothrow__ ));
extern void _IO_funlockfile (_IO_FILE *) __attribute__ ((__nothrow__ ));
extern int _IO_ftrylockfile (_IO_FILE *) __attribute__ ((__nothrow__ ));
# 465 "/usr/include/libio.h" 3 4
extern int _IO_vfscanf (_IO_FILE * __restrict, const char * __restrict,
   __gnuc_va_list, int *__restrict);
extern int _IO_vfprintf (_IO_FILE *__restrict, const char *__restrict,
    __gnuc_va_list);
extern __ssize_t _IO_padn (_IO_FILE *, int, __ssize_t);
extern size_t _IO_sgetn (_IO_FILE *, void *, size_t);

extern __off64_t _IO_seekoff (_IO_FILE *, __off64_t, int, int);
extern __off64_t _IO_seekpos (_IO_FILE *, __off64_t, int);

extern void _IO_free_backup_area (_IO_FILE *) __attribute__ ((__nothrow__ ));
# 75 "/usr/include/stdio.h" 2 3 4




typedef __gnuc_va_list va_list;
# 110 "/usr/include/stdio.h" 3 4
typedef _G_fpos_t fpos_t;
# 164 "/usr/include/stdio.h" 3 4
# 1 "/usr/include/bits/stdio_lim.h" 1 3 4
# 165 "/usr/include/stdio.h" 2 3 4



extern struct _IO_FILE *stdin;
extern struct _IO_FILE *stdout;
extern struct _IO_FILE *stderr;







extern int remove (const char *__filename) __attribute__ ((__nothrow__ ));

extern int rename (const char *__old, const char *__new) __attribute__ ((__nothrow__ ));




extern int renameat (int __oldfd, const char *__old, int __newfd,
       const char *__new) __attribute__ ((__nothrow__ ));
# 195 "/usr/include/stdio.h" 3 4
extern FILE *tmpfile (void) ;
# 209 "/usr/include/stdio.h" 3 4
extern char *tmpnam (char *__s) __attribute__ ((__nothrow__ )) ;





extern char *tmpnam_r (char *__s) __attribute__ ((__nothrow__ )) ;
# 227 "/usr/include/stdio.h" 3 4
extern char *tempnam (const char *__dir, const char *__pfx)
     __attribute__ ((__nothrow__ )) __attribute__ ((__malloc__)) ;
# 237 "/usr/include/stdio.h" 3 4
extern int fclose (FILE *__stream);




extern int fflush (FILE *__stream);
# 252 "/usr/include/stdio.h" 3 4
extern int fflush_unlocked (FILE *__stream);
# 272 "/usr/include/stdio.h" 3 4
extern FILE *fopen (const char *__restrict __filename,
      const char *__restrict __modes) ;




extern FILE *freopen (const char *__restrict __filename,
        const char *__restrict __modes,
        FILE *__restrict __stream) ;
# 306 "/usr/include/stdio.h" 3 4
extern FILE *fdopen (int __fd, const char *__modes) __attribute__ ((__nothrow__ )) ;
# 319 "/usr/include/stdio.h" 3 4
extern FILE *fmemopen (void *__s, size_t __len, const char *__modes)
  __attribute__ ((__nothrow__ )) ;




extern FILE *open_memstream (char **__bufloc, size_t *__sizeloc) __attribute__ ((__nothrow__ )) ;






extern void setbuf (FILE *__restrict __stream, char *__restrict __buf) __attribute__ ((__nothrow__ ));



extern int setvbuf (FILE *__restrict __stream, char *__restrict __buf,
      int __modes, size_t __n) __attribute__ ((__nothrow__ ));





extern void setbuffer (FILE *__restrict __stream, char *__restrict __buf,
         size_t __size) __attribute__ ((__nothrow__ ));


extern void setlinebuf (FILE *__stream) __attribute__ ((__nothrow__ ));
# 356 "/usr/include/stdio.h" 3 4
extern int fprintf (FILE *__restrict __stream,
      const char *__restrict __format, ...);




extern int printf (const char *__restrict __format, ...);

extern int sprintf (char *__restrict __s,
      const char *__restrict __format, ...) __attribute__ ((__nothrow__));





extern int vfprintf (FILE *__restrict __s, const char *__restrict __format,
       __gnuc_va_list __arg);




extern int vprintf (const char *__restrict __format, __gnuc_va_list __arg);

extern int vsprintf (char *__restrict __s, const char *__restrict __format,
       __gnuc_va_list __arg) __attribute__ ((__nothrow__));





extern int snprintf (char *__restrict __s, size_t __maxlen,
       const char *__restrict __format, ...)
     __attribute__ ((__nothrow__)) __attribute__ ((__format__ (__printf__, 3, 4)));

extern int vsnprintf (char *__restrict __s, size_t __maxlen,
        const char *__restrict __format, __gnuc_va_list __arg)
     __attribute__ ((__nothrow__)) __attribute__ ((__format__ (__printf__, 3, 0)));
# 412 "/usr/include/stdio.h" 3 4
extern int vdprintf (int __fd, const char *__restrict __fmt,
       __gnuc_va_list __arg)
     __attribute__ ((__format__ (__printf__, 2, 0)));
extern int dprintf (int __fd, const char *__restrict __fmt, ...)
     __attribute__ ((__format__ (__printf__, 2, 3)));
# 425 "/usr/include/stdio.h" 3 4
extern int fscanf (FILE *__restrict __stream,
     const char *__restrict __format, ...) ;




extern int scanf (const char *__restrict __format, ...) ;

extern int sscanf (const char *__restrict __s,
     const char *__restrict __format, ...) __attribute__ ((__nothrow__ ));
# 443 "/usr/include/stdio.h" 3 4
extern int fscanf (FILE *__restrict __stream, const char *__restrict __format, ...) __asm__ ("" "__isoc99_fscanf") ;


extern int scanf (const char *__restrict __format, ...) __asm__ ("" "__isoc99_scanf") ;

extern int sscanf (const char *__restrict __s, const char *__restrict __format, ...) __asm__ ("" "__isoc99_sscanf") __attribute__ ((__nothrow__ ));
# 471 "/usr/include/stdio.h" 3 4
extern int vfscanf (FILE *__restrict __s, const char *__restrict __format,
      __gnuc_va_list __arg)
     __attribute__ ((__format__ (__scanf__, 2, 0))) ;





extern int vscanf (const char *__restrict __format, __gnuc_va_list __arg)
     __attribute__ ((__format__ (__scanf__, 1, 0))) ;


extern int vsscanf (const char *__restrict __s,
      const char *__restrict __format, __gnuc_va_list __arg)
     __attribute__ ((__nothrow__ )) __attribute__ ((__format__ (__scanf__, 2, 0)));
# 494 "/usr/include/stdio.h" 3 4
extern int vfscanf (FILE *__restrict __s, const char *__restrict __format, __gnuc_va_list __arg) __asm__ ("" "__isoc99_vfscanf")



     __attribute__ ((__format__ (__scanf__, 2, 0))) ;
extern int vscanf (const char *__restrict __format, __gnuc_va_list __arg) __asm__ ("" "__isoc99_vscanf")

     __attribute__ ((__format__ (__scanf__, 1, 0))) ;
extern int vsscanf (const char *__restrict __s, const char *__restrict __format, __gnuc_va_list __arg) __asm__ ("" "__isoc99_vsscanf") __attribute__ ((__nothrow__ ))



     __attribute__ ((__format__ (__scanf__, 2, 0)));
# 531 "/usr/include/stdio.h" 3 4
extern int fgetc (FILE *__stream);
extern int getc (FILE *__stream);





extern int getchar (void);
# 550 "/usr/include/stdio.h" 3 4
extern int getc_unlocked (FILE *__stream);
extern int getchar_unlocked (void);
# 561 "/usr/include/stdio.h" 3 4
extern int fgetc_unlocked (FILE *__stream);
# 573 "/usr/include/stdio.h" 3 4
extern int fputc (int __c, FILE *__stream);
extern int putc (int __c, FILE *__stream);





extern int putchar (int __c);
# 594 "/usr/include/stdio.h" 3 4
extern int fputc_unlocked (int __c, FILE *__stream);







extern int putc_unlocked (int __c, FILE *__stream);
extern int putchar_unlocked (int __c);






extern int getw (FILE *__stream);


extern int putw (int __w, FILE *__stream);
# 622 "/usr/include/stdio.h" 3 4
extern char *fgets (char *__restrict __s, int __n, FILE *__restrict __stream)
          ;
# 665 "/usr/include/stdio.h" 3 4
extern __ssize_t __getdelim (char **__restrict __lineptr,
          size_t *__restrict __n, int __delimiter,
          FILE *__restrict __stream) ;
extern __ssize_t getdelim (char **__restrict __lineptr,
        size_t *__restrict __n, int __delimiter,
        FILE *__restrict __stream) ;







extern __ssize_t getline (char **__restrict __lineptr,
       size_t *__restrict __n,
       FILE *__restrict __stream) ;
# 689 "/usr/include/stdio.h" 3 4
extern int fputs (const char *__restrict __s, FILE *__restrict __stream);





extern int puts (const char *__s);






extern int ungetc (int __c, FILE *__stream);






extern size_t fread (void *__restrict __ptr, size_t __size,
       size_t __n, FILE *__restrict __stream) ;




extern size_t fwrite (const void *__restrict __ptr, size_t __size,
        size_t __n, FILE *__restrict __s);
# 737 "/usr/include/stdio.h" 3 4
extern size_t fread_unlocked (void *__restrict __ptr, size_t __size,
         size_t __n, FILE *__restrict __stream) ;
extern size_t fwrite_unlocked (const void *__restrict __ptr, size_t __size,
          size_t __n, FILE *__restrict __stream);
# 749 "/usr/include/stdio.h" 3 4
extern int fseek (FILE *__stream, long int __off, int __whence);




extern long int ftell (FILE *__stream) ;




extern void rewind (FILE *__stream);
# 773 "/usr/include/stdio.h" 3 4
extern int fseeko (FILE *__stream, __off_t __off, int __whence);




extern __off_t ftello (FILE *__stream) ;
# 798 "/usr/include/stdio.h" 3 4
extern int fgetpos (FILE *__restrict __stream, fpos_t *__restrict __pos);




extern int fsetpos (FILE *__stream, const fpos_t *__pos);
# 826 "/usr/include/stdio.h" 3 4
extern void clearerr (FILE *__stream) __attribute__ ((__nothrow__ ));

extern int feof (FILE *__stream) __attribute__ ((__nothrow__ )) ;

extern int ferror (FILE *__stream) __attribute__ ((__nothrow__ )) ;




extern void clearerr_unlocked (FILE *__stream) __attribute__ ((__nothrow__ ));
extern int feof_unlocked (FILE *__stream) __attribute__ ((__nothrow__ )) ;
extern int ferror_unlocked (FILE *__stream) __attribute__ ((__nothrow__ )) ;
# 846 "/usr/include/stdio.h" 3 4
extern void perror (const char *__s);






# 1 "/usr/include/bits/sys_errlist.h" 1 3 4
# 26 "/usr/include/bits/sys_errlist.h" 3 4
extern int sys_nerr;
extern const char *const sys_errlist[];
# 854 "/usr/include/stdio.h" 2 3 4




extern int fileno (FILE *__stream) __attribute__ ((__nothrow__ )) ;




extern int fileno_unlocked (FILE *__stream) __attribute__ ((__nothrow__ )) ;
# 873 "/usr/include/stdio.h" 3 4
extern FILE *popen (const char *__command, const char *__modes) ;





extern int pclose (FILE *__stream);





extern char *ctermid (char *__s) __attribute__ ((__nothrow__ ));
# 913 "/usr/include/stdio.h" 3 4
extern void flockfile (FILE *__stream) __attribute__ ((__nothrow__ ));



extern int ftrylockfile (FILE *__stream) __attribute__ ((__nothrow__ )) ;


extern void funlockfile (FILE *__stream) __attribute__ ((__nothrow__ ));
# 934 "/usr/include/stdio.h" 3 4
# 1 "/usr/include/bits/stdio.h" 1 3 4
# 35 "/usr/include/bits/stdio.h" 3 4
extern __inline __attribute__ ((__gnu_inline__)) int
vprintf (const char *__restrict __fmt, __gnuc_va_list __arg)
{
  return vfprintf (stdout, __fmt, __arg);
}



extern __inline __attribute__ ((__gnu_inline__)) int
getchar (void)
{
  return _IO_getc (stdin);
}




extern __inline __attribute__ ((__gnu_inline__)) int
fgetc_unlocked (FILE *__fp)
{
  return (__builtin_expect (((__fp)->_IO_read_ptr >= (__fp)->_IO_read_end), 0) ? __uflow (__fp) : *(unsigned char *) (__fp)->_IO_read_ptr++);
}





extern __inline __attribute__ ((__gnu_inline__)) int
getc_unlocked (FILE *__fp)
{
  return (__builtin_expect (((__fp)->_IO_read_ptr >= (__fp)->_IO_read_end), 0) ? __uflow (__fp) : *(unsigned char *) (__fp)->_IO_read_ptr++);
}


extern __inline __attribute__ ((__gnu_inline__)) int
getchar_unlocked (void)
{
  return (__builtin_expect (((stdin)->_IO_read_ptr >= (stdin)->_IO_read_end), 0) ? __uflow (stdin) : *(unsigned char *) (stdin)->_IO_read_ptr++);
}




extern __inline __attribute__ ((__gnu_inline__)) int
putchar (int __c)
{
  return _IO_putc (__c, stdout);
}




extern __inline __attribute__ ((__gnu_inline__)) int
fputc_unlocked (int __c, FILE *__stream)
{
  return (__builtin_expect (((__stream)->_IO_write_ptr >= (__stream)->_IO_write_end), 0) ? __overflow (__stream, (unsigned char) (__c)) : (unsigned char) (*(__stream)->_IO_write_ptr++ = (__c)));
}





extern __inline __attribute__ ((__gnu_inline__)) int
putc_unlocked (int __c, FILE *__stream)
{
  return (__builtin_expect (((__stream)->_IO_write_ptr >= (__stream)->_IO_write_end), 0) ? __overflow (__stream, (unsigned char) (__c)) : (unsigned char) (*(__stream)->_IO_write_ptr++ = (__c)));
}


extern __inline __attribute__ ((__gnu_inline__)) int
putchar_unlocked (int __c)
{
  return (__builtin_expect (((stdout)->_IO_write_ptr >= (stdout)->_IO_write_end), 0) ? __overflow (stdout, (unsigned char) (__c)) : (unsigned char) (*(stdout)->_IO_write_ptr++ = (__c)));
}
# 124 "/usr/include/bits/stdio.h" 3 4
extern __inline __attribute__ ((__gnu_inline__)) int
__attribute__ ((__nothrow__ )) feof_unlocked (FILE *__stream)
{
  return (((__stream)->_flags & 0x10) != 0);
}


extern __inline __attribute__ ((__gnu_inline__)) int
__attribute__ ((__nothrow__ )) ferror_unlocked (FILE *__stream)
{
  return (((__stream)->_flags & 0x20) != 0);
}
# 935 "/usr/include/stdio.h" 2 3 4
# 106 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c" 2
# 550 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
__attribute__((always_inline)) static void hh_trafo_kernel_4_AVX2_2hv_double (double* q, double* hh, int nb, int ldq, int ldh,

 double s);
# 593 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
__attribute__((always_inline)) static void hh_trafo_kernel_8_AVX2_2hv_double (double* q, double* hh, int nb, int ldq, int ldh,

 double s);
# 637 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
__attribute__((always_inline)) static void hh_trafo_kernel_12_AVX2_2hv_double (double* q, double* hh, int nb, int ldq, int ldh,

 double s);
# 681 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
__attribute__((always_inline)) static void hh_trafo_kernel_16_AVX2_2hv_double (double* q, double* hh, int nb, int ldq, int ldh,

 double s);
# 725 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
__attribute__((always_inline)) static void hh_trafo_kernel_20_AVX2_2hv_double (double* q, double* hh, int nb, int ldq, int ldh,

 double s);
# 769 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
__attribute__((always_inline)) static void hh_trafo_kernel_24_AVX2_2hv_double (double* q, double* hh, int nb, int ldq, int ldh,

 double s);
# 780 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
void double_hh_trafo_real_AVX2_2hv_double (double* q, double* hh, int* pnb, int* pnq, int* pldq, int* pldh);
# 1606 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
void double_hh_trafo_real_AVX2_2hv_double (double* q, double* hh, int* pnb, int* pnq, int* pldq, int* pldh)
{
  int i;
  int nb = *pnb;
  int nq = *pldq;
  int ldq = *pldq;
  int ldh = *pldh;
  int worked_on;

  worked_on = 0;
# 1662 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
  double s = hh[(ldh)+1]*1.0;
# 1770 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
#pragma ivdep

  for (i = 2; i < nb; i++)
    {

      s += hh[i-1] * hh[(i+ldh)];
# 1811 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    }
# 1856 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
  for (i = 0; i < nq - 20; i+= 24 )
    {
      hh_trafo_kernel_24_AVX2_2hv_double (&q[i], hh, nb, ldq, ldh, s);
      worked_on += 24;
    }

  if (nq == i)
    {
      return;
    }
# 1895 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
  if (nq-i == 20)
    {
      hh_trafo_kernel_20_AVX2_2hv_double (&q[i], hh, nb, ldq, ldh, s);
      worked_on += 20;
    }
# 1930 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
  if (nq-i == 16)
    {
      hh_trafo_kernel_16_AVX2_2hv_double (&q[i], hh, nb, ldq, ldh, s);
      worked_on += 16;
    }
# 1963 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
  if (nq-i == 12)
    {
      hh_trafo_kernel_12_AVX2_2hv_double (&q[i], hh, nb, ldq, ldh, s);
      worked_on += 12;
    }
# 1991 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
  if (nq-i == 8)
    {
      hh_trafo_kernel_8_AVX2_2hv_double (&q[i], hh, nb, ldq, ldh, s);
      worked_on += 8;
    }
# 2016 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
  if (nq-i == 4)
    {
      hh_trafo_kernel_4_AVX2_2hv_double (&q[i], hh, nb, ldq, ldh, s);
      worked_on += 4;
    }
# 2300 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
}
# 2345 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
__attribute__((always_inline)) static void hh_trafo_kernel_24_AVX2_2hv_double (double* q, double* hh, int nb, int ldq, int ldh,

               double s)







  {
# 2369 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    int i;
# 2397 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
        __m256d sign = (__m256d)_mm256_set1_epi64x(0x8000000000000000);
# 2422 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    __m256d x1 = _mm256_load_pd(&q[ldq]);
    __m256d x2 = _mm256_load_pd(&q[ldq+4]);
    __m256d x3 = _mm256_load_pd(&q[ldq+2*4]);
    __m256d x4 = _mm256_load_pd(&q[ldq+3*4]);
    __m256d x5 = _mm256_load_pd(&q[ldq+4*4]);
    __m256d x6 = _mm256_load_pd(&q[ldq+5*4]);
# 2436 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    __m256d h1 = _mm256_broadcast_sd(&hh[ldh+1]);


    __m256d h2;
# 2454 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    __m256d q1 = _mm256_load_pd(q);
    __m256d y1 = _mm256_add_pd( q1, _mm256_mul_pd( x1, h1));
    __m256d q2 = _mm256_load_pd(&q[4]);
    __m256d y2 = _mm256_add_pd( q2, _mm256_mul_pd( x2, h1));
    __m256d q3 = _mm256_load_pd(&q[2*4]);
    __m256d y3 = _mm256_add_pd( q3, _mm256_mul_pd( x3, h1));
    __m256d q4 = _mm256_load_pd(&q[3*4]);
    __m256d y4 = _mm256_add_pd( q4, _mm256_mul_pd( x4, h1));
    __m256d q5 = _mm256_load_pd(&q[4*4]);
    __m256d y5 = _mm256_add_pd( q5, _mm256_mul_pd( x5, h1));
    __m256d q6 = _mm256_load_pd(&q[5*4]);
    __m256d y6 = _mm256_add_pd( q6, _mm256_mul_pd( x6, h1));
# 3018 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    for(i = 2; i < nb; i++)
      {
# 3030 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
        h1 = _mm256_broadcast_sd(&hh[i-(2 -1)]);
        h2 = _mm256_broadcast_sd(&hh[ldh+i-(2 -2)]);


        q1 = _mm256_load_pd(&q[i*ldq]);
        q2 = _mm256_load_pd(&q[(i*ldq)+4]);
        q3 = _mm256_load_pd(&q[(i*ldq)+2*4]);
        q4 = _mm256_load_pd(&q[(i*ldq)+3*4]);
        q5 = _mm256_load_pd(&q[(i*ldq)+4*4]);
        q6 = _mm256_load_pd(&q[(i*ldq)+5*4]);
# 3055 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
        x1 = _mm256_add_pd( x1, _mm256_mul_pd( q1,h1));
        y1 = _mm256_add_pd( y1, _mm256_mul_pd( q1,h2));
        x2 = _mm256_add_pd( x2, _mm256_mul_pd( q2,h1));
        y2 = _mm256_add_pd( y2, _mm256_mul_pd( q2,h2));
        x3 = _mm256_add_pd( x3, _mm256_mul_pd( q3,h1));
        y3 = _mm256_add_pd( y3, _mm256_mul_pd( q3,h2));
        x4 = _mm256_add_pd( x4, _mm256_mul_pd( q4,h1));
        y4 = _mm256_add_pd( y4, _mm256_mul_pd( q4,h2));
        x5 = _mm256_add_pd( x5, _mm256_mul_pd( q5,h1));
        y5 = _mm256_add_pd( y5, _mm256_mul_pd( q5,h2));
        x6 = _mm256_add_pd( x6, _mm256_mul_pd( q6,h1));
        y6 = _mm256_add_pd( y6, _mm256_mul_pd( q6,h2));
# 3186 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
      }







    h1 = _mm256_broadcast_sd(&hh[nb-(2 -1)]);


    q1 = _mm256_load_pd(&q[nb*ldq]);
    q2 = _mm256_load_pd(&q[(nb*ldq)+4]);
    q3 = _mm256_load_pd(&q[(nb*ldq)+2*4]);
    q4 = _mm256_load_pd(&q[(nb*ldq)+3*4]);
    q5 = _mm256_load_pd(&q[(nb*ldq)+4*4]);
    q6 = _mm256_load_pd(&q[(nb*ldq)+5*4]);
# 3211 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    x1 = _mm256_add_pd( x1, _mm256_mul_pd( q1,h1));
    x2 = _mm256_add_pd( x2, _mm256_mul_pd( q2,h1));
    x3 = _mm256_add_pd( x3, _mm256_mul_pd( q3,h1));
    x4 = _mm256_add_pd( x4, _mm256_mul_pd( q4,h1));
    x5 = _mm256_add_pd( x5, _mm256_mul_pd( q5,h1));
    x6 = _mm256_add_pd( x6, _mm256_mul_pd( q6,h1));
# 3840 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   __m256d tau1 = _mm256_broadcast_sd(hh);
   __m256d tau2 = _mm256_broadcast_sd(&hh[ldh]);
# 3852 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   __m256d vs = _mm256_broadcast_sd(&s);
# 3884 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    h1 = _mm256_xor_pd(tau1, sign);
# 3911 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   x1 = _mm256_mul_pd( x1, h1);
   x2 = _mm256_mul_pd( x2, h1);
   x3 = _mm256_mul_pd( x3, h1);
   x4 = _mm256_mul_pd( x4, h1);
   x5 = _mm256_mul_pd( x5, h1);
   x6 = _mm256_mul_pd( x6, h1);



   h1 = _mm256_xor_pd(tau2, sign);
# 3941 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   h2 = _mm256_mul_pd( h1, vs);
# 3959 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   y1 = _mm256_add_pd( _mm256_mul_pd( y1,h1), _mm256_mul_pd( x1,h2));
   y2 = _mm256_add_pd( _mm256_mul_pd( y2,h1), _mm256_mul_pd( x2,h2));
   y3 = _mm256_add_pd( _mm256_mul_pd( y3,h1), _mm256_mul_pd( x3,h2));
   y4 = _mm256_add_pd( _mm256_mul_pd( y4,h1), _mm256_mul_pd( x4,h2));
   y5 = _mm256_add_pd( _mm256_mul_pd( y5,h1), _mm256_mul_pd( x5,h2));
   y6 = _mm256_add_pd( _mm256_mul_pd( y6,h1), _mm256_mul_pd( x6,h2));
# 4078 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   q1 = _mm256_load_pd(&q[0]);

   q1 = _mm256_add_pd( q1, y1);







   _mm256_store_pd(&q[0], q1);
   q2 = _mm256_load_pd(&q[4]);

   q2 = _mm256_add_pd( q2, y2);







   _mm256_store_pd(&q[4], q2);
   q3 = _mm256_load_pd(&q[2*4]);

   q3 = _mm256_add_pd( q3, y3);







   _mm256_store_pd(&q[2*4], q3);
   q4 = _mm256_load_pd(&q[3*4]);

   q4 = _mm256_add_pd( q4, y4);







   _mm256_store_pd(&q[3*4], q4);
   q5 = _mm256_load_pd(&q[4*4]);

   q5 = _mm256_add_pd( q5, y5);







   _mm256_store_pd(&q[4*4], q5);
   q6 = _mm256_load_pd(&q[5*4]);

   q6 = _mm256_add_pd( q6, y6);







   _mm256_store_pd(&q[5*4], q6);
# 4153 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   h2 = _mm256_broadcast_sd(&hh[ldh+1]);


   q1 = _mm256_load_pd(&q[ldq]);
   q2 = _mm256_load_pd(&q[ldq+4]);
   q3 = _mm256_load_pd(&q[ldq+2*4]);
   q4 = _mm256_load_pd(&q[ldq+3*4]);
   q5 = _mm256_load_pd(&q[ldq+4*4]);
   q6 = _mm256_load_pd(&q[ldq+5*4]);
# 4170 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   q1 = _mm256_add_pd( q1, _mm256_add_pd( x1, _mm256_mul_pd( y1, h2)));
   q2 = _mm256_add_pd( q2, _mm256_add_pd( x2, _mm256_mul_pd( y2, h2)));
   q3 = _mm256_add_pd( q3, _mm256_add_pd( x3, _mm256_mul_pd( y3, h2)));
   q4 = _mm256_add_pd( q4, _mm256_add_pd( x4, _mm256_mul_pd( y4, h2)));
   q5 = _mm256_add_pd( q5, _mm256_add_pd( x5, _mm256_mul_pd( y5, h2)));
   q6 = _mm256_add_pd( q6, _mm256_add_pd( x6, _mm256_mul_pd( y6, h2)));

   _mm256_store_pd(&q[ldq], q1);
   _mm256_store_pd(&q[ldq+4], q2);
   _mm256_store_pd(&q[ldq+2*4], q3);
   _mm256_store_pd(&q[ldq+3*4], q4);
   _mm256_store_pd(&q[ldq+4*4], q5);
   _mm256_store_pd(&q[ldq+5*4], q6);
# 4904 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   for (i = 2; i < nb; i++)
   {
# 4915 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    h1 = _mm256_broadcast_sd(&hh[i-(2 -1)]);
    h2 = _mm256_broadcast_sd(&hh[ldh+i-(2 -2)]);


     q1 = _mm256_load_pd(&q[i*ldq]);
     q2 = _mm256_load_pd(&q[(i*ldq)+4]);
     q3 = _mm256_load_pd(&q[(i*ldq)+2*4]);
     q4 = _mm256_load_pd(&q[(i*ldq)+3*4]);
     q5 = _mm256_load_pd(&q[(i*ldq)+4*4]);
     q6 = _mm256_load_pd(&q[(i*ldq)+5*4]);
# 4941 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
     q1 = _mm256_add_pd( q1, _mm256_add_pd( _mm256_mul_pd( x1,h1), _mm256_mul_pd( y1, h2)));
     q2 = _mm256_add_pd( q2, _mm256_add_pd( _mm256_mul_pd( x2,h1), _mm256_mul_pd( y2, h2)));
     q3 = _mm256_add_pd( q3, _mm256_add_pd( _mm256_mul_pd( x3,h1), _mm256_mul_pd( y3, h2)));
     q4 = _mm256_add_pd( q4, _mm256_add_pd( _mm256_mul_pd( x4,h1), _mm256_mul_pd( y4, h2)));
     q5 = _mm256_add_pd( q5, _mm256_add_pd( _mm256_mul_pd( x5,h1), _mm256_mul_pd( y5, h2)));
     q6 = _mm256_add_pd( q6, _mm256_add_pd( _mm256_mul_pd( x6,h1), _mm256_mul_pd( y6, h2)));
# 5098 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
     _mm256_store_pd(&q[i*ldq], q1);
     _mm256_store_pd(&q[(i*ldq)+4], q2);
     _mm256_store_pd(&q[(i*ldq)+2*4], q3);
     _mm256_store_pd(&q[(i*ldq)+3*4], q4);
     _mm256_store_pd(&q[(i*ldq)+4*4], q5);
     _mm256_store_pd(&q[(i*ldq)+5*4], q6);

   }







   h1 = _mm256_broadcast_sd(&hh[nb-(2 -1)]);


   q1 = _mm256_load_pd(&q[nb*ldq]);
   q2 = _mm256_load_pd(&q[(nb*ldq)+4]);
   q3 = _mm256_load_pd(&q[(nb*ldq)+2*4]);
   q4 = _mm256_load_pd(&q[(nb*ldq)+3*4]);
   q5 = _mm256_load_pd(&q[(nb*ldq)+4*4]);
   q6 = _mm256_load_pd(&q[(nb*ldq)+5*4]);
# 5133 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   q1 = _mm256_add_pd( q1, _mm256_mul_pd( x1, h1));
   q2 = _mm256_add_pd( q2, _mm256_mul_pd( x2, h1));
   q3 = _mm256_add_pd( q3, _mm256_mul_pd( x3, h1));
   q4 = _mm256_add_pd( q4, _mm256_mul_pd( x4, h1));
   q5 = _mm256_add_pd( q5, _mm256_mul_pd( x5, h1));
   q6 = _mm256_add_pd( q6, _mm256_mul_pd( x6, h1));
# 5272 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   _mm256_store_pd(&q[nb*ldq], q1);
   _mm256_store_pd(&q[(nb*ldq)+4], q2);
   _mm256_store_pd(&q[(nb*ldq)+2*4], q3);
   _mm256_store_pd(&q[(nb*ldq)+3*4], q4);
   _mm256_store_pd(&q[(nb*ldq)+4*4], q5);
   _mm256_store_pd(&q[(nb*ldq)+5*4], q6);
# 5613 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
}
# 5658 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
__attribute__((always_inline)) static void hh_trafo_kernel_20_AVX2_2hv_double (double* q, double* hh, int nb, int ldq, int ldh,

               double s)







  {
# 5682 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    int i;
# 5709 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
        __m256d sign = (__m256d)_mm256_set1_epi64x(0x8000000000000000);
# 5734 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    __m256d x1 = _mm256_load_pd(&q[ldq]);
    __m256d x2 = _mm256_load_pd(&q[ldq+4]);
    __m256d x3 = _mm256_load_pd(&q[ldq+2*4]);
    __m256d x4 = _mm256_load_pd(&q[ldq+3*4]);
    __m256d x5 = _mm256_load_pd(&q[ldq+4*4]);
# 5747 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    __m256d h1 = _mm256_broadcast_sd(&hh[ldh+1]);

    __m256d h2;
# 5763 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    __m256d q1 = _mm256_load_pd(q);
    __m256d y1 = _mm256_add_pd( q1, _mm256_mul_pd( x1, h1));
    __m256d q2 = _mm256_load_pd(&q[4]);
    __m256d y2 = _mm256_add_pd( q2, _mm256_mul_pd( x2, h1));
    __m256d q3 = _mm256_load_pd(&q[2*4]);
    __m256d y3 = _mm256_add_pd( q3, _mm256_mul_pd( x3, h1));
    __m256d q4 = _mm256_load_pd(&q[3*4]);
    __m256d y4 = _mm256_add_pd( q4, _mm256_mul_pd( x4, h1));
    __m256d q5 = _mm256_load_pd(&q[4*4]);
    __m256d y5 = _mm256_add_pd( q5, _mm256_mul_pd( x5, h1));
# 6259 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    for(i = 2; i < nb; i++)
      {
# 6270 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
        h1 = _mm256_broadcast_sd(&hh[i-(2 -1)]);
        h2 = _mm256_broadcast_sd(&hh[ldh+i-(2 -2)]);


        q1 = _mm256_load_pd(&q[i*ldq]);
        q2 = _mm256_load_pd(&q[(i*ldq)+4]);
        q3 = _mm256_load_pd(&q[(i*ldq)+2*4]);
        q4 = _mm256_load_pd(&q[(i*ldq)+3*4]);
        q5 = _mm256_load_pd(&q[(i*ldq)+4*4]);
# 6291 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
        x1 = _mm256_add_pd( x1, _mm256_mul_pd( q1,h1));
        y1 = _mm256_add_pd( y1, _mm256_mul_pd( q1,h2));
        x2 = _mm256_add_pd( x2, _mm256_mul_pd( q2,h1));
        y2 = _mm256_add_pd( y2, _mm256_mul_pd( q2,h2));
        x3 = _mm256_add_pd( x3, _mm256_mul_pd( q3,h1));
        y3 = _mm256_add_pd( y3, _mm256_mul_pd( q3,h2));
        x4 = _mm256_add_pd( x4, _mm256_mul_pd( q4,h1));
        y4 = _mm256_add_pd( y4, _mm256_mul_pd( q4,h2));
        x5 = _mm256_add_pd( x5, _mm256_mul_pd( q5,h1));
        y5 = _mm256_add_pd( y5, _mm256_mul_pd( q5,h2));
# 6412 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
      }







    h1 = _mm256_broadcast_sd(&hh[nb-(2 -1)]);


    q1 = _mm256_load_pd(&q[nb*ldq]);
    q2 = _mm256_load_pd(&q[(nb*ldq)+4]);
    q3 = _mm256_load_pd(&q[(nb*ldq)+2*4]);
    q4 = _mm256_load_pd(&q[(nb*ldq)+3*4]);
    q5 = _mm256_load_pd(&q[(nb*ldq)+4*4]);







    x1 = _mm256_add_pd( x1, _mm256_mul_pd( q1,h1));
    x2 = _mm256_add_pd( x2, _mm256_mul_pd( q2,h1));
    x3 = _mm256_add_pd( x3, _mm256_mul_pd( q3,h1));
    x4 = _mm256_add_pd( x4, _mm256_mul_pd( q4,h1));
    x5 = _mm256_add_pd( x5, _mm256_mul_pd( q5,h1));
# 7024 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   __m256d tau1 = _mm256_broadcast_sd(hh);
   __m256d tau2 = _mm256_broadcast_sd(&hh[ldh]);
# 7036 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   __m256d vs = _mm256_broadcast_sd(&s);
# 7068 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    h1 = _mm256_xor_pd(tau1, sign);
# 7096 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   x1 = _mm256_mul_pd( x1, h1);
   x2 = _mm256_mul_pd( x2, h1);
   x3 = _mm256_mul_pd( x3, h1);
   x4 = _mm256_mul_pd( x4, h1);
   x5 = _mm256_mul_pd( x5, h1);



   h1 = _mm256_xor_pd(tau2, sign);
# 7125 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   h2 = _mm256_mul_pd( h1, vs);
# 7142 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   y1 = _mm256_add_pd( _mm256_mul_pd( y1,h1), _mm256_mul_pd( x1,h2));
   y2 = _mm256_add_pd( _mm256_mul_pd( y2,h1), _mm256_mul_pd( x2,h2));
   y3 = _mm256_add_pd( _mm256_mul_pd( y3,h1), _mm256_mul_pd( x3,h2));
   y4 = _mm256_add_pd( _mm256_mul_pd( y4,h1), _mm256_mul_pd( x4,h2));
   y5 = _mm256_add_pd( _mm256_mul_pd( y5,h1), _mm256_mul_pd( x5,h2));
# 7250 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   q1 = _mm256_load_pd(&q[0]);

   q1 = _mm256_add_pd( q1, y1);







   _mm256_store_pd(&q[0], q1);
   q2 = _mm256_load_pd(&q[4]);

   q2 = _mm256_add_pd( q2, y2);







   _mm256_store_pd(&q[4], q2);
   q3 = _mm256_load_pd(&q[2*4]);

   q3 = _mm256_add_pd( q3, y3);







   _mm256_store_pd(&q[2*4], q3);
   q4 = _mm256_load_pd(&q[3*4]);

   q4 = _mm256_add_pd( q4, y4);







   _mm256_store_pd(&q[3*4], q4);
   q5 = _mm256_load_pd(&q[4*4]);

   q5 = _mm256_add_pd( q5, y5);







   _mm256_store_pd(&q[4*4], q5);
# 7314 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   h2 = _mm256_broadcast_sd(&hh[ldh+1]);


   q1 = _mm256_load_pd(&q[ldq]);
   q2 = _mm256_load_pd(&q[ldq+4]);
   q3 = _mm256_load_pd(&q[ldq+2*4]);
   q4 = _mm256_load_pd(&q[ldq+3*4]);
   q5 = _mm256_load_pd(&q[ldq+4*4]);







   q1 = _mm256_add_pd( q1, _mm256_add_pd( x1, _mm256_mul_pd( y1, h2)));
   q2 = _mm256_add_pd( q2, _mm256_add_pd( x2, _mm256_mul_pd( y2, h2)));
   q3 = _mm256_add_pd( q3, _mm256_add_pd( x3, _mm256_mul_pd( y3, h2)));
   q4 = _mm256_add_pd( q4, _mm256_add_pd( x4, _mm256_mul_pd( y4, h2)));
   q5 = _mm256_add_pd( q5, _mm256_add_pd( x5, _mm256_mul_pd( y5, h2)));

   _mm256_store_pd(&q[ldq], q1);
   _mm256_store_pd(&q[ldq+4], q2);
   _mm256_store_pd(&q[ldq+2*4], q3);
   _mm256_store_pd(&q[ldq+3*4], q4);
   _mm256_store_pd(&q[ldq+4*4], q5);
# 7996 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   for (i = 2; i < nb; i++)
   {
# 8007 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    h1 = _mm256_broadcast_sd(&hh[i-(2 -1)]);
    h2 = _mm256_broadcast_sd(&hh[ldh+i-(2 -2)]);


     q1 = _mm256_load_pd(&q[i*ldq]);
     q2 = _mm256_load_pd(&q[(i*ldq)+4]);
     q3 = _mm256_load_pd(&q[(i*ldq)+2*4]);
     q4 = _mm256_load_pd(&q[(i*ldq)+3*4]);
     q5 = _mm256_load_pd(&q[(i*ldq)+4*4]);
# 8030 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
     q1 = _mm256_add_pd( q1, _mm256_add_pd( _mm256_mul_pd( x1,h1), _mm256_mul_pd( y1, h2)));
     q2 = _mm256_add_pd( q2, _mm256_add_pd( _mm256_mul_pd( x2,h1), _mm256_mul_pd( y2, h2)));
     q3 = _mm256_add_pd( q3, _mm256_add_pd( _mm256_mul_pd( x3,h1), _mm256_mul_pd( y3, h2)));
     q4 = _mm256_add_pd( q4, _mm256_add_pd( _mm256_mul_pd( x4,h1), _mm256_mul_pd( y4, h2)));
     q5 = _mm256_add_pd( q5, _mm256_add_pd( _mm256_mul_pd( x5,h1), _mm256_mul_pd( y5, h2)));
# 8175 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
     _mm256_store_pd(&q[i*ldq], q1);
     _mm256_store_pd(&q[(i*ldq)+4], q2);
     _mm256_store_pd(&q[(i*ldq)+2*4], q3);
     _mm256_store_pd(&q[(i*ldq)+3*4], q4);
     _mm256_store_pd(&q[(i*ldq)+4*4], q5);

   }







   h1 = _mm256_broadcast_sd(&hh[nb-(2 -1)]);


   q1 = _mm256_load_pd(&q[nb*ldq]);
   q2 = _mm256_load_pd(&q[(nb*ldq)+4]);
   q3 = _mm256_load_pd(&q[(nb*ldq)+2*4]);
   q4 = _mm256_load_pd(&q[(nb*ldq)+3*4]);
   q5 = _mm256_load_pd(&q[(nb*ldq)+4*4]);
# 8207 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   q1 = _mm256_add_pd( q1, _mm256_mul_pd( x1, h1));
   q2 = _mm256_add_pd( q2, _mm256_mul_pd( x2, h1));
   q3 = _mm256_add_pd( q3, _mm256_mul_pd( x3, h1));
   q4 = _mm256_add_pd( q4, _mm256_mul_pd( x4, h1));
   q5 = _mm256_add_pd( q5, _mm256_mul_pd( x5, h1));
# 8336 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   _mm256_store_pd(&q[nb*ldq], q1);
   _mm256_store_pd(&q[(nb*ldq)+4], q2);
   _mm256_store_pd(&q[(nb*ldq)+2*4], q3);
   _mm256_store_pd(&q[(nb*ldq)+3*4], q4);
   _mm256_store_pd(&q[(nb*ldq)+4*4], q5);
# 8647 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
}
# 8693 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
__attribute__((always_inline)) static void hh_trafo_kernel_16_AVX2_2hv_double (double* q, double* hh, int nb, int ldq, int ldh,

               double s)







  {
# 8717 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    int i;
# 8744 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
        __m256d sign = (__m256d)_mm256_set1_epi64x(0x8000000000000000);
# 8769 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    __m256d x1 = _mm256_load_pd(&q[ldq]);
    __m256d x2 = _mm256_load_pd(&q[ldq+4]);
    __m256d x3 = _mm256_load_pd(&q[ldq+2*4]);
    __m256d x4 = _mm256_load_pd(&q[ldq+3*4]);
# 8781 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    __m256d h1 = _mm256_broadcast_sd(&hh[ldh+1]);

    __m256d h2;
# 8795 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    __m256d q1 = _mm256_load_pd(q);
    __m256d y1 = _mm256_add_pd( q1, _mm256_mul_pd( x1, h1));
    __m256d q2 = _mm256_load_pd(&q[4]);
    __m256d y2 = _mm256_add_pd( q2, _mm256_mul_pd( x2, h1));
    __m256d q3 = _mm256_load_pd(&q[2*4]);
    __m256d y3 = _mm256_add_pd( q3, _mm256_mul_pd( x3, h1));
    __m256d q4 = _mm256_load_pd(&q[3*4]);
    __m256d y4 = _mm256_add_pd( q4, _mm256_mul_pd( x4, h1));
# 9219 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    for(i = 2; i < nb; i++)
      {
# 9230 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
        h1 = _mm256_broadcast_sd(&hh[i-(2 -1)]);
        h2 = _mm256_broadcast_sd(&hh[ldh+i-(2 -2)]);


        q1 = _mm256_load_pd(&q[i*ldq]);
        q2 = _mm256_load_pd(&q[(i*ldq)+4]);
        q3 = _mm256_load_pd(&q[(i*ldq)+2*4]);
        q4 = _mm256_load_pd(&q[(i*ldq)+3*4]);
# 9249 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
        x1 = _mm256_add_pd( x1, _mm256_mul_pd( q1,h1));
        y1 = _mm256_add_pd( y1, _mm256_mul_pd( q1,h2));
        x2 = _mm256_add_pd( x2, _mm256_mul_pd( q2,h1));
        y2 = _mm256_add_pd( y2, _mm256_mul_pd( q2,h2));
        x3 = _mm256_add_pd( x3, _mm256_mul_pd( q3,h1));
        y3 = _mm256_add_pd( y3, _mm256_mul_pd( q3,h2));
        x4 = _mm256_add_pd( x4, _mm256_mul_pd( q4,h1));
        y4 = _mm256_add_pd( y4, _mm256_mul_pd( q4,h2));
# 9358 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
      }







    h1 = _mm256_broadcast_sd(&hh[nb-(2 -1)]);


    q1 = _mm256_load_pd(&q[nb*ldq]);
    q2 = _mm256_load_pd(&q[(nb*ldq)+4]);
    q3 = _mm256_load_pd(&q[(nb*ldq)+2*4]);
    q4 = _mm256_load_pd(&q[(nb*ldq)+3*4]);







    x1 = _mm256_add_pd( x1, _mm256_mul_pd( q1,h1));
    x2 = _mm256_add_pd( x2, _mm256_mul_pd( q2,h1));
    x3 = _mm256_add_pd( x3, _mm256_mul_pd( q3,h1));
    x4 = _mm256_add_pd( x4, _mm256_mul_pd( q4,h1));
# 9928 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   __m256d tau1 = _mm256_broadcast_sd(hh);
   __m256d tau2 = _mm256_broadcast_sd(&hh[ldh]);
# 9940 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   __m256d vs = _mm256_broadcast_sd(&s);
# 9972 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    h1 = _mm256_xor_pd(tau1, sign);
# 10000 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   x1 = _mm256_mul_pd( x1, h1);
   x2 = _mm256_mul_pd( x2, h1);
   x3 = _mm256_mul_pd( x3, h1);
   x4 = _mm256_mul_pd( x4, h1);



   h1 = _mm256_xor_pd(tau2, sign);
# 10029 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   h2 = _mm256_mul_pd( h1, vs);
# 10045 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   y1 = _mm256_add_pd( _mm256_mul_pd( y1,h1), _mm256_mul_pd( x1,h2));
   y2 = _mm256_add_pd( _mm256_mul_pd( y2,h1), _mm256_mul_pd( x2,h2));
   y3 = _mm256_add_pd( _mm256_mul_pd( y3,h1), _mm256_mul_pd( x3,h2));
   y4 = _mm256_add_pd( _mm256_mul_pd( y4,h1), _mm256_mul_pd( x4,h2));
# 10142 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   q1 = _mm256_load_pd(&q[0]);
   q2 = _mm256_load_pd(&q[4]);
   q3 = _mm256_load_pd(&q[2*4]);
   q4 = _mm256_load_pd(&q[3*4]);


   q1 = _mm256_add_pd( q1, y1);
   q2 = _mm256_add_pd( q2, y2);
   q3 = _mm256_add_pd( q3, y3);
   q4 = _mm256_add_pd( q4, y4);
# 10166 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   _mm256_store_pd(&q[0], q1);
   _mm256_store_pd(&q[4], q2);
   _mm256_store_pd(&q[2*4], q3);
   _mm256_store_pd(&q[3*4], q4);
# 10179 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   h2 = _mm256_broadcast_sd(&hh[ldh+1]);


   q1 = _mm256_load_pd(&q[ldq]);
   q2 = _mm256_load_pd(&q[ldq+4]);
   q3 = _mm256_load_pd(&q[ldq+2*4]);
   q4 = _mm256_load_pd(&q[ldq+3*4]);







   q1 = _mm256_add_pd( q1, _mm256_add_pd( x1, _mm256_mul_pd( y1, h2)));
   q2 = _mm256_add_pd( q2, _mm256_add_pd( x2, _mm256_mul_pd( y2, h2)));
   q3 = _mm256_add_pd( q3, _mm256_add_pd( x3, _mm256_mul_pd( y3, h2)));
   q4 = _mm256_add_pd( q4, _mm256_add_pd( x4, _mm256_mul_pd( y4, h2)));


   _mm256_store_pd(&q[ldq], q1);
   _mm256_store_pd(&q[ldq+4], q2);
   _mm256_store_pd(&q[ldq+2*4], q3);
   _mm256_store_pd(&q[ldq+3*4], q4);
# 10800 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   for (i = 2; i < nb; i++)
   {
# 10811 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    h1 = _mm256_broadcast_sd(&hh[i-(2 -1)]);
    h2 = _mm256_broadcast_sd(&hh[ldh+i-(2 -2)]);


     q1 = _mm256_load_pd(&q[i*ldq]);
     q2 = _mm256_load_pd(&q[(i*ldq)+4]);
     q3 = _mm256_load_pd(&q[(i*ldq)+2*4]);
     q4 = _mm256_load_pd(&q[(i*ldq)+3*4]);
# 10831 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
     q1 = _mm256_add_pd( q1, _mm256_add_pd( _mm256_mul_pd( x1,h1), _mm256_mul_pd( y1, h2)));
     q2 = _mm256_add_pd( q2, _mm256_add_pd( _mm256_mul_pd( x2,h1), _mm256_mul_pd( y2, h2)));
     q3 = _mm256_add_pd( q3, _mm256_add_pd( _mm256_mul_pd( x3,h1), _mm256_mul_pd( y3, h2)));
     q4 = _mm256_add_pd( q4, _mm256_add_pd( _mm256_mul_pd( x4,h1), _mm256_mul_pd( y4, h2)));
# 10961 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
     _mm256_store_pd(&q[i*ldq], q1);
     _mm256_store_pd(&q[(i*ldq)+4], q2);
     _mm256_store_pd(&q[(i*ldq)+2*4], q3);
     _mm256_store_pd(&q[(i*ldq)+3*4], q4);

   }







   h1 = _mm256_broadcast_sd(&hh[nb-(2 -1)]);


   q1 = _mm256_load_pd(&q[nb*ldq]);
   q2 = _mm256_load_pd(&q[(nb*ldq)+4]);
   q3 = _mm256_load_pd(&q[(nb*ldq)+2*4]);
   q4 = _mm256_load_pd(&q[(nb*ldq)+3*4]);
# 10990 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   q1 = _mm256_add_pd( q1, _mm256_mul_pd( x1, h1));
   q2 = _mm256_add_pd( q2, _mm256_mul_pd( x2, h1));
   q3 = _mm256_add_pd( q3, _mm256_mul_pd( x3, h1));
   q4 = _mm256_add_pd( q4, _mm256_mul_pd( x4, h1));
# 11108 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   _mm256_store_pd(&q[nb*ldq], q1);
   _mm256_store_pd(&q[(nb*ldq)+4], q2);
   _mm256_store_pd(&q[(nb*ldq)+2*4], q3);
   _mm256_store_pd(&q[(nb*ldq)+3*4], q4);
# 11390 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
}
# 11435 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
__attribute__((always_inline)) static void hh_trafo_kernel_12_AVX2_2hv_double (double* q, double* hh, int nb, int ldq, int ldh,

               double s)







  {
# 11459 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    int i;
# 11486 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
        __m256d sign = (__m256d)_mm256_set1_epi64x(0x8000000000000000);
# 11511 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    __m256d x1 = _mm256_load_pd(&q[ldq]);
    __m256d x2 = _mm256_load_pd(&q[ldq+4]);
    __m256d x3 = _mm256_load_pd(&q[ldq+2*4]);
# 11522 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    __m256d h1 = _mm256_broadcast_sd(&hh[ldh+1]);

    __m256d h2;

    __m256d q1 = _mm256_load_pd(q);
    __m256d q2 = _mm256_load_pd(&q[4]);
    __m256d q3 = _mm256_load_pd(&q[2*4]);






    __m256d y1 = _mm256_add_pd( q1, _mm256_mul_pd( x1, h1));
    __m256d y2 = _mm256_add_pd( q2, _mm256_mul_pd( x2, h1));
    __m256d y3 = _mm256_add_pd( q3, _mm256_mul_pd( x3, h1));
# 11886 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    for(i = 2; i < nb; i++)
      {
# 11897 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
        h1 = _mm256_broadcast_sd(&hh[i-(2 -1)]);
        h2 = _mm256_broadcast_sd(&hh[ldh+i-(2 -2)]);


        q1 = _mm256_load_pd(&q[i*ldq]);
        q2 = _mm256_load_pd(&q[(i*ldq)+4]);
        q3 = _mm256_load_pd(&q[(i*ldq)+2*4]);
# 11913 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
        x1 = _mm256_add_pd( x1, _mm256_mul_pd( q1,h1));
        y1 = _mm256_add_pd( y1, _mm256_mul_pd( q1,h2));
        x2 = _mm256_add_pd( x2, _mm256_mul_pd( q2,h1));
        y2 = _mm256_add_pd( y2, _mm256_mul_pd( q2,h2));
        x3 = _mm256_add_pd( x3, _mm256_mul_pd( q3,h1));
        y3 = _mm256_add_pd( y3, _mm256_mul_pd( q3,h2));
# 12013 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
      }







    h1 = _mm256_broadcast_sd(&hh[nb-(2 -1)]);


    q1 = _mm256_load_pd(&q[nb*ldq]);
    q2 = _mm256_load_pd(&q[(nb*ldq)+4]);
    q3 = _mm256_load_pd(&q[(nb*ldq)+2*4]);






    x1 = _mm256_add_pd( x1, _mm256_mul_pd( q1,h1));
    x2 = _mm256_add_pd( x2, _mm256_mul_pd( q2,h1));
    x3 = _mm256_add_pd( x3, _mm256_mul_pd( q3,h1));
# 12538 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   __m256d tau1 = _mm256_broadcast_sd(hh);
   __m256d tau2 = _mm256_broadcast_sd(&hh[ldh]);
# 12550 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   __m256d vs = _mm256_broadcast_sd(&s);
# 12582 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    h1 = _mm256_xor_pd(tau1, sign);
# 12610 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   x1 = _mm256_mul_pd( x1, h1);
   x2 = _mm256_mul_pd( x2, h1);
   x3 = _mm256_mul_pd( x3, h1);



   h1 = _mm256_xor_pd(tau2, sign);
# 12637 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   h2 = _mm256_mul_pd( h1, vs);
# 12653 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   y1 = _mm256_add_pd( _mm256_mul_pd( y1,h1), _mm256_mul_pd( x1,h2));
   y2 = _mm256_add_pd( _mm256_mul_pd( y2,h1), _mm256_mul_pd( x2,h2));
   y3 = _mm256_add_pd( _mm256_mul_pd( y3,h1), _mm256_mul_pd( x3,h2));
# 12739 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   q1 = _mm256_load_pd(&q[0]);

   q1 = _mm256_add_pd( q1, y1);







   _mm256_store_pd(&q[0], q1);
   q2 = _mm256_load_pd(&q[4]);

   q2 = _mm256_add_pd( q2, y2);







   _mm256_store_pd(&q[4], q2);
   q3 = _mm256_load_pd(&q[2*4]);

   q3 = _mm256_add_pd( q3, y3);
# 12772 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   _mm256_store_pd(&q[2*4], q3);
# 12782 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   h2 = _mm256_broadcast_sd(&hh[ldh+1]);


   q1 = _mm256_load_pd(&q[ldq]);
   q2 = _mm256_load_pd(&q[ldq+4]);
   q3 = _mm256_load_pd(&q[ldq+2*4]);





   q1 = _mm256_add_pd( q1, _mm256_add_pd( x1, _mm256_mul_pd( y1, h2)));
   q2 = _mm256_add_pd( q2, _mm256_add_pd( x2, _mm256_mul_pd( y2, h2)));
   q3 = _mm256_add_pd( q3, _mm256_add_pd( x3, _mm256_mul_pd( y3, h2)));


   _mm256_store_pd(&q[ldq], q1);
   _mm256_store_pd(&q[ldq+4], q2);
   _mm256_store_pd(&q[ldq+2*4], q3);
# 13334 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   for (i = 2; i < nb; i++)
   {
# 13345 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    h1 = _mm256_broadcast_sd(&hh[i-(2 -1)]);
    h2 = _mm256_broadcast_sd(&hh[ldh+i-(2 -2)]);


     q1 = _mm256_load_pd(&q[i*ldq]);
     q2 = _mm256_load_pd(&q[(i*ldq)+4]);
     q3 = _mm256_load_pd(&q[(i*ldq)+2*4]);
# 13362 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
     q1 = _mm256_add_pd( q1, _mm256_add_pd( _mm256_mul_pd( x1,h1), _mm256_mul_pd( y1, h2)));
     q2 = _mm256_add_pd( q2, _mm256_add_pd( _mm256_mul_pd( x2,h1), _mm256_mul_pd( y2, h2)));
     q3 = _mm256_add_pd( q3, _mm256_add_pd( _mm256_mul_pd( x3,h1), _mm256_mul_pd( y3, h2)));
# 13478 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
     _mm256_store_pd(&q[i*ldq], q1);
     _mm256_store_pd(&q[(i*ldq)+4], q2);
     _mm256_store_pd(&q[(i*ldq)+2*4], q3);

   }







   h1 = _mm256_broadcast_sd(&hh[nb-(2 -1)]);


   q1 = _mm256_load_pd(&q[nb*ldq]);
   q2 = _mm256_load_pd(&q[(nb*ldq)+4]);
   q3 = _mm256_load_pd(&q[(nb*ldq)+2*4]);
# 13504 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   q1 = _mm256_add_pd( q1, _mm256_mul_pd( x1, h1));
   q2 = _mm256_add_pd( q2, _mm256_mul_pd( x2, h1));
   q3 = _mm256_add_pd( q3, _mm256_mul_pd( x3, h1));
# 13612 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   _mm256_store_pd(&q[nb*ldq], q1);
   _mm256_store_pd(&q[(nb*ldq)+4], q2);
   _mm256_store_pd(&q[(nb*ldq)+2*4], q3);
# 13864 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
}
# 13912 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
__attribute__((always_inline)) static void hh_trafo_kernel_8_AVX2_2hv_double (double* q, double* hh, int nb, int ldq, int ldh,

               double s)







  {
# 13936 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    int i;
# 13963 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
        __m256d sign = (__m256d)_mm256_set1_epi64x(0x8000000000000000);
# 13988 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    __m256d x1 = _mm256_load_pd(&q[ldq]);
    __m256d x2 = _mm256_load_pd(&q[ldq+4]);
# 13998 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    __m256d h1 = _mm256_broadcast_sd(&hh[ldh+1]);


    __m256d h2;






    __m256d q1 = _mm256_load_pd(q);
    __m256d y1 = _mm256_add_pd( q1, _mm256_mul_pd( x1, h1));
    __m256d q2 = _mm256_load_pd(&q[4]);
    __m256d y2 = _mm256_add_pd( q2, _mm256_mul_pd( x2, h1));
# 14291 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    for(i = 2; i < nb; i++)
      {
# 14302 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
        h1 = _mm256_broadcast_sd(&hh[i-(2 -1)]);
        h2 = _mm256_broadcast_sd(&hh[ldh+i-(2 -2)]);


        q1 = _mm256_load_pd(&q[i*ldq]);
        q2 = _mm256_load_pd(&q[(i*ldq)+4]);






        x1 = _mm256_add_pd( x1, _mm256_mul_pd( q1,h1));
        y1 = _mm256_add_pd( y1, _mm256_mul_pd( q1,h2));
        x2 = _mm256_add_pd( x2, _mm256_mul_pd( q2,h1));
        y2 = _mm256_add_pd( y2, _mm256_mul_pd( q2,h2));
# 14404 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
      }







    h1 = _mm256_broadcast_sd(&hh[nb-(2 -1)]);


    q1 = _mm256_load_pd(&q[nb*ldq]);
    q2 = _mm256_load_pd(&q[(nb*ldq)+4]);




    x1 = _mm256_add_pd( x1, _mm256_mul_pd( q1,h1));
    x2 = _mm256_add_pd( x2, _mm256_mul_pd( q2,h1));
# 14885 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   __m256d tau1 = _mm256_broadcast_sd(hh);
   __m256d tau2 = _mm256_broadcast_sd(&hh[ldh]);
# 14897 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   __m256d vs = _mm256_broadcast_sd(&s);
# 14929 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    h1 = _mm256_xor_pd(tau1, sign);
# 14957 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   x1 = _mm256_mul_pd( x1, h1);
   x2 = _mm256_mul_pd( x2, h1);



   h1 = _mm256_xor_pd(tau2, sign);
# 14983 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   h2 = _mm256_mul_pd( h1, vs);
# 14997 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   y1 = _mm256_add_pd( _mm256_mul_pd( y1,h1), _mm256_mul_pd( x1,h2));
   y2 = _mm256_add_pd( _mm256_mul_pd( y2,h1), _mm256_mul_pd( x2,h2));
# 15072 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   q1 = _mm256_load_pd(&q[0]);

   q1 = _mm256_add_pd( q1, y1);







   _mm256_store_pd(&q[0], q1);
   q2 = _mm256_load_pd(&q[4]);

   q2 = _mm256_add_pd( q2, y2);







   _mm256_store_pd(&q[4], q2);
# 15103 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   h2 = _mm256_broadcast_sd(&hh[ldh+1]);
# 15114 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   q1 = _mm256_load_pd(&q[ldq]);
   q1 = _mm256_add_pd( q1, _mm256_add_pd( x1, _mm256_mul_pd( y1, h2)));
   _mm256_store_pd(&q[ldq], q1);
   q2 = _mm256_load_pd(&q[ldq+4]);
   q2 = _mm256_add_pd( q2, _mm256_add_pd( x2, _mm256_mul_pd( y2, h2)));
   _mm256_store_pd(&q[ldq+4], q2);
# 15582 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   for (i = 2; i < nb; i++)
   {
# 15593 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    h1 = _mm256_broadcast_sd(&hh[i-(2 -1)]);
    h2 = _mm256_broadcast_sd(&hh[ldh+i-(2 -2)]);


     q1 = _mm256_load_pd(&q[i*ldq]);
     q2 = _mm256_load_pd(&q[(i*ldq)+4]);
# 15607 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
     q1 = _mm256_add_pd( q1, _mm256_add_pd( _mm256_mul_pd( x1,h1), _mm256_mul_pd( y1, h2)));
     q2 = _mm256_add_pd( q2, _mm256_add_pd( _mm256_mul_pd( x2,h1), _mm256_mul_pd( y2, h2)));
# 15710 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
     _mm256_store_pd(&q[i*ldq], q1);
     _mm256_store_pd(&q[(i*ldq)+4], q2);

   }







   h1 = _mm256_broadcast_sd(&hh[nb-(2 -1)]);


   q1 = _mm256_load_pd(&q[nb*ldq]);
   q2 = _mm256_load_pd(&q[(nb*ldq)+4]);







   q1 = _mm256_add_pd( q1, _mm256_mul_pd( x1, h1));
   q2 = _mm256_add_pd( q2, _mm256_mul_pd( x2, h1));
# 15829 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   _mm256_store_pd(&q[nb*ldq], q1);
   _mm256_store_pd(&q[(nb*ldq)+4], q2);
# 16054 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
}
# 16103 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
__attribute__((always_inline)) static void hh_trafo_kernel_4_AVX2_2hv_double (double* q, double* hh, int nb, int ldq, int ldh,

               double s)







  {
# 16126 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    int i;
# 16153 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
        __m256d sign = (__m256d)_mm256_set1_epi64x(0x8000000000000000);
# 16178 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    __m256d x1 = _mm256_load_pd(&q[ldq]);
# 16187 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    __m256d h1 = _mm256_broadcast_sd(&hh[ldh+1]);


    __m256d h2;




    __m256d q1 = _mm256_load_pd(q);
    __m256d y1 = _mm256_add_pd( q1, _mm256_mul_pd( x1, h1));
# 16408 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    for(i = 2; i < nb; i++)
      {
# 16419 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
        h1 = _mm256_broadcast_sd(&hh[i-(2 -1)]);
        h2 = _mm256_broadcast_sd(&hh[ldh+i-(2 -2)]);







        q1 = _mm256_load_pd(&q[i*ldq]);
        x1 = _mm256_add_pd( x1, _mm256_mul_pd( q1,h1));
        y1 = _mm256_add_pd( y1, _mm256_mul_pd( q1,h2));
# 16508 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
      }







    h1 = _mm256_broadcast_sd(&hh[nb-(2 -1)]);






    q1 = _mm256_load_pd(&q[nb*ldq]);
    x1 = _mm256_add_pd( x1, _mm256_mul_pd( q1,h1));
# 16948 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   __m256d tau1 = _mm256_broadcast_sd(hh);
   __m256d tau2 = _mm256_broadcast_sd(&hh[ldh]);
# 16960 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   __m256d vs = _mm256_broadcast_sd(&s);
# 16992 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    h1 = _mm256_xor_pd(tau1, sign);
# 17019 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   x1 = _mm256_mul_pd( x1, h1);



   h1 = _mm256_xor_pd(tau2, sign);
# 17044 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   h2 = _mm256_mul_pd( h1, vs);
# 17057 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   y1 = _mm256_add_pd( _mm256_mul_pd( y1,h1), _mm256_mul_pd( x1,h2));
# 17123 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   q1 = _mm256_load_pd(&q[0]);

   q1 = _mm256_add_pd( q1, y1);







   _mm256_store_pd(&q[0], q1);
# 17143 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   h2 = _mm256_broadcast_sd(&hh[ldh+1]);







   q1 = _mm256_load_pd(&q[ldq]);
   q1 = _mm256_add_pd( q1, _mm256_add_pd( x1, _mm256_mul_pd( y1, h2)));
   _mm256_store_pd(&q[ldq], q1);
# 17550 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   for (i = 2; i < nb; i++)
   {
# 17561 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
    h1 = _mm256_broadcast_sd(&hh[i-(2 -1)]);
    h2 = _mm256_broadcast_sd(&hh[ldh+i-(2 -2)]);


     q1 = _mm256_load_pd(&q[i*ldq]);






     q1 = _mm256_add_pd( q1, _mm256_add_pd( _mm256_mul_pd( x1,h1), _mm256_mul_pd( y1, h2)));
# 17664 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
     _mm256_store_pd(&q[i*ldq], q1);

   }







   h1 = _mm256_broadcast_sd(&hh[nb-(2 -1)]);


   q1 = _mm256_load_pd(&q[nb*ldq]);






   q1 = _mm256_add_pd( q1, _mm256_mul_pd( x1, h1));
# 17769 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
   _mm256_store_pd(&q[nb*ldq], q1);
# 17965 "../src/elpa2/kernels/real_128bit_256bit_512bit_BLOCK_template.c"
}
# 55 "../src/elpa2/kernels/real_avx2_2hv_double_precision.c" 2
